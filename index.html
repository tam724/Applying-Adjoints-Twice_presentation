<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Appying Adjoints Twice</title>

		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300..700&display=swap" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&family=Quicksand:wght@300..700&display=swap" rel="stylesheet">
		
		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/serif.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Title frame -->
				<section class="center">
					<div class="r-stretch"></div> 
					<h3 style="margin-bottom: 100px;">
						Applying Adjoints Twice: An Efficient Gradient Implementation for Models with Linear Structure with Applications in Reconstruction for EPMA
					</h3>
					<p style="margin-bottom:50px; "><strong>Tamme Claus</strong><sup>1</sup>, Gaurav Achuda<sup>2</sup>, Silvia Richter<sup>2</sup>, Manuel Torrilhon<sup>1</sup></p>
					<p><sup>1</sup> ACoM, Applied and Computational Mathematics, RWTH Aachen University</p>
					<p><sup>2</sup> GFE, Central Facility for Electron Microscopy, RWTH Aachen University</p>
					<p style="margin-top:50px; "><em>May 2024</em></p>
					<div style=" margin-top:300px; text-align: right;">
						<img src="figures/gfe_logo.svg" height="290px"> 
						<img src="figures/rwth_acom_en_cmyk.svg" height="300px">
					</div>
				</section>
				
				<!-- Motivation -->
				<section style="text-align: left">
					<h3>
						Motivation: Matrix Product Bracketing
					</h3>
					Consider Matrix Multiplication ($N \gg I, J$):
					<div class="r-stack">
						<span class="fragment fade-out" data-fragment-index="1">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ji)} &   \\
							&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
							\color{white}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & h^{(1)} & — \\
							— & h^{(j)} & — \\
							— & h^{(J)} & — \\
							\end{pmatrix}}_{J \times N} 
							\cdot{}
							\color{white}{\Bigg(}
							\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
							\color{white}{\Bigg)
							}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								g^{(1)} &  g^{(i)} & g^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{white}{\Bigg)}
						\]</span>
						<span class="fragment fade-in-then-out" data-fragment-index="1">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ji)} &   \\
							&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
							\color{blue}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & h^{(1)} & — \\
							— & h^{(j)} & — \\
							— & h^{(J)} & — \\
							\end{pmatrix}}_{J \times N} 
							\cdot{}
							\color{white}{\Bigg(}
							\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
							\color{blue}{\Bigg)
							}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								g^{(1)} &  g^{(i)} & g^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{white}{\Bigg)}
						\]</span>
						<span class="fragment fade-in-then-out" data-fragment-index="2">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ji)} &   \\
							&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
							\color{white}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & h^{(1)} & — \\
							— & h^{(j)} & — \\
							— & h^{(J)} & — \\
							\end{pmatrix}}_{J \times N} 
							\cdot{}
							\color{red}{\Bigg(}
							\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
							\color{white}{\Bigg)
							}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								g^{(1)} &  g^{(i)} & g^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{red}{\Bigg)}
						\]</span>
						<span class="fragment fade-in" data-fragment-index="3">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ji)} &   \\
							&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
							\color{blue}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & h^{(1)} & — \\
							— & h^{(j)} & — \\
							— & h^{(J)} & — \\
							\end{pmatrix}}_{J \times N} 
							\cdot{}
							\color{red}{\Bigg(}
							\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
							\color{blue}{\Bigg)
							}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								g^{(1)} &  g^{(i)} & g^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{red}{\Bigg)}
						\]</span>
						<!-- <span style="background-color:#fff;" class="fragment fade-in" data-fragment-index="6">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ij)} &   \\
							&   &   \end{pmatrix}^{\color{black}{T}}}_{J \times I} = 
							\color{white}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & g^{(1)} & — \\
							— & g^{(j)} & — \\
							— & g^{(J)} & — \\
							\end{pmatrix}}_{J \times N}
							\cdot{}
							\color{white}{\Bigg(}
							\underbrace{\boldsymbol{A}^*}_{N \times N}
							\color{white}{\Bigg)}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								h^{(1)} &  h^{(i)} & h^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{white}{\Bigg)}
						\]</span> -->
					</div>
					<p>where to bracket?</p>
					<p style="text-align: center" class="fragment fade-in" data-fragment-index="4">
						different computational costs: $\color{red}{I\times N^2 + IJ\times N} \text{ or } \color{blue}{J\times N^2 + IJ\times N}$
					</p>
					<span class="fragment fade-in" data-fragment-index="5">
						<ul>
							<li>
								what happens if $\boldsymbol A$ is more general, e.g. "solution of a linear system / (linear, linearized) weak form" instead of "matrix multiplication"?
								\[
									\boldsymbol A g^{(i)} = \{u \in V \, | \, a(u, v) + \langle g^{(i)}, v \rangle = 0 \quad \forall v \in V\}
								\]
							</li>
							<li>
								what happens if $\boldsymbol A$ is a derivative (structure: chain rule, product rule, ...)?
								\[
									\boldsymbol A g^{(i)} = \frac{\partial f(h(w))}{\partial v}\frac{\partial h(w)}{\partial w}g^{(i)}
								\]
							</li>
						</ul>
					</span>
					<span class="fragment fade-in" data-fragment-index="6">
						In general (reduced cost, if $J < I$):
						\[
							\underbrace{\Sigma^T}_{I \times J} = \underbrace{G^T}_{I \times \infty} \cdot{}  \underbrace{A^*}_{\infty \times \infty} \cdot{} \underbrace{H^T}_{\infty \times J}
						\]
						where $\boldsymbol A ^* $is the <em>adjoint </em> operator.
					</span>
				</section>
				
				<section>
					<!-- Material Reconstruction in EPMA-->
					<section  style="text-align: left">
						<h3>Motivation: Electron Probe Microanalysis (EPMA)</h3>
						<div style="display:flex;">
							<div style="flex:1; margin-right: 40px; margin-top:-20px;">
								<p style="text-align: center; "><em>"Material imaging based on characteristic x-ray emission"</em></p>
								<ul style="margin-top: 0px;">
									<li>Material Science, Quality control, Mineralogy; Meteorite analysis</li>
									<li>Ionization of material sample using high-energy focused electron beam</li>
									<li>Wavelength-dispersive spectrometers measure characteristic x-radiation</li>

								</ul>
							</div>
							<div style="flex:1; text-align: center;">
								<img src="figures/epma.jpg" height="400px">
								<p style="font-size:60% ; margin-top: -20px;">Microprobe at GFE (source: <a href="https://www.gfe.rwth-aachen.de/cms/gfe/dienste/~rhxt/esma/?lidx=1">gfe.rwth-aachen.de</a>)</p>
							</div> 
						</div>
						<div style="display:flex;">
							<div class="r-stack" style="flex:2; text-align: center;">
								<div class="fragment fade-in-then-out" data-fragment-index="1">
									<img src="figures/01_material.jpg" alt="Material" width="700px" style="margin-top: 60px;">
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="2">
									<video data-autoplay width="700px" style="margin-top: 40px;">
										<source data-src="figures/02_forward.webm" type="video/webm">
									</video>
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="3">
									<video data-autoplay  width="700px" style="margin-top: 40px;">
										<source data-src="figures/03_ionization.webm" type="video/webm">
									</video>
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="4">
									<img src="figures/04_interaction_volume.jpg" alt="Interaction Volume"  width="700px" style="margin-top: 60px;">
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="5">
									<video data-autoplay  width="700px" style="margin-top: 40px;">
										<source data-src="figures/05_linescan.webm" type="video/webm">
									</video>
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="6">
									<video data-autoplay width="700px" style="margin-top: 40px;">
										<source data-src="figures/06_linescan.webm" type="video/webm">
									</video>
								</div>
								<div class="fragment fade-in" data-fragment-index="7">
									<video data-autoplay width="700px" style="margin-top: 40px;">
										<source data-src="figures/07_linescan.webm" type="video/webm">
									</video>
								</div>
							</div>
							<div style="flex:3">
								<ul>
									<li class="fragment fade-in" data-fragment-index="1">Example Material: two-phase (<span style="color: orange">A</span>, <span style="color: green;">B</span>)</li>
									<li class="fragment fade-in" data-fragment-index="2"><span style="color:blue">Electron</span> propagation affected by scattering (animated: different energies) </li>
									<li class="fragment fade-in" data-fragment-index="3">Ionization and emission of characteristic x-rays (wavelength of <span style="color:orange">A-rays</span> and <span style="color:green">B-rays</span> differ)</li>
									<li class="fragment fade-in" data-fragment-index="4">Measure wavelength-dispersive x-ray intensity</li>
									<li class="fragment fade-in" data-fragment-index="5">Spatial information gained via different beam positions (linescan), <span class="fragment fade-in" data-fragment-index="6">beam angles </span> <span class="fragment fade-in" data-fragment-index="7">and beam energies</span></li>
									<li class="fragment fade-in" data-fragment-index="8"><em>inverse problem of material reconstruction</em>
										<div class="r-stack">
											<span class="fragment fade-in-then-out" data-fragment-index="8">
												\begin{equation}
													p^* = \underset{p \in P}{\text{arg min}} \underbrace{|| \Sigma(p)  - \tilde{\Sigma} ||^2}_{=C(p)}
												\end{equation}
											</span>
											<span class="fragment fade-in" data-fragment-index="9">
												\begin{equation}
													p^* = \underset{p \in P}{\text{arg min}} \underbrace{|| (\Sigma \circ \gamma)(p)  - \tilde{\Sigma} ||^2}_{=C(p)}
												\end{equation}
											</span>
										</div>
									</li>
									<li class="fragment fade-in" data-fragment-index="10">gradient-based iterative methods (Steepest Descent, BFGS, or HMC)</li>
									<li class="fragment fade-in" data-fragment-index="11"><em>we focus on computing $\nabla_p C(p)$ efficiently via adjoints</em></li>
								</ul>
							</div>
						</div>
					</section>
					<!-- Gradients in Inverse Problems-->
					<section style="text-align: left;">
						<h3>Gradients in Inverse Problems</h3>
						<span><em>"Determine the model parameters $p$ such that the model $\Sigma(p)$ produces the observed data $\tilde{\Sigma}$."</em></span>
						<div style="display:flex;">
							<div style="flex:2 ;">
								<h4 style="margin-top: 50px;">classical approach</h4>
								\begin{align*}
									p^* = \underset{p \in P}{\text{arg min}} || \Sigma(p)  - \tilde{\Sigma} ||^2
								\end{align*}
								<ul>
									<li>iterative approximation of the minimum based on the gradient of the objective function $p \mapsto ||\Sigma(p) - \tilde{\Sigma}||^2$</li>
									<li>Gradient Descent, CG, BFGS, ...</li>
									<li>regularization: addition of a penalty term $p \mapsto ||\Sigma(p) - \tilde{\Sigma}||^2 + \mathcal{R}(p)$</li>
									<li>we will do: $\Sigma = \Sigma \circ \gamma$</li>
								</ul>
							</div>
							<div style="flex:1; ">
								<img src="figures/gd.png" width="500px">
							</div>
						</div>
						<div style="display:flex;">
							<div style="flex:1; ">
								<img src="figures/hmc.png" width="500px">
							</div>
							<div style="flex:2">
								<h4 style="margin-top: 50px;">bayesian approach</h4>
								<ul>
									<li>approximate the posterior $\pi(p|\tilde{\Sigma})$ (typically: iterative sampling, MCMC)</li>
									<li>Hamiltonian Monte-Carlo (based on the gradient of the target density)</li>
								</ul>
							</div>
						</div>
						</ul>
						<p style="text-align: center;"><em>Efficient evaluation of the objective function and its gradient is crucial!</em></p>
					</section>
				</section>

				<!-- An adjoint method -->
				<section style="text-align: left;">
					<section style="text-align: left";>
						<h3>An Adjoint Method</h3>
						<p>
							Definition of the <em>Adjoint</em> ($H, G$ Hilbert spaces, $A: G \to H$ continuous, linear)
							\[
								\langle h, A(g) \rangle_H := \langle A^*(h), g \rangle_G \quad \forall h \in H \, \forall g \in G
							\]
						</p>
						<ul>
							<li>given multiple inputs $g^{(1)}, ... g^{(I)} \in G$, $h^{(1)}, ... h^{(J)} \in H$
							</li>
							<li> we want to compute
							</li>
						</ul>
						\[
						\Sigma^{(ji)} = \langle h^{(j)}, A(g^{(i)}) \rangle_H = \langle A^*(h^{(j)}), g^{(i)} \rangle_G \quad \forall i=1, ...I \, j=1, ...J
						\]
						<div style="display:flex;">
							<div style="flex: 1;">
								<h4>non-adjoint implementation</h4>
								\begin{align*}
									&\text{foreach } \color{orange}{i = 1, ..., I} \text{ do} \\
									&\qquad \color{orange}{v^{(i)} \leftarrow A(g^{(i)})}\\
									&\qquad \text{foreach } \color{green}{j = 1, ..., J} \text{ do}\\
									&\qquad \qquad \Sigma^{(ji)} \leftarrow \langle \color{green}{h^{(j)}}, \color{orange}{v^{(i)}} \rangle_H\\
									&\qquad \text{end}\\
									&\text{end}
								\end{align*}
								<ul>
									<li>cost: $\color{orange}{I} \times \mathcal{C}(A) + \color{orange}{I}\color{green}{J} \times \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_H)$</li>
									<li>use if $\color{green}{J} > \color{orange}{I}$</li>
								</ul>
							</div>
							<div style="flex: 1;">
								<h4>adjoint implementation</h4>
								\begin{align*}
									&\text{foreach } \color{green}{j = 1, ..., J} \text{ do} \\
									&\qquad \color{green}{\lambda^{(j)} \leftarrow A^*(h^{(j)})}\\
									&\qquad \text{foreach } \color{orange}{i = 1, ..., I} \text{ do}\\
									&\qquad \qquad \Sigma^{(ji)} \leftarrow \langle \color{green}{\lambda^{(j)}}, \color{orange}{g^{(i)}} \rangle_G\\
									&\qquad \text{end}\\
									&\text{end}
								\end{align*}
								<ul>
									<li>cost: $\color{green}{J} \times \mathcal{C}(A^*) + \color{orange}{I}\color{green}{J} \times \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_G)$
									</li>
									<li>use if $\color{orange}{I} > \color{green}{J} $</li>
								</ul>
							</div>
						</div>
					</section>

				<!-- Riesz Representation Theorem/Def Adjoint -->
				<section style="text-align: left;">
					<h3>Riesz Representation Theorem/Definition of the Adjoint</h3>

					<h4>Riesz Representation Theorem</h4>
					<p>
						Let $(H, \langle \cdot{}, \cdot{} \rangle_H)$ be a (real) Hilbert space with inner product $\langle \cdot{}, \cdot{} \rangle_H$. For every continuous linear functional $f_h \in H^*$ (the dual of $H$), there exists a unique vector $h \in H$, called the <em>Riesz Representation</em> of $f_h$, such that
						\[
							f_h(x) = \langle h, x \rangle_H \quad \forall x \in H.
						\]
					</p>

					<h4>Definition: Adjoint Operator</h4>
					<p>
						Let $(G, \langle \cdot{}, \cdot{} \rangle_G)$ be another (real) Hilbert space, $A:G\to H$ a continuous linear operator between $G$ and $H$ and $f_h \in  H^*$ a continuous linear functional.
						\[
							f_h(A(g)) = \langle h, A(g) \rangle_H \quad \forall g \in G
						\]
						But $f_h(A(g))$ is also a continuous linear functional $f_\lambda(g)$ in $G$ with a Riesz Representation $\lambda \in G$
						\[
							f_h(A(g)) = f_\lambda(g) = \langle \lambda, g \rangle_G := \langle A^*(h), g \rangle_G.
						\]
					</p>
				</section>
					<section style="text-align: left;">
						<h3>Examples of (real) adjoint operators</h3>
						<div>
							<h4 style="color:forestgreen;">Scalar Multiplication</h4>
							<ul>
								<li>$G, H = \mathbb{R}$</li>
								<li>$A: \mathbb{R} \to \mathbb{R}, g \mapsto Ag = a \cdot{} g,\quad  a \in \mathbb{R}$</li>
								<li>$A^*: \mathbb{R} \to \mathbb{R}, \mu \mapsto A^* \mu = a \cdot{} \mu$</li>
							</ul>
							<p>
								<em>Derivation:</em>
								\begin{equation*}
									\langle \mu, Ag \rangle_{\mathbb{R}} = \langle \mu, a \cdot{} g \rangle_{\mathbb{R}} = \langle a \cdot{} \mu, g \rangle_{\mathbb{R}} = \langle A^*\mu , g \rangle_{\mathbb{R}} \quad \forall g \in \mathbb{R}\, \forall \mu \in \mathbb{R}
								\end{equation*}
							</p>
						</div>
						<div class="fragment fade-in">
							<h4 style="color:forestgreen;">Matrix Multiplication</h4>
							<ul>
								<li>$G = \mathbb{R}^n, H = \mathbb{R}^m$</li>
								<li>$A: \mathbb{R}^n \to \mathbb{R}^m, g \mapsto Ag = M \cdot{} g, \quad M \in \mathbb{R}^{m \times n}$</li>
								<li>$A^*: \mathbb{R}^m \to \mathbb{R}^n, \mu \mapsto A^*\mu = M^T \cdot{} \mu$</li>
							</ul>
							<p>
								<em>Derivation:</em>
								\begin{equation*}
									\langle \mu, Ag \rangle_{\mathbb{R}^m} = \langle \mu, M \cdot{} g \rangle_{\mathbb{R}^m} = \langle M^T \cdot{} \mu, g \rangle_{\mathbb{R}^n} = \langle A^*\mu, g \rangle_{\mathbb{R}^n} \quad \forall g \in \mathbb{R}^n \, \forall \mu \in \mathbb{R}^m
								\end{equation*}
							</p>
						</div>
						<div class="fragment fade-in">
							<h4 style="color:forestgreen;">Integration</h4>
							<ul>
								<li>$G = L^2(\Omega)$ (with $\langle \cdot{}, \cdot{} \rangle_{L^2(\Omega)} = \int_{\Omega} \cdot{} \cdot{} d x$), $H = \mathbb{R}$</li>
								<li>$A: L^2(\Omega) \to \mathbb{R}, g(\cdot{}) \mapsto Ag = \int_{\Omega} g(x) d x$</li>
								<li>$A^*: \mathbb{R} \to L^2(\Omega), \mu \mapsto (A^* \mu)(\cdot{}) = (x \mapsto \mu \cdot{} 1_{\Omega}(x))$</li>
							</ul>
							<p>
								<em>Derivation:</em>
								\begin{equation*}
									\langle \mu, Ag \rangle_\mathbb{R} = \mu \cdot{} \int_\Omega g(x) dx = \int_\Omega \mu \cdot{} 1_\Omega(x) g(x) dx = \langle (A^*\mu)(\cdot{}), g(\cdot{}) \rangle_{L^2(\Omega)} \quad \forall g \in L^2(\Omega) \, \forall \mu \in \mathbb{R}
								\end{equation*}
							</p>
						</div>
					</section>
					<section style="text-align: left;">
						<h3>Examples of (real) adjoint operators</h3>
						<h4 style="color:forestgreen;">Linear Solve</h4>
						<ul>
							<li>$G, H = \mathbb{R}^n$ and $M \in \mathbb{R}^{n\times n}$ invertible</li>
							<li>$A: \mathbb{R}^n \to \mathbb{R}^n, g \mapsto (h \in \mathbb{R}^n | M \cdot{} h = g)$, (or $A = M^{-1}$)</li>
							<li>$A^*: \mathbb{R}^n \to \mathbb{R}^n, \mu \mapsto (\lambda \in \mathbb{R}^n | M^T \cdot{} \lambda = \mu)$, (or $A^* = M^{-T}$)</li>
						</ul>
						<p>
							<em>Derivation (intentionally complicated):</em> We reinterpret
							\begin{equation*}
								M \cdot{} h = g \quad \Leftrightarrow \quad \langle v,  M \cdot{} h \rangle = \langle v, g \rangle \quad \forall v \in G 
							\end{equation*}
							in particular (for a fixed but unspecified $\lambda \in G$)
							\begin{equation}
								0 = \langle \lambda, g \rangle - \langle \lambda, M \cdot{} h \rangle \quad \Leftrightarrow \quad 0 = \langle \lambda, g \rangle - \langle M^T \cdot{} \lambda, h \rangle
							\end{equation}
							\begin{align*}
								\langle \mu, Ag \rangle &= \langle \mu, h \rangle\\
								&= \langle \mu, h \rangle - \langle M^T \cdot{} \lambda, h \rangle + \langle \lambda, g \rangle \\
								& \quad \text{ with } \langle \mu, h \rangle - \langle M^T \cdot{} \lambda, h \rangle = 0 \quad \forall h \in H \\
								& \quad \text{ or } M^T \cdot{} \lambda = \mu \\
								&= \langle A^* \mu, g \rangle
							\end{align*}
							(alternatively)
							\begin{equation*}
								\langle \mu, Ag \rangle_{\mathbb{R}^n} = \langle \mu, M^{-1} g \rangle_{\mathbb{R}^n} = \langle M^{-T} \mu, g \rangle_{\mathbb{R}^n} = \langle A^*\mu, g \rangle_{\mathbb{R}^n}
							\end{equation*}
						</p>
					</section>

					<section style="text-align: left;">
						<h3>Examples of (real) adjoint operators</h3>
						<h4 style="color:forestgreen;">Weak form</h4>
						<ul>
							<li>$G, H$ (real Hilbert spaces)</li>
							<li>$A: G \to H, g \mapsto \{h \in H | a(h, v) + \langle g, v \rangle_G = 0 \quad \forall v \in G\}$, $\quad a(\cdot{}, \cdot{}) \text{ coercive bilinear form}$</li>
							<li>$A^*: H \to G, \mu \mapsto \{\lambda \in G | a(v, \lambda) + \langle \mu,v \rangle_H = 0 \quad \forall v \in H\}$</li>
						</ul>
						<p>
							<em>Derivation:</em>
							\begin{align*}
								\langle \mu, Ag \rangle_H = f_\mu(h) &= f_\mu(h) + \underbrace{a(h, \lambda) + f_g(\lambda)}_{=0\quad \forall \lambda \in G} \\
								&= \underbrace{f_\mu(h) + a(h, \lambda)}_{!= 0 \quad \forall h \in H} + f_g(\lambda) = f_g(\lambda) = \langle \lambda, g \rangle_G
							\end{align*}
						</p>
					</section>
				</section>
<!-- 
				<section style="text-align: left;">
					<section>
						<h3>Problem with linear structure</h3>
						<ul>
							<li>$m = \gamma_p$ with $\gamma_{(\cdot{})}: \mathbb{R}^{N} \to M$: parametrization</li>
							<li>$u^{(i)} \in V$ a state where $a_m(u^{(i)}, v) = b_m^{(i)}(v) \quad \forall v \in V$ with $b^{(i)}_m$ an excitation $\forall i\in [1, I]$</li>
							<li>$\Sigma^{(i, j)} = c^{(j)}_m(u^{(i)})$ a measurement with $c_m^{(j)}: V \to \mathbb{R}$ an extraction $\forall i=[1, I], j=[1, J]$</li>
							<li>$C = g_{\Sigma}$ with $g_{(\cdot{})}: \mathbb{R}^{I\times J} \to \mathbb{R}$ the objective</li>
							<li>we are interested in $\nabla_p C$</li>
						</ul>
						<h4>Naive Implementation (Sensitivity Equations)</h4>
						<ul>
							<li>$\dot{m}^{(n)} = \dot{\gamma}_p(e_n)$ $\forall n \in [1, N]$</li>
							<li>$\dot{u}^{(i, n)} \in V$ where $a_m(\dot{u}^{(i, n)}, v) + \dot{a}_m(u^{(i)}, v, \dot{m}^{(n)})=\dot{b}^{(i)}_m(v, \dot{m}^{(n)})$ $\forall i \in [1, I], \forall n \in [1, N]$</li>
							<li>$\dot{\Sigma}^{(i, j, n)} = c_m^{(j)}(\dot{u}^{(i, n)}) + \dot{c}_m^{(j)}(u^{(i)}, \dot{m}^{(n)})$</li>
							<li>$\dot{C}^{(n)} = \dot{g}_{\Sigma}(\dot{\Sigma}^{(:, :, n)})$</li>
							<li>$(\nabla_{p} C)_n = \dot{C}^{(n)}$</li>
						</ul>
						<h4>First Adjoint</h4>
						<ul>
							<li>$a_m(u, \lambda^{(j)}) = c_m^{(j)}(u) \quad \forall u \in U$, $\forall j \in [1, J]$</li>
							<li>$\Sigma^{(i, j)} = b_m^{(i)}(\lambda^{(j)})$</li>
							<li>$a_m(\bar{u}^{(j)}, v) = \sum_{i=1}^{I}\bar{\Sigma}$</li>
						</ul>
					</section>
					<section>
						<h3>Full System</h3> 
					</section>
				</section> -->

				<section>
					<section style="text-align: left;">
						<h3>Derivative Notation (from Algorithmic Differentiation)</h3>
						<ul>
							<li>given $f_{(\cdot{})}: X \to Y, x \mapsto f_x$ ($X, Y$ Hilbert) non-linear, Frechet-differentiable</li>
						</ul>
						<p>
							we define:
						</p>	
						<ul>
							<li>the (continuous, linear) <em>tangent operator</em> $\dot{f}_x \in L(X, Y)$ (directional derivative)
								\begin{align*}
									\dot{f}_x(\dot{x}) = \frac{\partial f_x}{\partial x}(\dot{x}) = \lim_{h \to 0} \frac{f_{x + h\dot{x}} - f_{x}}{h}
								\end{align*}
							</li>
							<li>the (continuous, linear) <em>adjoint operator</em> $\bar{f}_x \in L(Y, X)$
								\begin{align*}
									\langle \bar{y} , \dot{f}_x(\dot{x}) \rangle_Y = \langle \bar{f}_x (\bar{y}), \dot{x} \rangle_X \quad \forall \bar{y} \in Y, \dot{x} \in X
								\end{align*}
							</li>
						</ul>
						<p>
							Also we define ($y = f_x$):
							<ul>
								<li><em>tangent variables</em> $\dot{x} \in X, \dot{y} \in Y$: given $\dot{x} \in X$, $\dot{y} = \dot{f}_x(\dot{x})$</li>
								<li><em>adjoint variables</em> $\bar{x} \in X, \bar{y} \in Y$: given $\bar{y} \in Y$, $\bar{x} = \bar{f}_x(\bar{y})$</li>
							</ul>
						</p>
						Applying this to a composition $ f_x = \varphi^{(N)}_{v^{(N-1)}} \circ \, ...\, \circ\, \varphi^{(1)}_x$ where $\varphi^{(n)}$ are single assignments (chain rule!) is tangent/adjoint mode automatic differentiation. (neglecting all implementational details..)
					</section>

					<section style="text-align: left;">
						<h3>Automatic/Algorithmic Differentiation</h3>
						<h4>Example</h4>
						\begin{equation}
							y = f_x = g \circ h_x \quad v = h_x
						\end{equation}
						by the chain rule, the <em>tangent operator</em> $\dot{f}_x$ is
						\begin{equation}
							\dot{y} = \dot{f}_x(\dot{x}) = \dot{g}_v(\dot{h}_x(\dot{x}))
						\end{equation}
						for the <em>adjoint operator</em> $\bar{f}_x$ we use 
						\begin{align*}
							\langle \bar{y} , \dot{f}_x(\dot{x}) \rangle_Y = \langle \bar{y}, \dot{g}_v(\dot{h}_x(\dot{x})) \rangle_Y = \langle \bar{g}_v (\bar{y}), \dot{h}_x(\dot{x}) \rangle_V = \langle \bar{h}_x(\bar{g}_v(\bar{y})), \dot{x} \rangle_X \quad \forall \bar{y} \in Y, \dot{x} \in X
						\end{align*}
						and find
						\begin{equation}
							\bar{f}_x = \bar{h}_x(\bar{g}_v(\bar{y}))
						\end{equation}
						<div style="display:flex;">
							<div style="flex: 1;">
								<h4>Tangent mode</h4>
								\begin{align*}
								&\text{foreach } j = 1, ... J \\
								&\qquad \dot{x} \leftarrow e_j \\
								&\qquad \dot{y} \leftarrow \dot{g}_v(\dot{h}_x(\dot{x}))\\
								&\qquad \text{foreach } i = 1, ... I \\
								&\qquad \qquad (Df)^{(i, j)} = \langle e_j, \dot{y} \rangle\\
								&\qquad \text{end}\\
								&\text{end}
								\end{align*}
							</div>
							<div style="flex: 1;">
								<h4>Adjoint mode</h4>
								\begin{align*}
								&\text{foreach } i = 1, ... I \\
								&\qquad \bar{y} \leftarrow e_i \\
								&\qquad \bar{x} \leftarrow \bar{h}_x(\bar{g}_v(\bar{y}))\\
								&\qquad \text{foreach } j = 1, ... J \\
								&\qquad \qquad (Df)^{(i, j)} = \langle \bar{x}, e_j \rangle\\
								&\qquad \text{end}\\
								&\text{end}
								\end{align*}
							</div>
						</div>
						<ul>
							<li>AD is even more systematic..</li>
						</ul>
					</section>

					<section style="text-align: left;">
						<h3>Automatic Algorithmic Differentiation</h3>
						Every numerical program $f: \mathbb{R}^{n-1} \to \mathbb{R}^{m+1}$ can be decomposed into <em>single assignments</em> $(\varphi^{(1)}, ... \varphi^{(N)})$.
						\begin{align*}
							&(v^{(0)}, v^{(-1)}, ..., v^{(-n)})^T = x \\
							&v^{(n)} = \varphi^{(n)}_{(v^{(j)})_{j \prec n}} \quad \forall n = 1, ... N \\
							&y = (v^{(N)}, v^{(N-1)}, ..., v^{(N-m)})^T
						\end{align*}
						<p>
							For every single assignment $\varphi^{(n)}_{(v^{(j)})_{j \prec n}}$ we know
						</p>
						<ul>
							<li>
								<em>tangent operator </em> $\dot{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\dot{v}^{(j)})_{j \prec n})$ and
							</li>
							<li>
								<em>adjoint operator</em> $\bar{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\bar{v}^{(k)})_{k \succ n})$
							</li>
						</ul>

						<div style="display:flex;">
							<div style="flex: 1;">
								<h4>Tangent mode</h4>
								\begin{align*}
									&(v^{(0)}, v^{(-1)}, ...)^T = x \\
									&(\dot{v}^{(0)}, \dot{v}^{(-1)}, ...)^T = \dot{x} \\
									&\hspace{-30px}\begin{cases}
										v^{(n)} = \varphi^{(n)}_{(v^{(j)})_{j \prec n}} \\
										\dot{v}^{(n)} = \dot{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\dot{v})_{j \prec n})
									\end{cases} \small{\quad \forall n = 1, ... N}\\
									&y = (v^{(N)}, v^{(N-1)}, ...)^T\\
									&\dot{y} = (\dot{v}^{(N)}, \dot{v}^{(N-1)}, ...)^T
								\end{align*}
							</div>
							<div style="flex: 1;">
								<h4>Adjoint mode</h4>
								\begin{alignat*}{2}
									&(v^{(0)}, v^{(-1)}, ...)^T = x \\
									&v^{(n)} = \varphi^{(n)}_{(v^{(j)})_{j \prec n}} &&\small{\quad \forall n = 1, ..., N}\\
									&y = (v^{(N)}, v^{(N-1)}, ...)^T\\
									&(\bar{v}^{(N)}, \bar{v}^{(N-1)}, ...)^T = \bar{y} \\
									&\bar{v}^{(n)} = \bar{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\bar{v}^{(k)})_{k \succ n}) &&\small{\quad \forall n = N-m-1, ..., -n}\\
									&\bar{x} = (\bar{v}^{(0)}, \bar{v}^{(-1)}, ...)^T
								\end{alignat*}
							</div>	
						</div>
					</section>
				</section>

				<section style="text-align: left;">
					<h3>Applying Adjoints Twice: First adjoint</h3>
					<div style="display:flex;">
						<div style="flex: 2;">
							We consider the model 
							\begin{align*}
								m &= \gamma_p \\
								a_m(u^{(i)}, v) + b^{(i)}(v) &= 0 \quad \forall v \in V(\mathcal{R}) \\
								\Sigma^{(i)} &= c(u^{(i)}) \\
								C &= g_{\boldsymbol \Sigma} \\
							\end{align*}
						</div>
						<div style="flex: 3; margin-left: 100px;">
							<ul>
								<li>computational domain $\mathcal{R}$</li>
								<li>parametrization: $\gamma: \mathbb{R}^{n} \to M(\mathcal{R})$</li>
								<li>system $a_m \in \text{Bil}(V(\mathcal{R}) \times V(\mathcal{R}), \mathbb{R})$ </li>
								<li>excitations $\boldsymbol{b} = (b^{(1)}, ... b^{(I)})^T \in L(V(\mathcal{R}), \mathbb{R}^I)$</li>
								<li>extraction $c \in L(V(\mathcal{R}), \mathbb{R})$</li>
								<li>measurements $\boldsymbol{\Sigma} = (\Sigma^{(1)}, ..., \Sigma^{(I)})^T \in \mathbb{R}^I$</li>
								<li>objective $g: \mathbb{R}^I \to \mathbb{R}$</li>
							</ul>
						</div>
					</div>
					<div style="margin-top: 50px;">
						<ul style="width: 100%">
							<li><span style="color: gray;">solution operator $A(g^{(i)}) = (u^{(i)} | a_m(u^{(i)}, v) + b^{(i)}(v) = 0 \, \forall v \text{ where } b^{(i)}(v) = \langle g^{(i)}, v \rangle)$</span></li>
							<li class="fragment fade-in" data-fragment-index="1">introduce a $\lambda \in V(\mathcal{R})$ (fixed but yet unspecified) "Lagrange-multiplier" 
								<div class="r-stack">
									<div class="fragment fade-in-then-out" data-fragment-index="1">
										\[
											\Sigma^{(i)} = c(u^{(i)}) \color{white}{+ \underbrace{a_m(u^{(i)}, \lambda) + b^{(i)}(\lambda)}_{=0 \vphantom{\forall u^{(i)}}}}
										\]
									</div>
									<div class="fragment fade-in-then-out" data-fragment-index="2">
										\[
										\Sigma^{(i)} = c(u^{(i)}) + \underbrace{a_m(u^{(i)}, \lambda) + b^{(i)}(\lambda)}_{=0 \vphantom{\forall u^{(i)}}}
									\]
									</div>
									<div class="fragment fade-in" data-fragment-index="3">
										\[
											\Sigma^{(i)} = \underbrace{c(u^{(i)}) + a_m(u^{(i)}, \lambda)}_{=0 \forall u^{(i)}} + b^{(i)}(\lambda)
										\]
									</div>
								</div>
							</li>
							<li class="fragment fade-in" data-fragment-index="4"> now we specify $\lambda \in V(\mathcal{R})$ s.t.
								<div>
									\begin{align*}
										a_m(u, \lambda) + c(u) &= 0 \quad \forall u \in V(\mathcal{R}) \\
										\Sigma^{(i)} &= b^{(i)}(\lambda) \\
									\end{align*}
								</div>	
							</li>
							<li class="fragment fade-in" data-fragment-index="4"><span style="color: gray;">adjoint solution operator $A^*(h) = (\lambda | a_m(u, \lambda) + c(u) = 0 \, \forall u \text{ where } c(u) = \langle h, u \rangle)$</span></li>
							<li class="fragment fade-in" data-fragment-index="5">
								1st "application of an adjoint method"
								<!-- \begin{align*}
									a_m(u, \lambda) + c(u) &= 0 \quad \forall u \in V(\mathcal{R}) \\
									C = g_{\boldsymbol\Sigma} &= g_{\boldsymbol b(\lambda)}
								\end{align*} -->
							</li>
						</ul>
					</div>
				</section>

				<section style="text-align: left;">
					<h3>Applying Adjoints Twice: Second adjoint</h3>
					We continue with
					<div> 
						\begin{align*}
							a_m(u, \lambda) + c(u) &= 0 \quad \forall u \in V(\mathcal{R}) \\
							C = g_{\boldsymbol\Sigma} &= g_{\boldsymbol b(\lambda)}
						\end{align*}
					</div>
					<ul style="width: 100%">
						<li class="fragment fade-in" data-fragment-index="1">
							derive tangent model ($\dot{m}^{(n)} = \dot{\gamma}_p(e_n)$)
							<div class="r-stack">
								<span class="fragment fade-in-then-out" data-fragment-index="1">
									\begin{alignat*}{2}
										a_m(u, \dot{\lambda}^{(n)}) &+ \dot{a}_m(u, \lambda, \dot{m}^{(n)}) &&= 0 \quad \forall u \in V(\mathcal{R}) \\
										&\dot{C}^{(n)} = \dot{g}_{\boldsymbol \Sigma}(\boldsymbol b(\dot{\lambda}^{(n)}))
									\end{alignat*} 
								</span>
								<span class="fragment fade-in-then-out" data-fragment-index="2"> 
									\begin{alignat*}{2}
										a_m(u, \dot{\lambda}^{(n)}) &+ \beta^{(n)}(u) && = 0 \quad \forall u \in V(\mathcal{R}) \\
										&\dot{C}^{(n)} = \dot{g}_{\boldsymbol \Sigma}(\boldsymbol b(\dot{\lambda}^{(n)}))
									\end{alignat*}
								</span>
								<span class="fragment fade-in-then-out" data-fragment-index="3"> 
									\begin{alignat*}{2}
										a_m(u, \dot{\lambda}^{(n)}) &+ \beta^{(n)}(u) && = 0 \quad \forall u \in V(\mathcal{R}) \\
										&\dot{C}^{(n)} = \alpha(\dot{\lambda}^{(n)}) \hspace{19pt}
									\end{alignat*}
								</span>
								<span class="fragment fade-in-then-out" data-fragment-index="4"> 
									\begin{alignat*}{2}
										a_m(u, \dot{\lambda}^{(n)}) &+ \beta^{(n)}(u) && = 0 \quad \forall u \in V(\mathcal{R}) \\
										&\dot{C}^{(n)} = \alpha(\dot{\lambda}^{(n)}) \hspace{19pt}
									\end{alignat*} 
								</span>
								<span class="fragment fade-in" data-fragment-index="5">
									\begin{alignat*}{2}
										a_m(u, \dot{\lambda}^{(n)}) &+ \dot{a}_m(u, \lambda, \dot{m}^{(n)}) &&= 0 \quad \forall u \in V(\mathcal{R}) \\
										&\dot{C}^{(n)} = \dot{g}_{\boldsymbol \Sigma}(\boldsymbol b(\dot{\lambda}^{(n)}))
									\end{alignat*} 
								</span>
							</div> 
						</li>
						<li class="fragment fade-in" data-fragment-index="4">
							second application of "an adjoint method" ($\bar{\lambda} \in U$) <em>(continuous adjoint method)</em>
							<div class="r-stack">
								<span class="fragment fade-in-then-out" data-fragment-index="4">
									\begin{alignat*}{2}
										a_m(\bar{\lambda}, \dot{\lambda}) &+ \alpha(\dot{\lambda}) && = 0 \quad \forall \dot{\lambda} \in V(\mathcal{R}) \\
										&\dot{C}^{(n)} = \beta^{(n)}(\bar{\lambda}) \hspace{30pt}
									\end{alignat*}
								</span>
								<span class="fragment fade-in" data-fragment-index="5">
									\begin{alignat*}{2}
										a_m(\bar{\lambda}, \dot{\lambda}) &+ \dot{g}_{\boldsymbol \Sigma}(\boldsymbol b(\dot{\lambda})) && = 0 \quad \forall \dot{\lambda} \in V(\mathcal{R}) \\
										&\dot{C}^{(n)} = \dot{a}_m(\bar{\lambda}, \lambda, \dot{m}^{(n)})
									\end{alignat*}
								</span>
							</div>
						</li>
						<li class="fragment fade-in" data-fragment-index="6">
							with adjoint variables ($\bar{\boldsymbol{\Sigma}} = \bar{g}_{\boldsymbol\Sigma}(\bar{C})$)
							\begin{alignat*}{2}
										a_m(\bar{\lambda}, \dot{\lambda}) &+ \bar{\boldsymbol \Sigma}^T \boldsymbol b(\dot{\lambda}) && = 0 \quad \forall \dot{\lambda} \in V(\mathcal{R}) \\
										&\bar{p} = \bar{\gamma}_p(\bar{a}_m(\bar{\lambda}, \lambda, \bar{C}))\\
										&(\nabla_p C)^{(n)} = \bar{p}^{(n)}
									\end{alignat*}
						</li>
					</ul>
				</section>

				<section style="text-align: left;">
					<h3>Applying Adjoints Twice</h3>
					<table style="text-align: center">
						<tr style="border-style: hidden;">
							<td style="text-align: center; ">
								<h4>non-adjoint forward + tangent derivative</h4>
							</td>
							<td style="text-align: center; ">
								<h4>adjoint forward + adjoint derivative</h4>
							</td>
						</tr>
						<tr>
							<td style="text-align: center; ">
								\begin{align*}
								a_m(u^{(i)}, v) + b^{(i)}(v) &= 0 \quad \forall v \in V(\mathcal{R}) \\
								\Sigma^{(i)} &= c(u^{(i)}) \\
								C &= g_{\boldsymbol \Sigma} \\
								a_m(\dot{u}^{(i, n)}, v) + \dot{a}_m(u^{(i)}, v, \dot{m}^{(n)}) &= 0 \quad \forall v \in V(\mathcal{R})\\
								\dot{\Sigma}^{(i, n)} &= c(\dot{u}^{(i, n)}) \\
								(\nabla_p C)^{(n)} &= \dot{g}_{\boldsymbol \Sigma}(\dot{\boldsymbol \Sigma}^{(n)})
							\end{align*}
							</td>
							<td style="text-align: center; ">
								\begin{align}
								a_m(u, \lambda) + c(u) &= 0 \quad \forall u \in V(\mathcal{R})\\
								\boldsymbol \Sigma &= \boldsymbol b(\lambda) \\
								C &= g_{\boldsymbol \Sigma} \\
								\bar{\boldsymbol \Sigma} &= \bar{g}_{\boldsymbol \Sigma}(\bar{C})\\
								a_m(\bar{\lambda}, \dot{\lambda}) + \bar{\boldsymbol{\Sigma}}^T \boldsymbol b(\dot{\lambda}) &= 0 \quad \forall \dot{\lambda} \in V(\mathcal{R}) \\
								\bar{p} &= \bar{\gamma}_p(\bar{a}_m(\bar{\lambda}, \lambda, \bar{C}))\\ 
								(\nabla_p C)^{(n)} &= \bar{p}^{(n)} 
							\end{align}
							</td>
						</tr>
					</table>
					<h4>Generalizations:</h4>
					<ul>
						<li>$m$-dependent excitation $b^{(i)}(v) \rightsquigarrow b_m^{(i)}(v)$</li>
						<li>$m$-dependent extractions $c(u) \rightsquigarrow c_m(u)$</li>
						<li>multiple extractions $c_m(u) \rightsquigarrow c^{(j)}_m(u)$ ($I, N \gg J$)</li>
					</ul>
					<h4 style="margin-top: 20px;">Assumptions:</h4>
					<ul>
						<li>the cost $\mathcal{C}(a)$ of solving $a(\cdot{}, \cdot{}) + ... = 0$ dominates (no direct solve possible)</li>
						<li>cost comparison (naive): $(I + IN)\times \mathcal{C}(a) + ... \mathcal{C}(\langle\cdot\rangle)$</li>
						<li>cost comparison (adjoint<sup>2</sup>): $2J \times \mathcal{C}(a) + ... \times \mathcal{C}(\langle\cdot\rangle)$</li>
					</ul>
				</section>

				<section style="text-align: left;">
					<section><h3>Example: An inverse problem based on the Poisson equation</h3>
						<table>
							<tr>
								<td style="text-align: center; vertical-align: top;">
									<h4>forward (strong form)</h4>
									\begin{align*}
										&m = \gamma_p\\
										&\begin{cases}
											-\nabla \cdot{} m \nabla u^{(i)} = 0 \quad &\forall x \in \mathcal{R}\\
											u^{(i)} = g^{(i)} \quad &\forall x \in \partial\mathcal{R}
											\end{cases}\\
										&\Sigma^{(i, j)} = \int_{\mathcal{R}} h^{(j)} u^{(i)} dx\\
										&C = \frac{1}{2 IJ}\sum_{i, j=1}^{I,J} (\Sigma^{(i, j)} - \tilde{\Sigma}^{(i, j)})^2  
									\end{align*}
								</td>
								<td style="text-align: center; vertical-align: top;">
									<div class="r-stack">
										<ul class="fragment fade-out" data-fragment-index="1">
											<li>$\gamma: \mathbb{R}^N \to L^{(2)}(\mathcal{R})$</li>
											<li>$m, h^{(j)} \in L^2(\mathcal{R})$, $g^{(i)} \in L^2(\partial \mathcal{R})$</li>
											<li>weak enforcement of boundary conditions $u, v \in H^1_h(\mathcal{R})$</li>
											<li>Aubin-Nitsche trick: stable bilinear form
												\[a(u^{(i)}, v) + b^{(i)}(v) = 0 \quad \forall v \in H^1_h(\mathcal{R})\]
											</li>
											<li>$I, N \gg J$ (many bc $g^{(i)}$, many parameters $p \in \mathbb{R}^N$, some extractions $h^{(j)}$)</li>
											<li>here: $I=800$, $N=2971$, $J=7$</li>
										</ul>
										<div class="fragment fade-in" data-fragment-index="1">
											<h4>adjoint forward (strong form)</h4>
											\begin{align*}
												& m = \gamma_p\\ 
												&\begin{cases}
													-\nabla \cdot{} m \nabla \lambda^{(j)} &= -h^{(j)} &\forall x \in \mathcal{R} \\
													\lambda^{(j)} &= 0 \quad &\forall x \in \partial \mathcal{R}
												\end{cases}\\
												&\Sigma^{(i, j)} = \int_{\partial \mathcal{R}} \nabla_n \lambda^{(j)} g^{(i)} \, d x\\
												&C = \frac{1}{2 I J} \sum_{i, j}^{I, J} (\Sigma^{(i, j)} - \tilde{\Sigma}^{(i, j)})^2
											\end{align*}
										</div>
									</div>
								</td>
							</tr>
						</table>
						<div class="r-stack">
							<div class="fragment fade-out" data-fragment-index="1">
								<table style="text-align: center">
									<tr style="border-style: hidden;">
										<td style="text-align: center; "><h5>forward solution $u^{(i)}$</h5></td>
										<td style="text-align: center; "><h5>extraction $h^{(j)}$</h5></td>
										<td style="text-align: center; "><h5>measurement $\Sigma^{(i, j)}$</h5></td>
									</tr>
									<tr>
										<td style="text-align: center; ">
											<img src="figures/poisson/forward_solution/100.svg" width="600px"/>
										</td>
										<td style="text-align: center; ">
											<img src="figures/poisson/extractions/7.svg" width="600px"/>
										</td>
										<td style="text-align: center; ">
											<img src="figures/poisson/measurements/7.svg" height="600px"/>
										</td>
									</tr>
									<tr style="border-style: hidden;">
										<td style="text-align: center; "></td>
										<td style="text-align: center; "></td>
										<td style="text-align: center; ">$\hphantom{\sim 10 ns}$</td>
									</tr>
								</table>
							</div>
							<div class="fragment fade-in" data-fragment-index="1">
								<table style="text-align: center;">
									<tr style="border-style: hidden;">
										<td style="text-align: center; "><h5>forward solution $u^{(i)}$</h5></td>
										<td style="text-align: center; "><h5>extraction $h^{(j)}$</h5></td>
										<td style="text-align: center; ">
											<div class="r-stack">
												<div class="fragment fade-out" data-fragment-index="2">
													<h5>measurement $\Sigma^{(i, j)}$</h5>
												</div>
												<div class="fragment fade-in" data-fragment-index="2">
													<h5>measurement $\Sigma^{(i, j)}$ (naive)</h5>
												</div>
											</div>
										</td>
									</tr>
									<tr style="border-style: hidden;">
										<td style="text-align: center; ">
											<img src="figures/poisson/forward_solution/500.svg" width="600px"/>
										</td>
										<td style="text-align: center; ">
											<img src="figures/poisson/extractions/3.svg" width="600px"/>
										</td>
										<td style="text-align: center; ">
											<div class="r-stack">
												<div class="fragment fade-out" data-fragment-index="2">
													<img src="figures/poisson/all_measurements.svg" height="600px"/>
												</div>
												<div class="fragment fade-in" data-fragment-index="2">
													<img src="figures/poisson/all_measurements_forward.svg" height="600px"/>
												</div>
											</div>
										</td>
									</tr>
									<tr style="border-style: hidden;">
										<td style="text-align: center; ">
											<div class="fragment fade-in" data-fragment-index="1">
												$\sim 7ms$
											</div></td>
										<td style="text-align: center; "></td>
										<td style="text-align: center; ">
											<div class="r-stack">
												<div class="fragment fade-out" data-fragment-index="2" style="background-color: green;">
													$\sim 55ms \approx 7 \times 7ms$
												</div>
												<div class="fragment fade-in" data-fragment-index="2" style="background-color: red;">
													$\sim 5300ms \approx 800 \times 7ms$
												</div>
											</div></td>
									</tr>
								</table>
							</div>
						</div>
					</section>
					<section>
						<h3>Example: Derivation</h3>
						<h4>weak forms</h4>
						\begin{align*}
							a_m(u, v) &= \int_{\mathcal{R}} m \nabla u \cdot{} \nabla v dx - \int_{\partial \mathcal{R}} m \nabla_n u v + u \nabla_n v \, d\Gamma + \int_{\partial \mathcal{R}} \alpha u v \, d \Gamma \\
							b^{(i)}(v) &= \int_{\partial \mathcal{R}} \nabla_n v  g^{(i)} \, d\Gamma - \int_{\partial \mathcal{R}} \alpha g^{(i)} v \, d\Gamma\\
							c^{(j)}(u) &= \int_{\mathcal{R}} h^{(j)} u \, dx
						\end{align*}
						<h4>adjoint forward</h4>
						\begin{align*}
							a_m(u, \lambda^{(j)}) + c^{(j)}(u) = 0 \quad \forall u\\
							\int_{\mathcal{R}} m \nabla u \cdot{} \nabla \lambda^{(j)} dx - \int_{\partial \mathcal{R}} m \nabla_n u \lambda^{(j)} + u \nabla_n \lambda^{(j)} \, d\Gamma + \int_{\partial \mathcal{R}} \alpha u \lambda^{(j)} \, d \Gamma + \int_{\mathcal{R}} \mu^{(j)} u \, dx = 0 \quad \forall u 
						\end{align*}
						<h4>adjoint forward (strong form)</h4>
						\begin{align*}
							\begin{cases}
								-\nabla m \cdot{} \nabla \lambda^{(j)} = -h^{(j)} \quad &\forall x \in \mathcal{R}\\
								\lambda^{(j)} = 0 \quad &\forall x \in \partial \mathcal{R}
							\end{cases}
						\end{align*}
					</section>
					<section>
						<h3>Example: Derivation</h3>
						<h4>adjoint derivative</h4>
						\begin{align*}
							&a_m(\bar{\lambda}^{(j)}, \dot{\lambda}) + \bar{\boldsymbol \Sigma}^{(j)T} \boldsymbol{b}(\dot{\lambda}) = 0 \quad \forall \dot{\lambda} \\
							&\int_{\mathcal{R}} m \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \dot{\lambda} dx - \int_{\partial \mathcal{R}} m \nabla_n \bar{\lambda}^{(j)} \dot{\lambda} + \bar{\lambda}^{(j)} \nabla_n \dot{\lambda} \, d\Gamma + \int_{\partial \mathcal{R}} \alpha \bar{\lambda}^{(j)} \dot{\lambda} \, d \Gamma \\
							&+ \int_{\partial \mathcal{R}} \nabla_n \dot{\lambda}  \sum_{i=1}^{I}(\bar{\Sigma}^{(i, j)} g^{(i)}) \, d\Gamma - \int_{\partial \mathcal{R}} \alpha \sum_{i=1}^{I} (\bar{\Sigma}^{(i, j)} g^{(i)}) \dot{\lambda} \, d\Gamma = 0 \quad \forall \dot{\lambda}
						\end{align*}
						<h4>adjoint derivative (strong form)</h4>
						\begin{align*}
							\begin{cases}
								- \nabla m \cdot{} \nabla \bar{\lambda}^{(j)} = 0 \quad &\forall x \in \mathcal{R} \\
								\bar{\lambda}^{(j)} = \sum_{i=1}^{I} \bar{\Sigma}^{(i, j)} g^{(i)} \quad &\forall x \in \partial \mathcal{R} 
							\end{cases}
						\end{align*}
						<h4>tangent model</h4>
						\begin{align*}
							\dot{C} = \sum_{j=1}^{J} \dot{a}_m(\bar{\lambda}^{(j)}, \lambda^{(j)}, \dot{m}) = \sum_{j=1}^{J} \int_{\mathcal{R}} \dot{m} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} \, dx - \int_{\partial \mathcal{R}} \dot{m} \nabla_n \bar{\lambda}^{(j)} \underbrace{\lambda^{(j)}}_{=0} \, d \Gamma
						\end{align*}
						<h4>adjoint model</h4>
						\begin{align*}
							\langle \bar{C}, \sum_{j=1}^{J} \sum_{j=1}^{J} \int_{\mathcal{R}} \dot{m} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} \, dx \rangle = \langle \bar{C} \sum_{j=1}^{J} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)}, \dot{m} \rangle_{L^2(\mathcal{R})} \\
							\bar{a}_m(\bar{\lambda}^{(:)}, \lambda^{(:)}, \bar{C}) = \sum_{j=1}^{J} \bar{C} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} 
						\end{align*}
					</section>
					<section>
						<h3>Example: Derivation</h3>
						<h4>adjoint parametrization</h4>
						\begin{align*}
							\langle \bar{m}, \dot{m} \rangle_{L^2(\mathcal{R})} = \langle \bar{m}, \dot{\gamma}_p(\dot{p}) \rangle_{L^2(\mathcal{R})} = \langle \int_{\mathcal{R}} \frac{\partial \gamma_p}{\partial p} \bar{m} \, dx, \dot{p} \rangle_{\mathbb{R}} 
						\end{align*}
					</section>
				</section>

				<section style="text-align: left;">
					<h3>Example: An inverse problem based on the Poisson equation</h3>
					<table>
						<tr style="border-style: hidden;">
							<!-- <td style="text-align: center; "><h5>adjoint forward (strong form)</h5></td> -->
							<td></td>
							<td style="text-align: center; vertical-align: top; "><h5>adjoint derivative (strong form)</h5></td>
						</tr>
						<tr>
							<!-- <td style="text-align: center; ">
								\begin{align*}
									& m = \gamma_p\\ 
									&\begin{cases}
										-\nabla \cdot{} m \nabla \lambda^{(j)} &= -h^{(j)} &\forall x \in \mathcal{R} \\
										\lambda^{(j)} &= 0 \quad &\forall x \in \partial \mathcal{R}
									\end{cases}\\
									&\Sigma^{(i, j)} = \int_{\partial \mathcal{R}} \nabla_n \lambda^{(j)} g^{(i)} \, d x\\
									&C = \frac{1}{2 I J} \sum_{i, j}^{I, J} (\Sigma^{(i, j)} - \tilde{\Sigma}^{(i, j)})^2
								\end{align*}
							</td> -->
							<td style="text-align: left; vertical-align: top; ">
								<ul>
									<li>more text dsasdsadsadshere!</li>
								</ul>
							</td>
							<td style="text-align: center; vertical-align: top; ">
								\begin{align*}
									&\bar{\Sigma}^{(i, j)} = \frac{1}{IJ} (\Sigma^{(i, j)} - \tilde{\Sigma}^{(i, j)}) \bar{C}\\
									&\begin{cases}
										-\nabla \cdot{} m \nabla \bar{\lambda}^{(j)}  &= 0 \quad  &\forall x \in \mathcal{R} \\
										\bar{\lambda}^{(j)} &= \sum_{i=1}^{I} \bar{\Sigma}^{(i, j)} g^{(i)} \quad &\forall x \in \partial \mathcal{R}\\
									\end{cases} \\
									&\bar{m} = \bar{C} \sum_{j=1}^{J} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} \\
									& (\nabla_p C)^{(n)} = (\bar{\gamma}_{p}(\bar{m}))^{(n)} = \int_{\mathcal{R}} \frac{\partial \gamma_p}{\partial p^{(n)}} \bar{m} \, d x\\
								\end{align*}
							</td>
						</tr>
					</table>

					<table style="text-align: center; padding: 0px">
						<tr style="border-style: hidden;">
							<td style="text-align: center; padding: 0px"><h5>adjoint forward $\lambda^{(j)}$</h5></td>
							<td style="text-align: center; padding: 0px"><h5>adjoint derivative $\bar{\lambda}^{(j)}$</h5></td>
							<td style="text-align: center; padding: 0px">
								<div class="r-stack">
									<h5 class="fragment fade-out" data-fragment-index="2">gradient $\bar{m}$</h5>
									<h5 class="fragment fade-in" data-fragment-index="2">gradient $\bar{m}$ (finite differences)</h5>
								</div>
							</td>
						</tr>
						<tr style="border-style: hidden;">
							<td style="text-align: center; padding: 0px;">
								<img src="figures/poisson/forward_adjoint/7.svg" style="width: 600px;"/>
							</td>
							<td style="text-align: center; padding: 0px;">
								<img src="figures/poisson/derivative_adjoint/7.svg" style="width: 600px;"/>
							</td>
							<td style="text-align: center; padding: 0px;">
								<div class="r-stack">
									<div class="fragment fade-out" data-fragment-index="2" >
										<img src="figures/poisson/gradient.svg" style="width: 600px;"/>
									</div>
									<div class="fragment fade-in" data-fragment-index="2" >
										<img src="figures/poisson/gradient_fd.svg" style="width: 600px;"/>
									</div>
								</div>
							</td>
						</tr>
						<tr style="border-style: hidden;">
							<td style="text-align: center; padding: 0px;"></td>
							<td style="text-align: center; padding: 0px;"></td>
							<td style="text-align: center; padding: 0px;">
								<div class="r-stack" style="margin: -50px">
									<div class="fragment fade-out" data-fragment-index="2" style="background-color: green;">
										$\sim 92 ms \approx 2 \times 7 \times 7ms$
									</div>
									<div class="fragment fade-in" data-fragment-index="2" style="background-color: red;">
										$\sim 160 s \approx 2972 \times 55ms$
									</div>
								</div>
							</td>
						</tr>
					</table>
				</section>

				<section style="text-align: left;">
					<h3>Example: An inverse problem based on the Poisson equation</h3>
					\begin{align*}
						m = \gamma_p = x \mapsto (0.1, 0.9, 0.4)^T \cdot{} \mathcal{NN}^{2 \to 20 \text{(tanh)} \to 20\text{(tanh)} \to 3\text{(softmax)}}_p(x)
					\end{align*}
					<ul>
						<li>inverting on $m$ directly is ill-posed</li>
						<li>motivated from SciML: use a $\mathcal{NN}$ as the parametrization $\gamma_p$</li>
						<li>we use the parametrization $\gamma_p$ to regularize (here: only phase classification)</li>
						<li>number of parameters: $544$</li>
						<li>coarser discretization for the inversion ($757$ material cells vs $2972$ for the true measurements)</li>
						<li>$1$% random noise added to the true measurements</li>
					</ul>
					<table>
						<tr style="border-style: hidden;">
							<td style="text-align: center; " width="620px">
								<h5>measurements $\Sigma^{(i, j)}$ and $\tilde{\Sigma}^{(i, j)}$</h5>
							</td>
							<td style="text-align: center; " width="580px">
								<h5>optimized material $m$</h5>
							</td>
							<td style="text-align: center; " width="600px">
								<h5>objective $\text{MSE}(\Sigma^{(i, j)}, \tilde{\Sigma}^{(i,j)})$</h5>
							</td>
						</tr>
						<tr class="fragment highlight-red" data-fragment-index="1">
							<td style="text-align: center; vertical-align: top;">
								<video data-autoplay style="width=550px; margin-top: 20px;"> 
									<source data-src="figures/poisson/optimization_measurements.webm" type="video/webm">
								</video>
							</td>
							<td style="text-align: center; ">
								<div class="r-stack">
									<div class="fragment highlight-red" data-fragment-index="1">
										<video data-autoplay style="width: 600px;">
											<source data-src="figures/poisson/optimization_material.webm" type="video/webm">
										</video>
									</div>
									<div class="fragment fade-in" data-fragment-index="2">
										<video style="width: 600px;">
											<source data-src="figures/poisson/true_material.webm" type="video/webm">
										</video>
									</div>
								</div>
							</td>
							<td style="text-align: center; vertical-align: top;">
								<video data-autoplay style="width=550px; margin-top: 50px; "> 
									<source data-src="figures/poisson/optimization_error.webm" type="video/webm">
								</video>
							</td>
						</tr>
					</table>
					<ul>
						<li>taylor the parametrization $\gamma_p$ to a specific problem (layers, other geometry, phase values, e.g.)</li>
					</ul>
				</section>

				<section>
					<section style="text-align: left;">
						<h3>Outlook: Material Reconstruction in EPMA</h3>
						<ul>
							<li>model: (stationary) radiative transfer/linear Boltzmann equation in continuous-slowing-down approximation (CSD)</li>
							<li>adjoint model: reversed in time and direction(or space) $a_m(\psi, \phi)$ vs. $a_m(\phi, \psi)$</li>
						</ul>
						<table>
							<tr style="border-style: hidden;">
								<td style="text-align: center; ">
									<h4>forward $\psi^{(i)}$</h4>
								</td>
								<td style="text-align: center; ">
									<h4>adjoint forward $\lambda^{(j)}$</h4>
								</td>
								<td style="text-align: center; ">
									<h4>gradient $\bar{\rho}_e$</h4>
								</td>
							</tr>
							<tr style="border-style: hidden;">
								<td style="text-align: center; ">
									<video data-autoplay style="width: 600px; margin-top: 00px;">
										<source data-src="figures/adjoint_visualization/forward.webm" type="video/webm">
									</video>
								</td>
								<td style="text-align: center; ">
									<video data-autoplay style="width: 600px; margin-top: 00px;">
										<source data-src="figures/adjoint_visualization/adjoint.webm" type="video/webm">
									</video>
								</td>
								<td style="text-align: center; ">
									<img src="figures/adjoint_visualization/gradient.svg" style="width: 600px">
								</td>
							</tr>
						</table>
						<h4 style="margin-top: 100px;">References</h4>
						<div style="font-size: 60%;"> 
							<ul>
								<li>T. Bui-Tanh: <em>Adjoint and Its roles in Sciences, Engineering, and Mathematics: A Tutorial</em>. arXiv (2023)<br><a href="https://doi.org/10.48550/arXiv.2306.09917">doi:10.48550/arXiv.2306.09917</a></li>
								<li>J.A. Halbleib and J.E. Morel: <em>Adjoint Monte Carlo Electron Transport in the Continuous-Slowing-Down-Approximation</em>. J. Comput. Phys. (1980)<br><a href="https://doi.org/10.1016/0021-9991(80)90106-0">doi:10.1016/0021-9991(80)90106-0</a></li>
								<li>U. Naumann: <em>The Art of Differentiating Computer Programs: An Introduction to Algorithmic Differentiation</em>. SIAM (2012)<br><a href="https://doi.org/10.1137/1.9781611972078">doi:10.1137/1.9781611972078</a></li>
								<li>R.E. Plessix: <em> A review of the adjoint-state method for computing the gradient of a functional with geophysical applications</em>. Geophys. J. Int. (2006)<br><a href="https://doi.org/10.1111/j.1365-246X.2006.02978.x">doi:10.1111/j.1365-246X.2006.02978.x</a></li>
							</ul>
						</div>
					</section>

					<section style="text-align: left;">
						<h3>Material Reconstruction in EPMA</h3>
						<ul>
							<li>
								(stationary) radiative transfer/linear boltzmann equation in continuous-slowing-down approximation (CSD) ($x \in \mathcal{R} \subset \mathbb{R}^3$, $\epsilon \in \mathbb{R}^+$, $\Omega \in S^2$)
								\begin{align}
									-\partial_\epsilon (s(x, \epsilon) \psi^{(i)}(x, \epsilon, \Omega)) + \Omega \cdot{} \nabla_x \psi^{(i)}(x, \epsilon, \Omega) = \int_{S^2} k(x, \epsilon, \Omega \cdot{} \Omega') \psi^{(i)}(x, \epsilon, \Omega') \, d\Omega' - \tau(x, \epsilon) \psi^{(i)}(x, \epsilon, \Omega)
								\end{align}
							</li>
							<li>
								beam is modeled by boundary conditions (excitations)
								\begin{align}
									\psi^{(i)}(x, \epsilon, \Omega) = g^{(i)}(x, \epsilon, \Omega) \quad \forall x \in \partial \mathcal{R} | n \cdot{} \Omega < 0 
								\end{align}
							</li>
							<li>$i = 1, ..., I$: beam position, beam energy, beam direction</li>
							<li>
								additivity approximation (for $\sigma = s$, $k$ and $\tau$), $p \in \mathbb{R}^N$
								\begin{equation}
									\sigma(x, \cdot{}) = \sum_{e=1}^{n_e} \rho_{e}(x) \sigma_e(\cdot{})\quad \text{where} \quad (\rho_1, ... \rho_{n_e})^T = \gamma_p
								\end{equation}
							</li>
							<li>
								x-ray generation and detection (extraction)
								\begin{align}
									\Sigma^{(i, j)} = \int_{\mathcal{R}} \int_{0}^{\infty} \sigma^{ion}_j(\epsilon) \rho_j(x) e^{-\int_{d(x)} \sum_{e=1}^{n_e} \mu_e^{(j)} \rho_e(y) \,d y}\int_{S^2} \psi^{(i)}(x, \epsilon, \Omega) \,d \Omega \, d \epsilon \,d  x

								\end{align}
							</li>
							<li>$j = 1, ..., J$: number of measured x-ray lines/materials</li>
						</ul>
					</section>

					<section style="text-align: left;">
						<h3>Material Reconstruction in EPMA</h3>
						<ul>
							<li>compute measurements $\Sigma^{(i, j)}$ with the extraction as the source to the adjoint RT-CSD</li>
							<li>adjoint RT-CSD: reversed in energy and direction(or space) $a_m(\psi, \phi)$ vs. $a_m(\phi, \psi)$</li>
							<table>
								<tr style="border-style: hidden;">
									<td style="text-align: center; ">
										<h4>forward $\psi^{(i)}$</h4>
									</td>
									<td style="text-align: center; ">
										<h4>adjoint forward $\lambda^{(j)}$</h4>
									</td>
								</tr>
								<tr style="border-style: hidden;">
									<td style="text-align: center; ">
										<video data-autoplay style="width: 800px; margin-top: 40px;">
											<source data-src="figures/adjoint_visualization/forward.webm" type="video/webm">
										</video>
									</td>
									<td style="text-align: center; ">
										<video data-autoplay style="width: 800px; margin-top: 40px;">
											<source data-src="figures/adjoint_visualization/adjoint.webm" type="video/webm">
										</video>
									</td>
								</tr>
								<tr style="border-style: hidden;">
									<td style="text-align: center; ">
										cost: $I \times \mathcal{C}(a) \sim I \times 2\text{min}$
									</td>
									<td style="text-align: center; ">
										cost: $J \times \mathcal{C}(a) \sim J \times 2\text{min}$
									</td>
								</tr>
							</table>
						</ul>
						<table>
							<tr>
								<td style="width:800px; vertical-align: top;"> 
									<ul>
										<li>orange/green marker: measurements from forward</li>
										<li>green lines: measurements from adjoint forward</li>
									</ul>
								</td>
								<td>
									<img src="figures/adjoint_visualization/measurements.svg" style="width: 800px;">
								</td>
							</tr>
						</table>
					</section>

					<section style="text-align: left;">
						<h3>Material Reconstruction in EPMA</h3>
						<ul>
							<li>compute the gradient $\bar{\rho}_e$ using the "adjoint augmented" excitations as the source to the RT-CSD</li>
						</ul>
						<table>
							<tr style="border-style: hidden;">
								<td style="text-align: center; "><h4>adjoint derivative $\bar{\lambda}^{(j)}$</h4></td>
								<td style="text-align: center; "><h4>gradient $\bar{\rho}_e$</h4></td>
							</tr>
							<tr style="border-style: hidden;">
								<td style="text-align: center; ">
									<video data-autoplay style="width: 800px; margin-top: 40px;">
										<source data-src="figures/adjoint_visualization/adjoint_derivative_1.webm" type="video/webm">
									</video>
								</td>
								<td style="text-align: center; ">
									<img src="figures/adjoint_visualization/gradient.svg" style="width: 800px">
								</td>
							</tr>
						</table>
					</section>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/

			Reveal.configure({pdfSeparateFragments: true});
			Reveal.initialize({
				hash: true,
				controls: true, 
				controlsLayout: 'bottom-right',
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: 1920,
				height: 1440,

				// Factor of the display size that should remain empty around
				// the content
				margin: 0.1,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.01,
				maxScale: 2.0,
				center: false,

				transition: 'none',
				backgroundTransition: 'fade-in',

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
			});
		</script>
	</body>
</html>
