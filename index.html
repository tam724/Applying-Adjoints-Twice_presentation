<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Appying Adjoints Twice</title>

		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300..700&display=swap" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&family=Quicksand:wght@300..700&display=swap" rel="stylesheet">
		
		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/serif.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
		<style>
			.fragment.blur {
				filter: blur(10px);
			}
			.fragment.blur.visible {
			  filter: none;
			}
			.section {
				text-align: left;
			}
			.reveal .slides{
				border-style: solid;
				border-color: gray;
				border-width: 1px;
			}
			.tr {
				border-style: none;
			}
		  </style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Title frame -->
				<section class="center">
					<div class="r-stretch"></div> 
					<h3 style="margin-bottom: 100px;">
						Applying Adjoints Twice: Efficient Gradients for Inverse Modeling with Radiative Transfer in EPMA
					</h3>
					<p style="margin-bottom:50px; "><strong>Tamme Claus</strong><sup>1</sup>, Gaurav Achuda<sup>2</sup>, Silvia Richter<sup>2</sup>, Manuel Torrilhon<sup>1</sup></p>
					<p><sup>1</sup> ACoM, Applied and Computational Mathematics, RWTH Aachen University</p>
					<p><sup>2</sup> GFE, Central Facility for Electron Microscopy, RWTH Aachen University</p>
					<p style="margin-top:50px; "><em>IGPM Seminar 2025, RWTH Aachen</em></p>
					<p style="margin-top:50px; "><em>23.01.2025</em></p>
					<div style=" margin-top:300px; text-align: right;">
						<img src="figures/gfe_logo.svg" height="290px"> 
						<img src="figures/rwth_acom_en_cmyk.svg" height="300px">
					</div>
				</section>
				
				<section>
					<!-- Material Reconstruction in EPMA-->
					<section  style="text-align: left">
						<h3>Motivation: Electron Probe Microanalysis (EPMA)</h3>
						<div style="display:flex;">
							<div style="flex:1; margin-right: 40px; margin-top:-20px;">
								<p style="text-align: center; "><em>"Material imaging based on characteristic x-ray emission"</em></p>
								<ul style="margin-top: 0px;">
									<li>Material Science, Quality control, Mineralogy, Semiconductors</li>
									<li>Ionization of material sample using high-energy focused electron beam</li>
									<li>Wavelength-dispersive spectrometers measure characteristic x-radiation</li>

								</ul>
							</div>
							<div style="flex:1; text-align: center;">
								<img src="figures/epma.jpg" height="400px">
								<p style="font-size:60% ; margin-top: -20px;">Microprobe at GFE (source: <a href="https://www.gfe.rwth-aachen.de/cms/gfe/dienste/~rhxt/esma/?lidx=1">gfe.rwth-aachen.de</a>)</p>
							</div> 
						</div>
						<div style="display:flex;">
							<div class="r-stack" style="flex:2; text-align: center;">
								<div class="fragment fade-in-then-out" data-fragment-index="1">
									<img src="figures/01_material.jpg" alt="Material" width="700px" style="margin-top: 60px;">
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="2">
									<video loop data-autoplay width="700px" style="margin-top: 40px;">
										<source data-src="figures/02_forward.webm" type="video/webm">
									</video>
								</div>
								<!-- <div class="fragment fade-in-then-out" data-fragment-index="3">
									<video loop data-autoplay  width="700px" style="margin-top: 40px;">
										<source data-src="figures/03_ionization.webm" type="video/webm">
									</video>
								</div> -->
								<div class="fragment fade-in-then-out" data-fragment-index="3">
									<img src="figures/04_interaction_volume.jpg" alt="Interaction Volume"  width="700px" style="margin-top: 60px;">
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="4">
									<video loop data-autoplay  width="700px" style="margin-top: 40px;">
										<source data-src="figures/05_linescan.webm" type="video/webm">
									</video>
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="5">
									<video loop data-autoplay width="700px" style="margin-top: 40px;">
										<source data-src="figures/06_linescan.webm" type="video/webm">
									</video>
								</div>
								<div class="fragment fade-in" data-fragment-index="6">
									<video data-autoplay width="700px" style="margin-top: 40px;">
										<source data-src="figures/07_linescan.webm" type="video/webm">
									</video>
								</div>
							</div>
							<div style="flex:3">
								<ul>
									<li class="fragment fade-in" data-fragment-index="1">Example Material: two-phase (<span style="color: orange">A</span>, <span style="color: green;">B</span>)</li>
									<li class="fragment fade-in" data-fragment-index="2"><span style="color:blue">Electron</span> propagation affected by scattering (animated: different energies) </li>
									<li class="fragment fade-in" data-fragment-index="3">Ionization and emission of characteristic x-rays (wavelength of <span style="color:orange">A-rays</span> and <span style="color:green">B-rays</span> differ)</li>
									<!-- <li class="fragment fade-in" data-fragment-index="4">Measure wavelength-dispersive x-ray intensity</li> -->
									<li class="fragment fade-in" data-fragment-index="4">Spatial information gained via different beam positions (linescan), <span class="fragment fade-in" data-fragment-index="5">beam angles </span> <span class="fragment fade-in" data-fragment-index="6">and beam energies</span></li>
									<li class="fragment fade-in" data-fragment-index=7"><em>inverse problem of material reconstruction</em>
										<span class="fragment fade-in" data-fragment-index="7">
											\begin{equation}
												p^* = \underset{p \in P}{\text{arg min}} \underbrace{|| \Sigma(p)  - \tilde{\Sigma} ||^2}_{=C(p)}
											\end{equation}
										</span>
									</li>
									<li class="fragment fade-in" data-fragment-index="8">gradient-based iterative methods (Steepest Descent, BFGS, or HMC)</li>
									<li class="fragment fade-in" data-fragment-index="9"><em>we focus on computing $\nabla_p C(p)$ efficiently via adjoints</em></li>
								</ul>
							</div>
						</div>
					</section>
					
					<!-- Gradients in Inverse Problems-->
					<section style="text-align: left;">
						<h3>Gradients in Inverse Problems</h3>
						<span><em>"Determine the model parameters $p$ such that the model $\Sigma(p)$ produces the observed data $\tilde{\Sigma}$."</em></span>
						<div style="display:flex;">
							<div style="flex:2 ;">
								<h4 style="margin-top: 50px;">classical approach</h4>
								\begin{align*}
									p^* = \underset{p \in P}{\text{arg min}} || \Sigma(p)  - \tilde{\Sigma} ||^2
								\end{align*}
								<ul>
									<li>iterative approximation of the minimum based on the gradient of the objective function $p \mapsto ||\Sigma(p) - \tilde{\Sigma}||^2$</li>
									<li>Gradient Descent, CG, BFGS, ...</li>
									<li>regularization: addition of a penalty term $p \mapsto ||\Sigma(p) - \tilde{\Sigma}||^2 + \mathcal{R}(p)$</li>
									<li>we will do: $\Sigma = \Sigma \circ \gamma$</li>
								</ul>
							</div>
							<div style="flex:1; ">
								<img src="figures/gd.png" width="500px">
							</div>
						</div>
						<div style="display:flex;">
							<div style="flex:1; ">
								<img src="figures/hmc.png" width="500px">
							</div>
							<div style="flex:2">
								<h4 style="margin-top: 50px;">bayesian approach</h4>
								<ul>
									<li>approximate the posterior $\pi(p|\tilde{\Sigma})$ (typically: iterative sampling, MCMC)</li>
									<li>Hamiltonian Monte-Carlo (based on the gradient of the target density)</li>
								</ul>
							</div>
						</div>
						</ul>
						<p style="text-align: center;"><em>Efficient evaluation of the objective function and its gradient is crucial!</em></p>
					</section>
				</section>

				<!-- Matrix Product Bracketing -->
				<section style="text-align: left">
					<h3>
						Motivation: Matrix Product Bracketing
					</h3>
					Consider Matrix Multiplication ($N \gg I, J$):
					<div class="r-stack">
						<span class="fragment fade-out" data-fragment-index="1">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ji)} &   \\
							&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
							\color{white}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & h^{(1)} & — \\
							— & h^{(j)} & — \\
							— & h^{(J)} & — \\
							\end{pmatrix}}_{J \times N} 
							\cdot{}
							\color{white}{\Bigg(}
							\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
							\color{white}{\Bigg)
							}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								g^{(1)} &  g^{(i)} & g^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{white}{\Bigg)}
						\]</span>
						<span class="fragment fade-in-then-out" data-fragment-index="1">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ji)} &   \\
							&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
							\color{blue}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & h^{(1)} & — \\
							— & h^{(j)} & — \\
							— & h^{(J)} & — \\
							\end{pmatrix}}_{J \times N} 
							\cdot{}
							\color{white}{\Bigg(}
							\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
							\color{blue}{\Bigg)
							}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								g^{(1)} &  g^{(i)} & g^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{white}{\Bigg)}
						\]</span>
						<span class="fragment fade-in-then-out" data-fragment-index="2">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ji)} &   \\
							&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
							\color{white}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & h^{(1)} & — \\
							— & h^{(j)} & — \\
							— & h^{(J)} & — \\
							\end{pmatrix}}_{J \times N} 
							\cdot{}
							\color{red}{\Bigg(}
							\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
							\color{white}{\Bigg)
							}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								g^{(1)} &  g^{(i)} & g^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{red}{\Bigg)}
						\]</span>
						<span class="fragment fade-in" data-fragment-index="3">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ji)} &   \\
							&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
							\color{blue}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & h^{(1)} & — \\
							— & h^{(j)} & — \\
							— & h^{(J)} & — \\
							\end{pmatrix}}_{J \times N} 
							\cdot{}
							\color{red}{\Bigg(}
							\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
							\color{blue}{\Bigg)
							}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								g^{(1)} &  g^{(i)} & g^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{red}{\Bigg)}
						\]</span>
						<!-- <span style="background-color:#fff;" class="fragment fade-in" data-fragment-index="6">\[
							\underbrace{\begin{pmatrix}
							&   &   \\
							& \Sigma^{(ij)} &   \\
							&   &   \end{pmatrix}^{\color{black}{T}}}_{J \times I} = 
							\color{white}{\Bigg(}
							\underbrace{\begin{pmatrix}
							— & g^{(1)} & — \\
							— & g^{(j)} & — \\
							— & g^{(J)} & — \\
							\end{pmatrix}}_{J \times N}
							\cdot{}
							\color{white}{\Bigg(}
							\underbrace{\boldsymbol{A}^*}_{N \times N}
							\color{white}{\Bigg)}
							\cdot{}
							\underbrace{
							\begin{pmatrix}
								| & | & | \\
								h^{(1)} &  h^{(i)} & h^{(I)} \\
								| & | & | \\
							\end{pmatrix}}_{N \times I}
							\color{white}{\Bigg)}
						\]</span> -->
					</div>
					<h4 style="display:inline;">Where to bracket?</h4>
					<span style="text-align: center" class="fragment fade-in" data-fragment-index="3">
						computational costs: $\color{red}{I\times N^2 + IJ\times N} \text{ or } \color{blue}{J\times N^2 + IJ\times N}$
					</span>
					<span class="fragment fade-in" data-fragment-index="4">
						<h4>Generalizations: </h4>
						<ul>
							<li>
								$\boldsymbol A$ is solution of a linear system / linear (partial?) differential equation?
								\[
									\boldsymbol A g^{(i)} = \{u \in U \, | \, a(u, v) + \langle g^{(i)}, v \rangle = 0 \quad \forall v \in V\}
								\]
							</li>
							<li>
								$\boldsymbol A$ is a (Frechet-) derivative (structure: chain rule, product rule, ...)?
								\[
									\boldsymbol A g^{(i)} = \frac{\partial f(y(x))}{\partial x} g^{(i)} = \frac{\partial f(y(x))}{\partial y}\frac{\partial y(x)}{\partial x}g^{(i)}
								\]
							</li>
						</ul>
					</span>
					<span class="fragment fade-in" data-fragment-index="5">
						<p><h4 style="display:inline;">Two equivalent formulations</h4> using the <em>adjoint</em> operator $\boldsymbol A ^*$</p>
							\[
							\underbrace{\Sigma}_{J \times I} = \underbrace{H}_{J \times \infty} \cdot{}  \underbrace{\boldsymbol A}_{\infty \times \infty} \cdot{} \underbrace{G}_{\infty \times I} \quad \Leftrightarrow \quad \underbrace{\Sigma^T}_{I \times J} = \underbrace{G^T}_{I \times \infty} \cdot{}  \underbrace{\boldsymbol A^*}_{\infty \times \infty} \cdot{} \underbrace{H^T}_{\infty \times J}
							\]
						
					</span>
					<!-- <div class="r-stretch"></div>
					<ul style="font-size:60%; ">
						<li>Naumann, U. (2020). <b>Optimization of Generalized Jacobian Chain Products without Memory Constraints</b>. <a href="https://doi.org/10.48550/ARXIV.2003.05755">doi:10.48550/ARXIV.2003.05755</a></li>
						<li>Claus, T. (2020). <b>Optimization of Sparse Generalized Jacobian Chain Products</b> <a href="https://www.acom.rwth-aachen.de/media/claus_opti_sparse_jac_chain_2020_SS.pdf">(unpublished)</a></li>
					</ul> -->
				</section>

				<!-- An abstract adjoint method -->
				<section style="text-align: left;">
					<section style="text-align: left";>
						<h3>An Abstract Adjoint Method in a Computational Context</h3>
						<p>
							Definition of the <b>Adjoint</b> ($H, G$ Hilbert spaces, $A: G \to H$ continuous, linear)
							\[
								\langle h, A(g) \rangle_H = \langle A^*(h), g \rangle_G \quad \forall h \in H \,,  g \in G
							\]
						</p>
						<div class="fragment fade-in">
							<ul>
								<li>given multiple $g^{(1)}, ... g^{(I)} \in G$ and $h^{(1)}, ... h^{(J)} \in H$ we want to compute</li>
							</ul>
							\[
							\Sigma^{(ji)} = \langle h^{(j)}, A(g^{(i)}) \rangle_H = \langle A^*(h^{(j)}), g^{(i)} \rangle_G \quad \forall i=1, ...I\, j=1, ...J
							\]
							<div style="display:flex; justify-content: center;">
								<div style="flex:1; display: inline-block; max-width: max-content; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; margin-right:50px; padding:20px;">
									<h4>Non-adjoint Implementation</h4>
									\begin{align*}
										&\text{foreach } \color{orange}{i = 1, ..., I} \text{ do} \\
										&\qquad \color{orange}{v^{(i)} \leftarrow A(g^{(i)})}\\
										&\qquad \text{foreach } \color{green}{j = 1, ..., J} \text{ do}\\
										&\qquad \qquad \Sigma^{(ji)} \leftarrow \langle \color{green}{h^{(j)}}, \color{orange}{v^{(i)}} \rangle_H\\
										&\qquad \text{end}\\
										&\text{end}
									\end{align*}
									<ul>
										<li>cost: $\color{orange}{I} \times \mathcal{C}(A) + \color{orange}{I}\color{green}{J} \times \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_H)$</li>
									</ul>
								</div>
								<div style="flex:1; display: inline-block; max-width: max-content; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed;  margin-left:50px; padding:20px;"> 
									<h4>Adjoint Implementation</h4>
									\begin{align*}
										&\text{foreach } \color{green}{j = 1, ..., J} \text{ do} \\
										&\qquad \color{green}{\lambda^{(j)} \leftarrow A^*(h^{(j)})}\\
										&\qquad \text{foreach } \color{orange}{i = 1, ..., I} \text{ do}\\
										&\qquad \qquad \Sigma^{(ji)} \leftarrow \langle \color{green}{\lambda^{(j)}}, \color{orange}{g^{(i)}} \rangle_G\\
										&\qquad \text{end}\\
										&\text{end}
									\end{align*}
									<ul>
										<li>cost: $\color{green}{J} \times \mathcal{C}(A^*) + \color{orange}{I}\color{green}{J} \times \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_G)$
										</li>
									</ul>
								</div>
							</div>
						</div>
						<div class="fragment fade-in" style="margin-top:30px;">
							<ul>
								<li>typically: $\mathcal{C}(A) \approx \mathcal{C}(A^*)$ and $\mathcal{C}(\langle \cdot{}, \cdot{} \rangle_G) \approx \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_H)$ and $\mathcal{C}(A) \gg \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_G)$ </li>
								<ul>
									<li>$\color{green}{J} > \color{orange}{I}$: non-adjoint implementation</li>
									<li>$\color{orange}{I} > \color{green}{J}$: adjoint implementation</li>
								</ul>
							</ul>
						</div>
						<div class="fragment fade-in" style="background-color: lightgray; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 5px; font-size: 100%; margin:30px;"> 
							<em>Instead of computing/approximating the "solution" $v^{(i)}= A(g^{(i)})$, we approximate the <b>Riesz representation</b> $\lambda^{(j)}$ of $\cdot \mapsto \langle h^{(j)}, A(\cdot{}) \rangle_H$</em>
						</div>
						<!-- <div style="background-color: lightgray; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 5px; font-size: 80%;">
							Notation: $f_{(\cdot{})}: X \to Y$ non-linear, differentiable
							<ul>
								<li><em>tangent operator</em> $\dot{f}_x(\dot{x}) = \lim_{h \to 0} \frac{f_{x+h\dot{x}} - f_x}{h}$</li>
								<li><em>adjoint operator</em> $\bar{f}_x: \langle \bar{y}, \dot{f}_x(\dot{x}) \rangle_Y = \langle \bar{f}_x(\bar{y}), \dot{x}\rangle_X \, \forall \bar{y}, \dot{x}$</li>
								<li><em>tangent variables</em> $\dot{x}, \dot{y}$, where $\dot{y} = \dot{f}_x(\dot{x})$</li>
								<li><em>adjoint variables</em> $\bar{x}, \bar{y}$, where $\bar{x} = \bar{f}_x(\bar{y})$</li>
							</ul>
						</div> -->
					</section>

				<!-- Riesz Representation Theorem/Def Adjoint -->
				<section style="text-align: left;">
					<h3>Riesz Representation Theorem/Definition of the Adjoint</h3>

					<h4>Riesz Representation Theorem</h4>
					<p>
						Let $(H, \langle \cdot{}, \cdot{} \rangle_H)$ be a (real) Hilbert space with inner product $\langle \cdot{}, \cdot{} \rangle_H$. For every continuous linear functional $f_h \in H^*$ (the dual of $H$), there exists a unique vector $h \in H$, called the <em>Riesz Representation</em> of $f_h$, such that
						\[
							f_h(x) = \langle h, x \rangle_H \quad \forall x \in H.
						\]
					</p>

					<h4>Definition: Adjoint Operator</h4>
					<p>
						Let $(G, \langle \cdot{}, \cdot{} \rangle_G)$ be another (real) Hilbert space, $A:G\to H$ a continuous linear operator between $G$ and $H$ and $f_h \in  H^*$ a continuous linear functional.
						\[
							f_h(A(g)) = \langle h, A(g) \rangle_H \quad \forall g \in G
						\]
						But $f_h(A(g))$ is also a continuous linear functional $f_\lambda(g)$ in $G$ with a Riesz Representation $\lambda \in G$
						\[
							f_h(A(g)) = f_\lambda(g) = \langle \lambda, g \rangle_G := \langle A^*(h), g \rangle_G.
						\]
					</p>
				</section>
					<section style="text-align: left;">
						<h3>Examples of (real) adjoint operators</h3>
						<div>
							<h4 style="color:forestgreen;">Scalar Multiplication</h4>
							<ul>
								<li>$G, H = \mathbb{R}$</li>
								<li>$A: \mathbb{R} \to \mathbb{R}, g \mapsto Ag = a \cdot{} g,\quad  a \in \mathbb{R}$</li>
								<li>$A^*: \mathbb{R} \to \mathbb{R}, \mu \mapsto A^* \mu = a \cdot{} \mu$</li>
							</ul>
							<p>
								<em>Derivation:</em>
								\begin{equation*}
									\langle \mu, Ag \rangle_{\mathbb{R}} = \langle \mu, a \cdot{} g \rangle_{\mathbb{R}} = \langle a \cdot{} \mu, g \rangle_{\mathbb{R}} = \langle A^*\mu , g \rangle_{\mathbb{R}} \quad \forall g \in \mathbb{R}\, \forall \mu \in \mathbb{R}
								\end{equation*}
							</p>
						</div>
						<div class="fragment fade-in">
							<h4 style="color:forestgreen;">Matrix Multiplication</h4>
							<ul>
								<li>$G = \mathbb{R}^n, H = \mathbb{R}^m$</li>
								<li>$A: \mathbb{R}^n \to \mathbb{R}^m, g \mapsto Ag = M \cdot{} g, \quad M \in \mathbb{R}^{m \times n}$</li>
								<li>$A^*: \mathbb{R}^m \to \mathbb{R}^n, \mu \mapsto A^*\mu = M^T \cdot{} \mu$</li>
							</ul>
							<p>
								<em>Derivation:</em>
								\begin{equation*}
									\langle \mu, Ag \rangle_{\mathbb{R}^m} = \langle \mu, M \cdot{} g \rangle_{\mathbb{R}^m} = \langle M^T \cdot{} \mu, g \rangle_{\mathbb{R}^n} = \langle A^*\mu, g \rangle_{\mathbb{R}^n} \quad \forall g \in \mathbb{R}^n \, \forall \mu \in \mathbb{R}^m
								\end{equation*}
							</p>
						</div>
						<div class="fragment fade-in">
							<h4 style="color:forestgreen;">Integration</h4>
							<ul>
								<li>$G = L^2(\Omega)$ (with $\langle \cdot{}, \cdot{} \rangle_{L^2(\Omega)} = \int_{\Omega} \cdot{} \cdot{} d x$), $H = \mathbb{R}$</li>
								<li>$A: L^2(\Omega) \to \mathbb{R}, g(\cdot{}) \mapsto Ag = \int_{\Omega} g(x) d x$</li>
								<li>$A^*: \mathbb{R} \to L^2(\Omega), \mu \mapsto (A^* \mu)(\cdot{}) = (x \mapsto \mu \cdot{} 1_{\Omega}(x))$</li>
							</ul>
							<p>
								<em>Derivation:</em>
								\begin{equation*}
									\langle \mu, Ag \rangle_\mathbb{R} = \mu \cdot{} \int_\Omega g(x) dx = \int_\Omega \mu \cdot{} 1_\Omega(x) g(x) dx = \langle (A^*\mu)(\cdot{}), g(\cdot{}) \rangle_{L^2(\Omega)} \quad \forall g \in L^2(\Omega) \, \forall \mu \in \mathbb{R}
								\end{equation*}
							</p>
						</div>
					</section>
					<section style="text-align: left;">
						<h3>Examples of (real) adjoint operators</h3>
						<h4 style="color:forestgreen;">Linear Solve</h4>
						<ul>
							<li>$G, H = \mathbb{R}^n$ and $M \in \mathbb{R}^{n\times n}$ invertible</li>
							<li>$A: \mathbb{R}^n \to \mathbb{R}^n, g \mapsto (h \in \mathbb{R}^n | M \cdot{} h = g)$, (or $A = M^{-1}$)</li>
							<li>$A^*: \mathbb{R}^n \to \mathbb{R}^n, \mu \mapsto (\lambda \in \mathbb{R}^n | M^T \cdot{} \lambda = \mu)$, (or $A^* = M^{-T}$)</li>
						</ul>
						<p>
							<em>Derivation (intentionally complicated):</em> We reinterpret
							\begin{equation*}
								M \cdot{} h = g \quad \Leftrightarrow \quad \langle v,  M \cdot{} h \rangle = \langle v, g \rangle \quad \forall v \in G 
							\end{equation*}
							in particular (for a fixed but unspecified $\lambda \in G$)
							\begin{equation}
								0 = \langle \lambda, g \rangle - \langle \lambda, M \cdot{} h \rangle \quad \Leftrightarrow \quad 0 = \langle \lambda, g \rangle - \langle M^T \cdot{} \lambda, h \rangle
							\end{equation}
							\begin{align*}
								\langle \mu, Ag \rangle &= \langle \mu, h \rangle\\
								&= \langle \mu, h \rangle - \langle M^T \cdot{} \lambda, h \rangle + \langle \lambda, g \rangle \\
								& \quad \text{ with } \langle \mu, h \rangle - \langle M^T \cdot{} \lambda, h \rangle = 0 \quad \forall h \in H \\
								& \quad \text{ or } M^T \cdot{} \lambda = \mu \\
								&= \langle A^* \mu, g \rangle
							\end{align*}
							(alternatively)
							\begin{equation*}
								\langle \mu, Ag \rangle_{\mathbb{R}^n} = \langle \mu, M^{-1} g \rangle_{\mathbb{R}^n} = \langle M^{-T} \mu, g \rangle_{\mathbb{R}^n} = \langle A^*\mu, g \rangle_{\mathbb{R}^n}
							\end{equation*}
						</p>
					</section>

					<section style="text-align: left;">
						<h3>Examples of (real) adjoint operators</h3>
						<h4 style="color:forestgreen;">Weak form</h4>
						<ul>
							<li>$G, H$ (real Hilbert spaces)</li>
							<li>$A: G \to H, g \mapsto \{h \in H | a(h, v) + \langle g, v \rangle_G = 0 \quad \forall v \in G\}$, $\quad a(\cdot{}, \cdot{}) \text{ coercive bilinear form}$</li>
							<li>$A^*: H \to G, \mu \mapsto \{\lambda \in G | a(v, \lambda) + \langle \mu,v \rangle_H = 0 \quad \forall v \in H\}$</li>
						</ul>
						<p>
							<em>Derivation:</em>
							\begin{align*}
								\langle \mu, Ag \rangle_H = f_\mu(h) &= f_\mu(h) + \underbrace{a(h, \lambda) + f_g(\lambda)}_{=0\quad \forall \lambda \in G} \\
								&= \underbrace{f_\mu(h) + a(h, \lambda)}_{!= 0 \quad \forall h \in H} + f_g(\lambda) = f_g(\lambda) = \langle \lambda, g \rangle_G
							\end{align*}
						</p>
					</section>

					<section style="text-align: left;">
						<h3>Derivative Notation (from Algorithmic Differentiation)</h3>
						<ul>
							<li>given $f_{(\cdot{})}: X \to Y, x \mapsto f_x$ ($X, Y$ Hilbert) non-linear, Frechet-differentiable</li>
						</ul>
						<p>
							we define:
						</p>	
						<ul>
							<li>the (continuous, linear) <em>tangent operator</em> $\dot{f}_x \in L(X, Y)$ (directional derivative)
								\begin{align*}
									\dot{f}_x(\dot{x}) = \frac{\partial f_x}{\partial x}(\dot{x}) = \lim_{h \to 0} \frac{f_{x + h\dot{x}} - f_{x}}{h}
								\end{align*}
							</li>
							<li>the (continuous, linear) <em>adjoint operator</em> $\bar{f}_x \in L(Y, X)$
								\begin{align*}
									\langle \bar{y} , \dot{f}_x(\dot{x}) \rangle_Y = \langle \bar{f}_x (\bar{y}), \dot{x} \rangle_X \quad \forall \bar{y} \in Y, \dot{x} \in X
								\end{align*}
							</li>
						</ul>
						<p>
							Also we define ($y = f_x$):
							<ul>
								<li><em>tangent variables</em> $\dot{x} \in X, \dot{y} \in Y$: given $\dot{x} \in X$, $\dot{y} = \dot{f}_x(\dot{x})$</li>
								<li><em>adjoint variables</em> $\bar{x} \in X, \bar{y} \in Y$: given $\bar{y} \in Y$, $\bar{x} = \bar{f}_x(\bar{y})$</li>
							</ul>
						</p>
						Applying this to a composition $ f_x = \varphi^{(N)}_{v^{(N-1)}} \circ \, ...\, \circ\, \varphi^{(1)}_x$ where $\varphi^{(n)}$ are single assignments (chain rule!) is tangent/adjoint mode automatic differentiation. (neglecting all implementational details..)
					</section>

					<section style="text-align: left;">
						<h3>Automatic/Algorithmic Differentiation</h3>
						<h4>Example</h4>
						\begin{equation}
							y = f_x = g \circ h_x \quad v = h_x
						\end{equation}
						by the chain rule, the <em>tangent operator</em> $\dot{f}_x$ is
						\begin{equation}
							\dot{y} = \dot{f}_x(\dot{x}) = \dot{g}_v(\dot{h}_x(\dot{x}))
						\end{equation}
						for the <em>adjoint operator</em> $\bar{f}_x$ we use 
						\begin{align*}
							\langle \bar{y} , \dot{f}_x(\dot{x}) \rangle_Y = \langle \bar{y}, \dot{g}_v(\dot{h}_x(\dot{x})) \rangle_Y = \langle \bar{g}_v (\bar{y}), \dot{h}_x(\dot{x}) \rangle_V = \langle \bar{h}_x(\bar{g}_v(\bar{y})), \dot{x} \rangle_X \quad \forall \bar{y} \in Y, \dot{x} \in X
						\end{align*}
						and find
						\begin{equation}
							\bar{f}_x = \bar{h}_x(\bar{g}_v(\bar{y}))
						\end{equation}
						<div style="display:flex;">
							<div style="flex: 1;">
								<h4>Tangent mode</h4>
								\begin{align*}
								&\text{foreach } j = 1, ... J \\
								&\qquad \dot{x} \leftarrow e_j \\
								&\qquad \dot{y} \leftarrow \dot{g}_v(\dot{h}_x(\dot{x}))\\
								&\qquad \text{foreach } i = 1, ... I \\
								&\qquad \qquad (Df)^{(i, j)} = \langle e_j, \dot{y} \rangle\\
								&\qquad \text{end}\\
								&\text{end}
								\end{align*}
							</div>
							<div style="flex: 1;">
								<h4>Adjoint mode</h4>
								\begin{align*}
								&\text{foreach } i = 1, ... I \\
								&\qquad \bar{y} \leftarrow e_i \\
								&\qquad \bar{x} \leftarrow \bar{h}_x(\bar{g}_v(\bar{y}))\\
								&\qquad \text{foreach } j = 1, ... J \\
								&\qquad \qquad (Df)^{(i, j)} = \langle \bar{x}, e_j \rangle\\
								&\qquad \text{end}\\
								&\text{end}
								\end{align*}
							</div>
						</div>
						<ul>
							<li>AD is even more systematic..</li>
						</ul>
					</section>

					<section style="text-align: left;">
						<h3>Automatic Algorithmic Differentiation</h3>
						Every numerical program $f: \mathbb{R}^{n-1} \to \mathbb{R}^{m+1}$ can be decomposed into <em>single assignments</em> $(\varphi^{(1)}, ... \varphi^{(N)})$.
						\begin{align*}
							&(v^{(0)}, v^{(-1)}, ..., v^{(-n)})^T = x \\
							&v^{(n)} = \varphi^{(n)}_{(v^{(j)})_{j \prec n}} \quad \forall n = 1, ... N \\
							&y = (v^{(N)}, v^{(N-1)}, ..., v^{(N-m)})^T
						\end{align*}
						<p>
							For every single assignment $\varphi^{(n)}_{(v^{(j)})_{j \prec n}}$ we know
						</p>
						<ul>
							<li>
								<em>tangent operator </em> $\dot{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\dot{v}^{(j)})_{j \prec n})$ and
							</li>
							<li>
								<em>adjoint operator</em> $\bar{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\bar{v}^{(k)})_{k \succ n})$
							</li>
						</ul>

						<div style="display:flex;">
							<div style="flex: 1;">
								<h4>Tangent mode</h4>
								\begin{align*}
									&(v^{(0)}, v^{(-1)}, ...)^T = x \\
									&(\dot{v}^{(0)}, \dot{v}^{(-1)}, ...)^T = \dot{x} \\
									&\hspace{-30px}\begin{cases}
										v^{(n)} = \varphi^{(n)}_{(v^{(j)})_{j \prec n}} \\
										\dot{v}^{(n)} = \dot{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\dot{v})_{j \prec n})
									\end{cases} \small{\quad \forall n = 1, ... N}\\
									&y = (v^{(N)}, v^{(N-1)}, ...)^T\\
									&\dot{y} = (\dot{v}^{(N)}, \dot{v}^{(N-1)}, ...)^T
								\end{align*}
							</div>
							<div style="flex: 1;">
								<h4>Adjoint mode</h4>
								\begin{alignat*}{2}
									&(v^{(0)}, v^{(-1)}, ...)^T = x \\
									&v^{(n)} = \varphi^{(n)}_{(v^{(j)})_{j \prec n}} &&\small{\quad \forall n = 1, ..., N}\\
									&y = (v^{(N)}, v^{(N-1)}, ...)^T\\
									&(\bar{v}^{(N)}, \bar{v}^{(N-1)}, ...)^T = \bar{y} \\
									&\bar{v}^{(n)} = \bar{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\bar{v}^{(k)})_{k \succ n}) &&\small{\quad \forall n = N-m-1, ..., -n}\\
									&\bar{x} = (\bar{v}^{(0)}, \bar{v}^{(-1)}, ...)^T
								\end{alignat*}
							</div>	
						</div>
					</section>
				</section>

				<section style="text-align: left">
					<h3>Abstract Adjoint: Algorithmic Differentiation (AD)</h3>
					<ul>
						<li>
							consider $f: \mathcal{X} \to Y, x \mapsto f(x) =: y$ with open $\mathcal{X} \subseteq X $ 
						</li>
						<li>define the (Frechet-) derivative (or tangent operator) $\dot{f}_x: T_xX \to T_yY$ by
							\begin{equation}
							\lim_{||\dot{x}|| \to 0} \frac{||f(x + \dot{x}) - f(x) - \dot{f}_x(\dot{x})||}{||\dot{x}||} \quad \text{with } \dot{x} \in T_xX
							\end{equation}
						</li>
						<li>assuming that the tangent operator is continuous and $T_xX$, $T_yY$ are Hilbert spaces:
							\begin{equation}
							\langle \bar{y}, \dot{f}_x(\dot{x}) \rangle_{T_y Y} = \langle \bar{f}_x(\bar{y}), \dot{x} \rangle_{T_x X} \quad \forall \dot{x} \in T_xX, \bar{y} \in T_yY
							\end{equation}
							<div style="font-size:60%; width:100%; text-align: right; margin-top:-30px;">(notation adopted from AD)</div>
						</li>
						<li>Choose cartesian unit vectors for $\bar{y}$ and $\dot{x}$ (seeding), then (glossing over details): 
							<ul>
								<li>Non-Adjoint Implementation = tangent mode of AD</li>
								<li>Adjoint Implementation = adjoint mode of AD</li>
							</ul>
						</li>
					</ul>

					<div style="display: flex;">
						<div class="fragment fade-in" style="flex:3; background-color: lightgray; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 5px; font-size: 100%; margin:30px;">
							<h4>AD Notation</h4>
							<ul>
								<li>tangent vectors: $\dot{y} = \dot{f}_x(\dot{x})$</li>
								<li>adjoint vectors: $\bar{x} = \bar{f}_x(\bar{y})$</li>
							</ul>
						</div>
						<div  class="fragment fade-in" style="flex: 3; background-color: lightgray; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 5px; font-size: 100%; margin:30px;">
							<h4>Special Case: Implicit Algorithmic Differentiation</h4>
							<ul>
								<li>
									consider $f: \mathcal{X} \to Y, x \mapsto (y \in Y : F(x, y) = 0)$
								</li>
								<li>$\dot{f}_x(\dot{x}) = (\dot{y} \in T_yY : \dot{v} = -\dot{F}^{(1)}_{x, y}(\dot{x}) \land  \dot{F}^{(2)}_{x, y}(\dot{y}) = \dot{v} )$
								</li>
								<li>$\bar{f}_x(\bar{y}) = (\bar{x} \in T_xX : \bar{F}^{(1)}_{x, y}(\bar{v}) = \bar{x} \land \bar{y} = -\bar{F}^{(2)}_{x, y}(\bar{v}) )$
								</li>
							</ul>
						</div>
					</div>
					
					<div class="r-stretch"></div>
					<ul style="font-size:60%; ">
						<li>Griewank, A., & Walther, A. (2008). <b>Evaluating Derivatives</b>. <a href="https://doi.org/10.1137/1.9780898717761">doi:10.1137/1.9780898717761</a></li>
						<li>Naumann, U. (2011). <b>The Art of Differentiating Computer Programs</b>. <a href="https://doi.org/10.1137/1.9781611972078">doi:10.1137/1.9781611972078</a></li>
						<li>Pearlmutter, B. A., & Siskind, J. M. (2008). <b>Reverse-mode AD in a functional framework</b>. <a href="https://doi.org/10.1145/1330017.1330018">doi:10.1145/1330017.1330018</a></li>
						<li>Blondel, M., et. al. (2021). <b>Efficient and Modular Implicit Differentiation</b>. <a href="https://doi.org/10.48550/ARXIV.2105.15183">doi:10.48550/ARXIV.2105.15183</a></li>
					</ul>
				</section>

				<section style="text-align: left;">
					<h3>Abstract Adjoint: Example: linear pde (weak form)</h3>
					$A: G \to H$ is the solution operator of a linear pde ($U, V, G, H$ Hilbert spaces $U\subseteq H$, $V\subseteq G$)
					\begin{equation}
						\color{orange}{A(g) = (u \in U | a(u, v) + \langle v, g\rangle_G = 0\quad \forall v \in V)}
					\end{equation}

					<div class="fragment fade-in" style="margin-top: 50px;">
						Derivation: we introduce $\lambda \in V$
						\begin{align*}
						\Sigma = \langle h, A(g) \rangle_H &= \langle h, u \rangle_H \\
						&=\langle h, u \rangle_H + \color{orange}{\underbrace{a(u, \lambda) + \langle \lambda, g\rangle_G}_{=0 \, \forall \lambda \in U}} \\
						&=\color{green}{\underbrace{\langle h, u \rangle_H + a(u, \lambda)}_{=0 \, \forall u \in U}} + \langle \lambda, g\rangle_G \\
						& \hphantom{\,  = \langle h, u \rangle_H + a(u, \lambda)} = \langle \lambda, g \rangle_G = \langle A^*(h), g \rangle_G = \Sigma
						\end{align*}
					</div>
					<div style="margin-top: 50px;" class="fragment fade-in">
						The adjoint operator $A^*: H \to G$ is
						\begin{equation}
							\color{green}{A^*(h) = (\lambda \in V | a(u, \lambda) + \langle h, u \rangle_H = 0 \quad \forall u \in U)}
						\end{equation}
					</div>
					<div class="fragment fade-in" style="flex:3; background-color: lightgray; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 5px; font-size: 100%; margin:50px;">
						<h5>Duality (from optimization theory)</h5>
						<ul>
							<li>Lagrangian: $\mathcal{L} = \langle h, u \rangle_H + a(u, \lambda) + \langle \lambda, g\rangle_G$</li>
							<li>Lagrange-multiplyer: $\lambda \in V$</li>
						</ul>
					</div>
				</section>

				<section style="text-align: left">
					<h3>Abstract Adjoint: Dual/Adjoint Consistency</h3>
					A <b>discretization</b> is dual consistent, if:
					<div style="text-align: center;">
						<em>"The adjoint of the discretized problem is a consistent discretization of the adjoint of the continuous problem."</em>
					</div>
					<ul>
						<li>with $h \in H$, $g \in G$ vectors, $A: G \to H$ continuous operator, $\cdot{}_\Delta$ the consistent discretization</li>
						<li>if the discretization if dual consistent, the following diagram commutes:</li>
					</ul>
					\begin{equation}
						\begin{matrix}
							\langle h, A(g) \rangle_H & \leftrightarrow & \langle A^*(h), g \rangle_G \\
							\downarrow_{\Delta} & & \downarrow_{\Delta}\\
							\langle h_\Delta, A_\Delta \cdot{} g_\Delta \rangle_{H_{\Delta}} & \leftrightarrow & \langle A^T_{\Delta}\cdot{} h_{\Delta}, g_{\Delta} \rangle_{G_{\Delta}}
						\end{matrix}
					\end{equation}
					<ul>
						<li>
							interpretability of the "adjoint solution" $A_\Delta^T \cdot{} h_\Delta$ as the solution of the continuous adjoint equation
						</li>
						<li>
							"superconvergence" of linear functionals $\langle \cdot{}, g_\Delta\rangle_{G_\Delta}$ (convergence order doubling)
						</li>
					</ul>

					<div style="background-color: lightgray; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; font-size: 80%; margin:30px;">
						<h5>"optimize-then-discretize" vs. "discretize-then-optimize"</h5>
						<h5>"continuous adjoint method" vs. "discrete adjoint method"</h5>
						<ul>
							<li>related concepts from optimization/optimal control literature</li>
							<li>for a dual consistent discretization both approaches are identical</li>
						</ul>
					</div>
					<div class="r-stretch"></div>

					<ul style="font-size:60%">
						<li>Hicken, J. E., & Zingg, D. W. (2014). <b>Dual consistency and functional accuracy: a finite-difference perspective</b>. <a href="https://doi.org/10.1016/j.jcp.2013.08.014">doi:10.1016/j.jcp.2013.08.014</a> 
						</li>
						<li>Oliver, T. A., & Darmofal, D. L. (2009). <b>Analysis of Dual Consistency for Discontinuous Galerkin Discretizations of Source Terms</b>. <a href="https://doi.org/10.1137/080721467">doi:10.1137/080721467</a> 
						</li>
						<li>Hartmann, R. (2007). <b>Adjoint Consistency Analysis of Discontinuous Galerkin Discretizations</b>. <a href="https://doi.org/10.1137/060665117">doi:10.1137/060665117</a> 
						</li>
						<li>Giles, M. B., & Pierce, N. A. (2003). <b>Adjoint Error Correction for Integral Outputs</b>. <a href="https://doi.org/10.1007/978-3-662-05189-4_2">doi:10.1007/978-3-662-05189-4_2</a> 
						</li>
					</ul>
				</section>

				<section style="text-align: left">
					<h3>Applying Adjoints Twice</h3>
					<div style="display:flex;">
						<div style="flex: 4; font-size:80%">
							<ul>
								<li>parameters $p \in P$</li>
								<li>parameter-dependent bilinear form $a_p: U \times V (\times P) \to \mathbb{R}$ </li>
								<li>excitations $b^{(i)} : V \to \mathbb{R}$</li>
								<li>extractions $c^{(j)}: U \to \mathbb{R}$</li>
								<li>observables $\Sigma^{(ji)} \in \mathbb{R}$</li>
							</ul>
						</div>
						<div style="flex: 3; font-size:80%">
							<ul>
								<li>tangent of $a_p$ $\dot{a}_p: U \times V \times P \to \mathbb{R}$</li>
								<li>corres. adjoint $\bar{a}_p: U \times V \times \mathbb{R} \to P$</li>
								<li>tangent spaces = spaces (?)</li>
								<li>given tangent/adjoint vectors $\dot{p}$ and $\bar{\Sigma}^{(ji)}$ resp.</li>
							</ul>
						</div>
					</div>
					<table style="width: 100%; margin-top: 50px;">
						<tr style="border-style: hidden;">
							<td style="text-align: left">
								<h4>Model ($u^{(i)} \in U)$:</h4>
								\begin{align*}
									a_p(u^{(i)}, v) + b^{(i)}(v) &= 0 \quad \forall v \in V \\
									\Sigma^{(ji)} &= c^{(j)}(u^{(i)})
								\end{align*}
							</td>
							<td class="fragment fade-in" data-fragment-index="1">
								<h4>1st "adjoint method" ($\lambda^{(j)} \in V$):</h4>
									\begin{align*}
										a_p(v, \lambda^{(j)}) + c^{(j)}(v) &= 0 \quad \forall v \in U \\
										\Sigma^{(ji)} &= b^{(i)}(\lambda^{(j)}) \\
									\end{align*}
							</td>
						</tr>
						<tr style="border-style: hidden;">
							<td class="fragment fade-in" data-fragment-index="2">
								<h4>Tangent/sensitivity model ($\dot{\lambda}^{(j)} \in V)$):</h4>
								<div class="r-stack">
									<div class="fragment fade-in" data-fragment-index="2"> 
										\begin{align*}
											a_p(v, \dot{\lambda}^{(j)}) + \dot{a}_p(v, \lambda^{(j)}, \dot{p}) &= 0 \quad \forall v \in U\\
											\dot{\Sigma}^{(ji)} &= b^{(i)}(\dot{\lambda}^{(j)})
										\end{align*}
									</div>
								</div>
								
							</td>
							<td class="fragment fade-in" data-fragment-index="4">
								<h4>2nd "adjoint method" ($\bar{\lambda}^{(j)} \in U$):</h4>
								<div class="r-stack">
									<div class="fragment fade-out" data-fragment-index="5">
										\begin{align}
											a_p(\bar{\lambda}^{(j)}, v) + \bar{\Sigma}^{(ji)} b^{(i)}(v) &= 0 \quad \forall v \in V \\
											\bar{\Sigma}^{(ji)} \dot{\Sigma}^{(ji)} &= \dot{a}_p(\bar{\lambda}^{(j)}, \lambda^{(j)}, \dot{p})
										\end{align}
									</div>
									<div class="fragment fade-in" data-fragment-index="5"> 
										\begin{align}
											a_p(\bar{\lambda}^{(j)}, v) + \bar{\Sigma}^{(ji)} b^{(i)}(v) &= 0 \quad \forall v \in V \\
											\bar{\Sigma}^{(ji)} \dot{\Sigma}^{(ji)} &= \langle \bar{a}_p(\bar{\lambda}^{(j)}, \lambda^{(j)}, 1), \dot{p} \rangle
										\end{align}
									</div>
								</div>
							</td>
						</tr>
						<tr style="border-style: hidden;">
							<td colspan=2 style="text-align: center;">
								<div class="fragment fade-in" data-fragment-index="3" style="display: inline-block; text-align: left;">
									<h4>Algorithmic Differentiation ($\langle \bar{\Sigma}, \dot{\Sigma} \rangle = \langle \bar{p}, \dot{p} \rangle$):</h4>
									\begin{align}
										a_p(\bar{\Sigma}^{(ji)} v, \dot{\lambda}^{(j)}) + \dot{a}_p(\bar{\Sigma}^{(ji)} v, \lambda^{(j)}, \dot{p}) &= 0 \quad \forall v \in U \\
										\bar{\Sigma}^{(ji)} \dot{\Sigma}^{(ji)} &= \bar{\Sigma}^{(ji)} b^{(i)}(\dot{\lambda}^{(j)})
									\end{align}
								</div>
							</td>
						</tr>
					</table>
				</section>


				<section style="text-align: left;">
					<h3>Applying Adjoints Twice: Summary</h3>
					<table>
						<tr style="border-style: hidden;">
							<td style="text-align: center; ">
								<h4>non-adjoint forward + non-adjoint derivative</h4>
							</td>
							<td style="text-align: center; ">
								<h4>adjoint forward + adjoint derivative</h4>
							</td>
						</tr>
						<tr style="border-style: hidden;">
							<td style="text-align: center; ">
								\begin{align*}
									a_p(u^{(i)}, v) + b^{(i)}(v) &= 0 \quad \forall v \in V \\
									\Sigma^{(ji)} &= c^ {(j)}(u^{(i)}) \\
									C &= g(\Sigma^{(\cdot{}\cdot{})}) \\
									a_p(\dot{u}^{(in)}, v) + \dot{a}_p(u^{(i)}, v, e^{(n)}) &= 0 \quad \forall v \in V\\
									\dot{\Sigma}^{(jin)} &= c^{(jin)}(\dot{u}^{(in)}) \\
									(\nabla_p C)^{(n)} &= \dot{g}_{\Sigma^{(\cdot{}\cdot{})}}(\dot{\Sigma}^{(\cdot{}\cdot{}n)})
								\end{align*}
							</td>
							<td style="text-align: center; ">
								\begin{align}
									a_p(v, \lambda^{(j)}) + c^{(j)}(v) &= 0 \quad \forall v \in U\\
									\Sigma^{(ji)} &= b^{(i)}(\lambda^{(j)}) \\
									C &= g(\Sigma^{(\cdot{}\cdot{})}) \\
									\bar{\Sigma}^{(\cdot{}\cdot{})} &= \bar{g}_{\Sigma^{(\cdot{}\cdot{})}}(\bar{C})\\
									a_p(\bar{\lambda}^{(j)}, v) + \bar{\Sigma}^{(ji)} b^{(i)}(v) &= 0 \quad \forall v \in V \\
									(\nabla_p C)^{(n)} &= \langle \bar{a}_p(\bar{\lambda}^{(j)}, \lambda^{(j)}, 1), e^{(n)}\rangle
								\end{align}
							</td>
						</tr>
						<tr style="border-style: hidden;">
							<td>
								<ul>
									<li>$(I + IN)\times \mathcal{C}(a_p(\cdot{}, \cdot{})) + ... $</li>
								</ul>
							</td>
							<td>
								<ul>
									<li>$2J \times \mathcal{C}(a_p(\cdot{}, \cdot{})) + ... $</li>
								</ul>
							</td>
						</tr>
					</table>
					<h4>Generalization:</h4>
					<ul style="width: 100%;">
						<li>parameter-dependent $b_p^{(i)}(v)$ and $c_p^{(j)}(u)$</li>
						\begin{equation}
						\bar{p} = \bar{a}_p(\bar{\lambda}^{(j)}, \lambda^{(j)}, 1) + \bar{c}_p^{(j)}(\bar{\lambda}^{(j)}, 1) + \bar{b}^{(i)}_p(\lambda^{(j)}, \bar{\Sigma}^{(ji)})
						\end{equation}
					</ul>
					<h4 style="margin-top: 20px;">Assumptions:</h4>
					<ul>
						<li>$\mathcal{C}(a_p(\cdot{}, \cdot{})) \gg \mathcal{C}(b^{(i)}), \mathcal{C}(c^{(j)}), ...$ </li>
						<li>$a_p(\cdot{}, \cdot{})$ cannot be solved directly (e.g. LU)</li>
						<li>$I > J$, $N > J$, $C \in \mathbb{R}$</li>
					</ul>
					</ul>
				</section>

				<!-- Poisson Example -->
				<section style="text-align: left;">
					<section><h3>Example: An inverse problem based on the Poisson equation</h3>
						<table style="margin-top:30px;">
							<tr>
								<td style="text-align: center; vertical-align: top; padding: 0px;">
									<h4>model</h4>
									<div style="">
										\begin{align*}
											&\begin{cases}
												-\nabla \cdot{} m \nabla u^{(i)} = 0 \quad &\forall x \in \mathcal{R}\\
												u^{(i)} = g^{(i)} \quad &\forall x \in \partial\mathcal{R}
												\end{cases}\\
											&\Sigma^{(i, j)} = \int_{\mathcal{R}} h^{(j)} u^{(i)} dx\\
											&C = \frac{1}{2 IJ}\sum_{i, j=1}^{I,J} (\Sigma^{(i, j)} - \tilde{\Sigma}^{(i, j)})^2  
										\end{align*}
									</div>
								</td>
								<td style="text-align: center; vertical-align: top; padding: 0px;">
									<ul style="margin-left: 50px; ">
										<li>$m, h^{(j)} \in L^2(\mathcal{R})$, $g^{(i)} \in L^2(\partial \mathcal{R})$</li>
										<li>weak enforcement of boundary conditions $u, v \in H^1_h(\mathcal{R})$</li>
										<li>Aubin-Nitsche trick: stable bilinear form
											\[a(u^{(i)}, v) + b^{(i)}(v) = 0 \quad \forall v \in H^1_h(\mathcal{R})\]
										</li>
										<li>$I, N \gg J$ (many bc $g^{(i)}$, many parameters $p \in \mathbb{R}^N$, some extractions $h^{(j)}$)</li>
										<li>here: $I=200$, $N=2972$, $J=7$</li>
										<li>FE-framework: Gridap.jl, linsol: gmres + ilu</li>
									</ul>
								</td>
							</tr>
						</table>
						<table style="text-align: center; margin-top: 80px;">
							<tr style="border-style: hidden;">
								<td style="text-align: center; padding: 0px; vertical-align:top;"><h4>solutions $u^{(i)}$</h4></td>
								<td style="width: 100px; "></td>
								<td style="text-align: center; padding: 0px; vertical-align:top;"><h4>measurements $\Sigma^{(i, j)}$</h4></td>
							</tr>
							<tr>
								<td class="fragment highlight-red" style="text-align: center; padding: 0px; vertical-align:top;">  
									<video data-autoplay loop style="width: 900px; margin-top: 00px;">
										<source data-src="figures/upd_poisson/mp4s/forward.mp4" type="video/mp4">
									</video>
								</td>
								<td></td>
								<td style="text-align: center; padding: 0px; vertical-align:top;">
									<img style="width: 900px; " src="figures/upd_poisson/measurements.png">
								</td>
							</tr> 
						</table>
					</section>

					<section>
						<h3>Example: Derivation</h3>
						<h4>weak forms</h4>
						\begin{align*}
							a_m(u, v) &= \int_{\mathcal{R}} m \nabla u \cdot{} \nabla v dx - \int_{\partial \mathcal{R}} m \nabla_n u v + u \nabla_n v \, d\Gamma + \int_{\partial \mathcal{R}} \alpha u v \, d \Gamma \\
							b^{(i)}(v) &= \int_{\partial \mathcal{R}} \nabla_n v  g^{(i)} \, d\Gamma - \int_{\partial \mathcal{R}} \alpha g^{(i)} v \, d\Gamma\\
							c^{(j)}(u) &= \int_{\mathcal{R}} h^{(j)} u \, dx
						\end{align*}
						<h4>adjoint forward</h4>
						\begin{align*}
							a_m(u, \lambda^{(j)}) + c^{(j)}(u) = 0 \quad \forall u\\
							\int_{\mathcal{R}} m \nabla u \cdot{} \nabla \lambda^{(j)} dx - \int_{\partial \mathcal{R}} m \nabla_n u \lambda^{(j)} + u \nabla_n \lambda^{(j)} \, d\Gamma + \int_{\partial \mathcal{R}} \alpha u \lambda^{(j)} \, d \Gamma + \int_{\mathcal{R}} \mu^{(j)} u \, dx = 0 \quad \forall u 
						\end{align*}
						<h4>adjoint forward (strong form)</h4>
						\begin{align*}
							\begin{cases}
								-\nabla m \cdot{} \nabla \lambda^{(j)} = -h^{(j)} \quad &\forall x \in \mathcal{R}\\
								\lambda^{(j)} = 0 \quad &\forall x \in \partial \mathcal{R}
							\end{cases}
						\end{align*}
					</section>
					<section>
						<h3>Example: Derivation</h3>
						<h4>adjoint derivative</h4>
						\begin{align*}
							&a_m(\bar{\lambda}^{(j)}, \dot{\lambda}) + \bar{\boldsymbol \Sigma}^{(j)T} \boldsymbol{b}(\dot{\lambda}) = 0 \quad \forall \dot{\lambda} \\
							&\int_{\mathcal{R}} m \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \dot{\lambda} dx - \int_{\partial \mathcal{R}} m \nabla_n \bar{\lambda}^{(j)} \dot{\lambda} + \bar{\lambda}^{(j)} \nabla_n \dot{\lambda} \, d\Gamma + \int_{\partial \mathcal{R}} \alpha \bar{\lambda}^{(j)} \dot{\lambda} \, d \Gamma \\
							&+ \int_{\partial \mathcal{R}} \nabla_n \dot{\lambda}  \sum_{i=1}^{I}(\bar{\Sigma}^{(i, j)} g^{(i)}) \, d\Gamma - \int_{\partial \mathcal{R}} \alpha \sum_{i=1}^{I} (\bar{\Sigma}^{(i, j)} g^{(i)}) \dot{\lambda} \, d\Gamma = 0 \quad \forall \dot{\lambda}
						\end{align*}
						<h4>adjoint derivative (strong form)</h4>
						\begin{align*}
							\begin{cases}
								- \nabla m \cdot{} \nabla \bar{\lambda}^{(j)} = 0 \quad &\forall x \in \mathcal{R} \\
								\bar{\lambda}^{(j)} = \sum_{i=1}^{I} \bar{\Sigma}^{(i, j)} g^{(i)} \quad &\forall x \in \partial \mathcal{R} 
							\end{cases}
						\end{align*}
						<h4>tangent model</h4>
						\begin{align*}
							\dot{C} = \sum_{j=1}^{J} \dot{a}_m(\bar{\lambda}^{(j)}, \lambda^{(j)}, \dot{m}) = \sum_{j=1}^{J} \int_{\mathcal{R}} \dot{m} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} \, dx - \int_{\partial \mathcal{R}} \dot{m} \nabla_n \bar{\lambda}^{(j)} \underbrace{\lambda^{(j)}}_{=0} \, d \Gamma
						\end{align*}
						<h4>adjoint model</h4>
						\begin{align*}
							\langle \bar{C}, \sum_{j=1}^{J} \sum_{j=1}^{J} \int_{\mathcal{R}} \dot{m} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} \, dx \rangle = \langle \bar{C} \sum_{j=1}^{J} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)}, \dot{m} \rangle_{L^2(\mathcal{R})} \\
							\bar{a}_m(\bar{\lambda}^{(:)}, \lambda^{(:)}, \bar{C}) = \sum_{j=1}^{J} \bar{C} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} 
						\end{align*}
					</section>
					<section>
						<h3>Example: Derivation</h3>
						<h4>adjoint parametrization</h4>
						\begin{align*}
							\langle \bar{m}, \dot{m} \rangle_{L^2(\mathcal{R})} = \langle \bar{m}, \dot{\gamma}_p(\dot{p}) \rangle_{L^2(\mathcal{R})} = \langle \int_{\mathcal{R}} \frac{\partial \gamma_p}{\partial p} \bar{m} \, dx, \dot{p} \rangle_{\mathbb{R}} 
						\end{align*}
					</section>
				</section>

				<section style="text-align: left;">
					<h3>Example: An inverse problem based on the Poisson equation</h3>
					<div class="fragment fade-in" data-fragment-index="1" style="position:absolute; top:250px; left:1250px; background-color: white;border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; text-align: center; padding:10px; ">
						<h5>measurements $\Sigma^{(ji)}$</h5>
						<img style="width:600px;" src="figures/upd_poisson/measurements_extra_51.png">
					</div>
					<div style="display:flex; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; margin:20px;">
						<div style="width:min-content; text-align: center;">
							<h4>non-adjoint model</h4>
							<span style="font-size: 80%;">
								\begin{align*}
									&\begin{cases}
										-\nabla \cdot{} m \nabla u^{(i)} = 0 \quad &\forall x \in \mathcal{R}\\
										u^{(i)} = g^{(i)} \quad &\forall x \in \partial\mathcal{R}
										\end{cases}\\
									&\Sigma^{(ji)} = \int_{\mathcal{R}} h^{(j)} u^{(i)} dx\\
								\end{align*}
							</span>
							<ul>
								<li>
									some text...
								</li>
							</ul>
						</div>
						<div style="text-align: center;">
							<h4>solution $u^{(i)}$</h4>
							<img style="width:600px; display: inline; " src="figures/upd_poisson/forward_trace_51.png">
						</div>
					</div>
					<div class="fragment fade-in" data-fragment-index="2" style="position: absolute; top: 600px; left:550px; border-radius: 10px; background-color: white; border-color: red; border-width: 2px; border-style: solid; padding: 10px;margin:20px;">
						<ul>
							<li>$\approx 100ms (\approx 200\times 0.5ms + \text{...})$</li>
						</ul>
					</div>
					<div class="r-stretch"> </div>
					<div class="fragment fade-in" data-fragment-index="2" style="position: absolute; top: 770px; left:700px; border-radius: 10px; background-color: white; border-color: green; border-width: 2px; border-style: solid; padding: 10px;margin:20px;">
						<ul>
							<li>$\approx 5.5ms (\approx 7\times 0.5ms + \text{...})$</li>
						</ul>
					</div>
					<div style="display:flex; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; margin:20px;">
						<div style="width:min-content; text-align: center;">
							<h4>adjoint model</h4>
							<span style="font-size: 80%;">
								\begin{align*}
									&\begin{cases}
										-\nabla \cdot{} m \nabla \lambda^{(j)} = -h^{(j)} &\forall x \in \mathcal{R} \\
										\lambda^{(j)} = 0 \quad &\forall x \in \partial \mathcal{R}
									\end{cases}\\
									&\Sigma^{(ji)} = \int_{\partial \mathcal{R}} \nabla_n \lambda^{(j)} g^{(i)} \, d x\\
								\end{align*}
							</span>
							<ul>
								<li>
									some text...
								</li>
							</ul>
						</div>
						<div style="text-align: center;">
							<h5>adjoint solution $\lambda^{(j)}$</h5>
							<img style="width:600px; " src="figures/upd_poisson/adjoint_forward_1.png">
						</div>
						<div style="text-align: center;">
							<h5>riesz representation $\nabla_n \lambda^{(j)}$</h5>
							<img style="width:600px; " src="figures/upd_poisson/boundary_projections.png">
						</div>
					</div>
				</section>

				<section style="text-align: left;">
					<h3>Example: An inverse problem based on the Poisson equation</h3>

					<div class="fragment fade-in" data-fragment-index="1" style="position:absolute; top:250px; left:1250px; background-color: white;border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; text-align: center; padding:10px; ">
						<h5>gradient $\nabla_m C$</h5>
						<img style="width:600px;" src="figures/upd_poisson/gradient.png">
					</div>
					<div style="display:flex; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; margin:20px;">
						<div style="width:min-content; text-align: center;">
							<h4>non-adjoint derivative</h4>
							<span style="font-size: 70%;">
								\begin{align*}
									&\begin{cases}
									-\nabla \cdot{} m \nabla \dot{\lambda}^{(j)} = \nabla \cdot{} \dot{m} \nabla \lambda^{(j)} &\forall x \in \mathcal{R} \\
										\dot{\lambda}^{(j)} = 0 \quad &\forall x \in \partial \mathcal{R}
										\end{cases}\\
									&\dot{\Sigma}^{(ji)} = \int_{\partial \mathcal{R}} \nabla_n \dot{\lambda}^{(j)} g^{(i)} \, d x\\
									&\dot{C} = \frac{1}{IJ} \sum_{i, j=1}^{I,J} (\Sigma^{(ji)} - \tilde{\Sigma}^{(ji)})\dot{\Sigma}^{(ji)}
								\end{align*}
							</span>
							<ul>
								<li>
									some text...
								</li>
							</ul>
						</div>
						<div style="text-align: center;">
							<h5>tangent solution $\dot{\lambda}^{(j)}$</h5>
							<img style="width:600px; display: inline; " src="figures/upd_poisson/tangent_gradient/gradient_tangent_1_9.png">
						</div>
					</div>
					<div class="fragment fade-in" data-fragment-index="2" style="position: absolute; top: 600px; left:400px; border-radius: 10px; background-color: white; border-color: red; border-width: 2px; border-style: solid; padding: 10px;margin:20px;">
						<ul>
							<li>$\approx 16.6s (\approx 2972 \times 5.5ms + \text{...})$</li>
							<li>$\approx 4\text{min }30s (\approx 2972 \times 200 \times 0.5ms + \text{...})$</li>
						</ul>
					</div>
					<div class="r-stretch"> </div>
					<div class="fragment fade-in" data-fragment-index="2" style="position: absolute; top: 770px; left:700px; border-radius: 10px; background-color: white; border-color: green; border-width: 2px; border-style: solid; padding: 10px;margin:20px;">
						<ul>
							<li>$\approx 14.6ms (\approx 2\times 5.5ms + \text{...})$</li>
						</ul>
					</div>
					<div style="display:flex; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; margin:20px;">
						<div style="width:min-content; text-align: center;">
							<h4>adjoint derivative</h4>
							<span style="font-size: 70%;">
								\begin{align*}
									&\bar{\Sigma}^{(ji)} = \frac{1}{I J} (\Sigma^{(ji)} - \tilde{\Sigma}^{(ji)}) \bar{C} \\
									&\begin{cases}
										-\nabla \cdot{} m \nabla \bar{\lambda}^{(j)} = 0 &\forall x \in \mathcal{R} \\
										\bar{\lambda}^{(j)} = \sum_{i=1}^{I} \bar{\Sigma}^{(ji)} g^{(i)} \quad &\forall x \in \partial \mathcal{R}
									\end{cases}\\
									&\bar{m} = \sum_{j=1}^{J} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} \\

								\end{align*}
							</span>
							<ul>
								<li>
									some text...
								</li>
							</ul>
						</div>
						<div style="text-align: center;">
							<h5>adjoint solution $\bar{\lambda}^{(j)}$</h5>
							<img style="width:600px; " src="figures/upd_poisson/adjoint_gradient/adjoint_solution_1.png">
						</div>
						<div style="text-align: center;">
							<h5>Taylor remainder</h5>
							<img style="width:600px; " src="figures/upd_poisson/taylor_remainder.png">
						</div>
					</div>
				</section>

				<section style="text-align: left;">
					<h3>Example: An inverse problem based on the Poisson equation</h3>
					<div class="fragment fade-in" data-fragment-index="2" style="z-index:1000; position:absolute; top:1000px; left:1000px; background-color: white; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; text-align: center; padding:10px; ">
						<ul>
							<li>runtime: $5s$</li>
							<li>without adjoint sensitivity: $\approx 1h$</li>
							<li>without adjoint$^2$: $\approx 11\text{h } 45\text{min}$</li>
						</ul>
					</div>
					<div style="display:flex; ">
						<div>
							<h5>$m$-parametrization</h5>
							\begin{align}
								v^{[0]} &= (x, y)^T \\
								v^{[1]} &= \text{relu}(W^{[1]} \cdot{} v^{[0]} + b^{[1]}) \\
								v^{[2]} &= \text{relu}(W^{[2]} \cdot{} v^{[1]} + b^{[2]}) \\
								v^{[3]} &= \text{softmax}(W^{[3]} \cdot{} v^{[2]} + b^{[3]}) \\
								m &= (0.1, 0.4, 0.9) \cdot{} v^{[3]}
							\end{align}
						</div>
						<div>
							<h5>possible application</h5>
							<ul>
								<li>transformation to "phase classification problem"</li>
								<li>from SciML: neural network as parametrization $m(p)$</li>
								<li>the parametrization $m(p)$ "regularizes"</li>
								<li>number of parameters: $1903$</li>
								<li>no "inverse crime": 
									<ul>
										<li>$2972$ vs $72965$ grid cells</li>
										<li>additive noise $\mathcal{N}(\mu=0, \sigma=1e-5)$</li>
									</ul>
								</li>
								<li>L-BFGS</li>
							</ul>
						</div>
					</div>
					<div style="display:flex; justify-content: center;">
						<div class="fragment highlight-red" data-fragment-index="1" style="text-align: center;">
							<h5 style="margin-top:10px;">measurements $\Sigma^{(i, j)}$ and $\tilde{\Sigma}^{(i, j)}$</h5>
							<video loop data-autoplay style="width:700px; margin-top: 20px;"> 
								<source data-src="figures/upd_poisson/mp4s/measurement_optimization.mp4" type="video/mp4">
							</video>
						</div>
						<div class="fragment highlight-red" data-fragment-index="1" style="text-align: center;">
							<h5 style="margin-top:25px;">optimized material $m$</h5>
							<video loop data-autoplay style="width: 700px; margin-top:10px;">
								<source data-src="figures/upd_poisson/mp4s/material_optimization.mp4" type="video/mp4">
							</video>
						</div>
						<div style="text-align: center;">
							<h5 style="margin-top:25px;">true material</h5>
							<image style="width: 700px;" src="figures/upd_poisson/material.png">
						</div>
					</div>
					
					<ul>
						<li>"neural networks bias towards sparse solutions"</li>
						<li>convergence dependes on the inital guess (does not always converge)</li>
						<li>taylor the parametrization to specific problems (layers, other geometries, phase values, e.g.)</li>
					</ul>
				</section>

				<!--Material Reconstruction in EPMA-->
				<section style="text-align: left;">
					<h3>Material Reconstruction in EPMA</h3>
					<div style="display:flex; ">
						<div style="max-width: max-content;">
							<h5 style="display:inline; ">radiative transfer equation</h5> ($\psi^{(i)}$: electron fluence)
							<div style="font-size:80%">
								\begin{align*}
									\Omega \cdot{} \nabla_x \psi + \Sigma^{\text{tot}} \psi &= \int_{\mathbb{R}^+} \int_{S^2} \Sigma^{\text{scat}} \psi \, d\Omega' \, d\epsilon' \quad \forall (x, \epsilon, \Omega) \in \mathcal{R} \times \mathcal{E} \times S^2 \\
								\end{align*}
								\begin{align*}
									\psi &= g_{\text{beam}}^{(i)} \quad \forall n\cdot{}\Omega < 0 \\
									k^{(ji)} &= \int_{\mathcal{R}} \Sigma^{\text{x-ray}, (j)}\psi^{(i)} \, d \Gamma
								\end{align*}
							</div>
						</div>
						<div style="max-width: max-content;">
							<h5>additivity approximation</h5>
							<div style="font-size:80%">
								\begin{equation}
									\Sigma^{\times}(x, \epsilon, \epsilon', \Omega, \Omega') = \sum_{e=1}^{n_e} \mathcal{N}_e(x) \sigma_e^{\times}(\epsilon, \epsilon', \Omega, \Omega')
								\end{equation}
							</div>
							<h5>csd-approximation</h5>
							<div style="font-size:80%">
								\begin{equation}
									\Sigma \psi = \int_{\mathcal{E}} \Sigma \psi \, d \epsilon  \mapsto -\partial_{\epsilon}( s \psi )
								\end{equation}
							</div>
							</div>
					</div>

					<div style="display:flex; ">
						<div style="width: max-content; ">
							<h5>discretization</h5>
							<div style="font-size:80%">
								\begin{equation}
									\psi^\pm(\cdot{}, \cdot{}, \Omega) = \frac{\psi(\cdot{}, \cdot{}, \Omega) \pm \psi(\cdot{}, \cdot{}, -\Omega)}{2}
								\end{equation}
								\begin{align}
									\begin{split}\label{eq:linear_boltzmann_csd_biliner_form}
										a_p(\psi, \phi) =
										\textstyle -\langle\partial_\epsilon (s_p \psi) \phi \rangle
										&+ \langle \Omega \cdot{} \nabla_x \psi^+ \phi^- - \psi^- \Omega \cdot{} \nabla_x \phi^+ \rangle \\
										&+ \textstyle[|n \cdot{} \Omega| \psi^+ \phi^+]_{\partial_x}
										+ \langle \Sigma^{\text{el,tot}}_p \psi \phi - \int_{S^2} \Sigma^{\text{el}}_p \psi \, d \Omega' \phi \rangle
									\end{split}
								\end{align}
								\begin{align}
									b^{(i)}(\phi) &= 2 [n \cdot{} \Omega g^{(i)} \phi^+ ]_{\partial_x^-}.
								\end{align}
								\begin{align}
									c^{(j)}_p(\psi) &= \langle \mathcal{N}_j \sigma^{\text{x-ray}}_j \psi^+ \rangle.
								\end{align}
							</div>
						</div>
						<div style="width: max-content; ">
							<ul>
								<li>$\Omega$: spherical harmonics basis ($P_N$)</li>
								<li>$x$: mixed Lagrange basis ($L^2_h, H^1_h$)</li>
								<li>$\epsilon$: staggered implicit trapeziodal/midpoint rule</li>
								<li>Schur complement (eliminates odd $^-$ variables)</li>
								<li>MINRES: remaining symmetric system</li>
								<li>dual consistent discretization </li>
							</ul>
						</div>
					</div>
					<div style="font-size:80%; ">
						implementation in julia using: CUDA.jl, Gridap.jl, Krylov.jl, ...
					</div>

					<div class="r-stretch"></div>

					<ul style="font-size:60%">
						<li>Egger, H., & Schlottbom, M. (2012). A Mixed Variational Framework For The Radiative Transfer Equation. <a href="https://doi.org/10.1142/s021820251150014x">doi:10.1142/s021820251150014x</a></li>
						<li>Egger, H., & Schlottbom, M. (2014). Numerical methods for parameter identification in stationary radiative transfer. <a href="https://doi.org/10.1007/s10589-014-9657-9">doi:10.1007/s10589-014-9657-9</a></li>
					</ul>
				</section>

				<section style="text-align: left;">
					<h3>Material Reconstruction in EPMA</h3>
					<div class="fragment fade-in" data-fragment-index="1" style="position:absolute; top:260px; left:1250px; background-color: white;border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; text-align: center; padding:10px; ">
						<h5>measurements $\Sigma^{(ji)}$</h5>
						<img width="600px" src="figures/upd_epma/epma_measurements.png">
					</div>
					<div style="display:flex; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; margin:20px;">
						<div style="width:min-content; text-align: center;">
							<h6>non-adjoint model</h6>
							<span style="font-size: 80%;">
								\begin{align*}
									a_p(u^{(i)}, v) + b^{(i)}(v) &= 0 \, \forall v \in V \\
									\Sigma^{(ji)} &= c_p^{(j)}(u^{(i)}) 
								\end{align*}
							<ul>
								<li>$P_{21}$ moments ($253$ moments)</li>
								<li>$40 \times 120$ spatial grid</li>
								<li>$20$ energy steps</li>
								<li>$2.5 \times 10^7$ dof</li>
								
								<li style="margin-top:40px;">$2(\epsilon) \times 71(x) \times 3(\Omega) = 426 \text{ beams}$</li>
								<li>$2$ elements</li>
							</ul>
						</span>

						</div>
						<div style="text-align: center;">
							<h6>solution $\int_{S^2}\int_{\mathcal{E}} u^{(i)} \, d\epsilon \,d\Omega$</h6>
							<img width="600px" src="figures/upd_epma/epma_forward_main.png">
						</div>
					</div>
					<div class="fragment fade-in" data-fragment-index="2" style="position: absolute; top: 650px; left:550px; border-radius: 10px; background-color: white; border-color: red; border-width: 2px; border-style: solid; padding: 10px;margin:20px;">
						<ul>
							<li>$\approx 7\text{min } 6\text{s} (\approx 426\times 1\text{s} + \text{...})$</li>
						</ul>
					</div>
					<div class="r-stretch"> </div>
					<div class="fragment fade-in" data-fragment-index="2" style="position: absolute; top: 740px; left:700px; border-radius: 10px; background-color: white; border-color: green; border-width: 2px; border-style: solid; padding: 10px;margin:20px;">
						<ul>
							<li>$\approx 2s (\approx 2\times 1\text{s} + \text{...})$</li>
						</ul>
					</div>
					<div style="display:flex; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; margin:20px;">
						<div style="width:min-content; text-align: center;">
							<h6>adjoint model</h6>
							<span style="font-size: 80%;">
								\begin{align*}
									a_p(v, \lambda^{(j)}) + c_p^{(j)}(v) = 0 \, \forall v \in U \\
									\Sigma^{(ji)} = b^{(i)}(\lambda^{(j)}) 
								\end{align*}
							</span>
							<ul>
								<li>
									some text...
								</li>
							</ul>
						</div>
						<div style="text-align: center;">
							<h6>adjoint solution $\int_{S^2}\int_{\mathcal{E}} \lambda^{(j)} \, d\epsilon \,d\Omega$</h6>
							<img width="600px" src="figures/upd_epma/epma_forward_adjoint.png">
						</div>
						<div style="text-align: center;">
							<h6>riesz representation $\int_{S^2} \lambda^{(j)} \,d\Omega$</h6>
							<img width="600px" src="figures/upd_epma/epma_riesz_forward.png">
						</div>
					</div>
				</section>

				<section style="text-align: left;">
					<h3>Material Reconstruction in EPMA</h3>
					<div class="fragment fade-in" data-fragment-index="1" style="position:absolute; top:260px; left:1250px; background-color: white;border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; text-align: center; padding:10px; ">
						<h5>gradient $\Sigma^{(ji)}$</h5>
						<img width="600px" src="figures/upd_epma/epma_gradient2.png">
					</div>
					<div style="display:flex; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; margin:20px;">
						<div style="width:min-content; text-align: center;">
							<h6>non-adjoint derivative</h6>
							<span style="font-size: 80%;">
								\begin{align*}
									a_p(v, \dot{\lambda}^{(j)}) + \dot{a}_p(v, \lambda^{(j)}, \dot{p}) + \dot{c}_p(v, \dot{p}) = 0 \\
									\dot{\Sigma}^{(ji)} = b^{(i)}(\dot{\lambda}^{(j)})
 								\end{align*}
							<ul>
								
							</ul>
						</span>

						</div>
						<div style="text-align: center;">
							<h6>tangent $\int_{S^2}\int_{\mathcal{E}} \dot{\lambda}^{(j)} \, d\epsilon \,d\Omega$</h6>
							<img width="600px" src="figures/upd_epma/epma_tangent_nonadjoint.png">
						</div>
					</div>
					<div class="fragment fade-in" data-fragment-index="2" style="position: absolute; top: 650px; left:550px; border-radius: 10px; background-color: white; border-color: red; border-width: 2px; border-style: solid; padding: 10px;margin:20px;">
						<ul>
							<li>$\approx 2\text{h } 40\text{min} (\approx 4800\times 2\text{s} + \text{...})$</li>
							<li>$\approx 23\text{d } 16\text{h} (\approx 4800\times 7\text{min } 6\text{s} + \text{...})$</li>
						</ul>
					</div>
					<div class="r-stretch"> </div>
					<div class="fragment fade-in" data-fragment-index="2" style="position: absolute; top: 740px; left:700px; border-radius: 10px; background-color: white; border-color: green; border-width: 2px; border-style: solid; padding: 10px;margin:20px;">
						<ul>
							<li>$\approx 5s (\approx 2\times 2\text{s} + \text{...})$</li>
						</ul>
					</div>
					<div style="display:flex; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; margin:20px;">
						<div style="width:min-content; text-align: center;">
							<h6>adjoint derivative</h6>
							<span style="font-size: 80%;">
								\begin{align*}
									a_p(\bar{\lambda}^{(j)}, v) + \bar{\Sigma}^{(ij)}b^{(i)}(v) = 0 \, \forall v \in U \\
									\bar{p} = \bar{a}_p(\bar{\lambda}^{(j)}, \lambda^{(j)}, 1) + \bar{c}_p(\bar{\lambda}^{(j)}, 1)
								\end{align*}
							</span>
							<ul>
								<li>
									some text...
								</li>
							</ul>
						</div>
						<div style="text-align: center;">
							<h6>adjoint solution $\int_{S^2}\int_{\mathcal{E}} \lambda^{(j)} \, d\epsilon \,d\Omega$</h6>
							<img width="600px" src="figures/upd_epma/epma_tangent_adjoint.png">
						</div>
						<div style="text-align: center;">
							<h6>Taylor remainder</h6>
							<img width="600px" src="figures/upd_epma/epma_taylor_remainder.png">
						</div>
					</div>
				</section>

				<section style="text-align: left;">
					<h3>Outlook: Material Reconstruction in EPMA</h3>
					<ul>

					</ul>
					
					<div class="r-stretch"></div>
					<div style="display: flex;">
						<div style="font-size: 60%; flex:3;">
							<h4 style="font-size: 150%;">References</h4>
							<ul>
								<li>J. Bünger: <b>Three-dimensional modelling of x-ray emission in electron probe microanalysis based on deterministic transport equations</b>. <a href="https://doi.org/10.18154/RWTH-2021-05180">doi:10.18154/RWTH-2021-05180</a></li>
								<li>T. Bui-Tanh: <b>Adjoint and Its roles in Sciences, Engineering, and Mathematics: A Tutorial</b>. <a href="https://doi.org/10.48550/arXiv.2306.09917">doi:10.48550/arXiv.2306.09917</a></li>
								<li>J.A. Halbleib and J.E. Morel: <b>Adjoint Monte Carlo Electron Transport in the Continuous-Slowing-Down-Approximation</b>. <a href="https://doi.org/10.1016/0021-9991(80)90106-0">doi:10.1016/0021-9991(80)90106-0</a></li>
							</ul>
						</div>
						
						<div style="flex:1 ; font-size: 60%; margin-left:3px;"> 
							<a href="https://github.com/tam724/claus_applying_adjoints_twice_presentation">
								<div style="display:flex; ">
									<div>
										<img width="350px" src="figures/tam724_github_qr.png" style="display: inline;">
									</div>
									<div>
										<h4 style="margin-top: 30px;">Slides + Pluto.jl notebook (Poisson example)</h4>
										github.com/tam724
									</div>
								</div>

							</a>
						</div>
					</div>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/

			Reveal.configure({pdfSeparateFragments: true});
			Reveal.initialize({
				hash: true,
				controls: true, 
				controlsLayout: 'bottom-right',
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: 1920,
				height: 1440,

				// Factor of the display size that should remain empty around
				// the content
				margin: 0.05,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.01,
				maxScale: 2.0,
				center: false,

				transition: 'none',
				backgroundTransition: 'fade-in',

				slideNumber: slide => {
					// Ignore numbering of vertical slides
					return [ Reveal.getIndices(slide ).h+"/"+(Reveal.getHorizontalSlides().length-1)];},

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
			});
		</script>
	</body>
</html>
