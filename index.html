<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Appying Adjoints Twice</title>

		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300..700&display=swap" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&family=Quicksand:wght@300..700&display=swap" rel="stylesheet">
		
		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/serif.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
		<style>
			.fragment.blur {
				filter: blur(10px);
			}
			.fragment.blur.visible {
			  filter: none;
			}
			.section {
				text-align: left;
			}
			.reveal .slides{
				border-style: solid;
				border-color: gray;
				border-width: 1px;
			}
			.tr {
				border-style: none;
			}
		  </style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Title frame -->
				<section class="center">
					<div class="r-stretch"></div> 
					<h3 style="margin-bottom: 100px;">
						Applying Adjoints Twice: Efficient Gradients for Inverse Modeling with Radiative Transfer in EPMA
					</h3>
					<p style="margin-bottom:50px; "><strong>Tamme Claus</strong><sup>1</sup>, Gaurav Achuda<sup>2</sup>, Silvia Richter<sup>2</sup>, Manuel Torrilhon<sup>1</sup></p>
					<p><sup>1</sup> ACoM, Applied and Computational Mathematics, RWTH Aachen University</p>
					<p><sup>2</sup> GFE, Central Facility for Electron Microscopy, RWTH Aachen University</p>
					<p style="margin-top:50px; "><em>IGPM Seminar 2025, RWTH Aachen</em></p>
					<p style="margin-top:50px; "><em>23.01.2025</em></p>
					<div style=" margin-top:300px; text-align: right;">
						<img src="figures/gfe_logo.svg" height="290px"> 
						<img src="figures/rwth_acom_en_cmyk.svg" height="300px">
					</div>
				</section>
				
				<section>
					<!-- Material Reconstruction in EPMA-->
					<section  style="text-align: left">
						<h3>Motivation: Electron Probe Microanalysis (EPMA)</h3>
						<div style="display:flex;">
							<div style="flex:1; margin-right: 40px; margin-top:-20px;">
								<p style="text-align: center; "><em>"Material imaging based on characteristic x-ray emission"</em></p>
								<ul style="margin-top: 0px;">
									<li>Material Science, Quality control, Mineralogy, Semiconductors</li>
									<li>Ionization of material sample using high-energy focused electron beam</li>
									<li>Wavelength-dispersive spectrometers measure characteristic x-radiation</li>

								</ul>
							</div>
							<div style="flex:1; text-align: center;">
								<img src="figures/epma.jpg" height="400px">
								<p style="font-size:60% ; margin-top: -20px;">Microprobe at GFE (source: <a href="https://www.gfe.rwth-aachen.de/cms/gfe/dienste/~rhxt/esma/?lidx=1">gfe.rwth-aachen.de</a>)</p>
							</div> 
						</div>
						<div style="display:flex;">
							<div class="r-stack" style="flex:2; text-align: center;">
								<div class="fragment fade-in-then-out" data-fragment-index="1">
									<img src="figures/01_material.jpg" alt="Material" width="700px" style="margin-top: 60px;">
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="2">
									<video loop data-autoplay width="700px" style="margin-top: 40px;">
										<source data-src="figures/02_forward.webm" type="video/webm">
									</video>
								</div>
								<!-- <div class="fragment fade-in-then-out" data-fragment-index="3">
									<video loop data-autoplay  width="700px" style="margin-top: 40px;">
										<source data-src="figures/03_ionization.webm" type="video/webm">
									</video>
								</div> -->
								<div class="fragment fade-in-then-out" data-fragment-index="3">
									<img src="figures/04_interaction_volume.jpg" alt="Interaction Volume"  width="700px" style="margin-top: 60px;">
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="4">
									<video loop data-autoplay  width="700px" style="margin-top: 40px;">
										<source data-src="figures/05_linescan.webm" type="video/webm">
									</video>
								</div>
								<div class="fragment fade-in-then-out" data-fragment-index="5">
									<video loop data-autoplay width="700px" style="margin-top: 40px;">
										<source data-src="figures/06_linescan.webm" type="video/webm">
									</video>
								</div>
								<div class="fragment fade-in" data-fragment-index="6">
									<video data-autoplay width="700px" style="margin-top: 40px;">
										<source data-src="figures/07_linescan.webm" type="video/webm">
									</video>
								</div>
							</div>
							<div style="flex:3">
								<ul>
									<li class="fragment fade-in" data-fragment-index="1">Example Material: two-phase (<span style="color: orange">A</span>, <span style="color: green;">B</span>)</li>
									<li class="fragment fade-in" data-fragment-index="2"><span style="color:blue">Electron</span> propagation affected by scattering (animated: different energies) </li>
									<li class="fragment fade-in" data-fragment-index="3">Ionization and emission of characteristic x-rays (wavelength of <span style="color:orange">A-rays</span> and <span style="color:green">B-rays</span> differ)</li>
									<!-- <li class="fragment fade-in" data-fragment-index="4">Measure wavelength-dispersive x-ray intensity</li> -->
									<li class="fragment fade-in" data-fragment-index="4">Spatial information gained via different beam positions (linescan), <span class="fragment fade-in" data-fragment-index="5">beam angles </span> <span class="fragment fade-in" data-fragment-index="6">and beam energies</span></li>
									<li class="fragment fade-in" data-fragment-index=7"><em>inverse problem of material reconstruction</em>
										<span class="fragment fade-in" data-fragment-index="7">
											\begin{equation}
												p^* = \underset{p \in P}{\text{arg min}} \underbrace{|| \Sigma(p)  - \tilde{\Sigma} ||^2}_{=C(p)}
											\end{equation}
										</span>
									</li>
									<li class="fragment fade-in" data-fragment-index="8">gradient-based iterative methods (Steepest Descent, BFGS, or HMC)</li>
									<li class="fragment fade-in" data-fragment-index="9"><em>we focus on computing $\nabla_p C(p)$ efficiently via adjoints</em></li>
								</ul>
							</div>
						</div>
					</section>
					
					<!-- Gradients in Inverse Problems-->
					<section style="text-align: left;">
						<h3>Gradients in Inverse Problems</h3>
						<span><em>"Determine the model parameters $p$ such that the model $\Sigma(p)$ produces the observed data $\tilde{\Sigma}$."</em></span>
						<div style="display:flex;">
							<div style="flex:2 ;">
								<h4 style="margin-top: 50px;">classical approach</h4>
								\begin{align*}
									p^* = \underset{p \in P}{\text{arg min}} || \Sigma(p)  - \tilde{\Sigma} ||^2
								\end{align*}
								<ul>
									<li>iterative approximation of the minimum based on the gradient of the objective function $p \mapsto ||\Sigma(p) - \tilde{\Sigma}||^2$</li>
									<li>Gradient Descent, CG, BFGS, ...</li>
									<li>regularization: addition of a penalty term $p \mapsto ||\Sigma(p) - \tilde{\Sigma}||^2 + \mathcal{R}(p)$</li>
									<li>we will do: $\Sigma = \Sigma \circ \gamma$</li>
								</ul>
							</div>
							<div style="flex:1; ">
								<img src="figures/gd.png" width="500px">
							</div>
						</div>
						<div style="display:flex;">
							<div style="flex:1; ">
								<img src="figures/hmc.png" width="500px">
							</div>
							<div style="flex:2">
								<h4 style="margin-top: 50px;">bayesian approach</h4>
								<ul>
									<li>approximate the posterior $\pi(p|\tilde{\Sigma})$ (typically: iterative sampling, MCMC)</li>
									<li>Hamiltonian Monte-Carlo (based on the gradient of the target density)</li>
								</ul>
							</div>
						</div>
						</ul>
						<p style="text-align: center;"><em>Efficient evaluation of the objective function and its gradient is crucial!</em></p>
					</section>
				</section>

				<section>
					<!-- Matrix Product Bracketing -->
					<section style="text-align: left">
						<h3>
							Motivation: Matrix Product Bracketing
						</h3>
						Consider Matrix Multiplication ($N \gg I, J$):
						<div class="r-stack">
							<span class="fragment fade-out" data-fragment-index="1">\[
								\underbrace{\begin{pmatrix}
								&   &   \\
								& \Sigma^{(ji)} &   \\
								&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
								\color{white}{\Bigg(}
								\underbrace{\begin{pmatrix}
								— & h^{(1)} & — \\
								— & h^{(j)} & — \\
								— & h^{(J)} & — \\
								\end{pmatrix}}_{J \times N} 
								\cdot{}
								\color{white}{\Bigg(}
								\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
								\color{white}{\Bigg)
								}
								\cdot{}
								\underbrace{
								\begin{pmatrix}
									| & | & | \\
									g^{(1)} &  g^{(i)} & g^{(I)} \\
									| & | & | \\
								\end{pmatrix}}_{N \times I}
								\color{white}{\Bigg)}
							\]</span>
							<span class="fragment fade-in-then-out" data-fragment-index="1">\[
								\underbrace{\begin{pmatrix}
								&   &   \\
								& \Sigma^{(ji)} &   \\
								&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
								\color{blue}{\Bigg(}
								\underbrace{\begin{pmatrix}
								— & h^{(1)} & — \\
								— & h^{(j)} & — \\
								— & h^{(J)} & — \\
								\end{pmatrix}}_{J \times N} 
								\cdot{}
								\color{white}{\Bigg(}
								\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
								\color{blue}{\Bigg)
								}
								\cdot{}
								\underbrace{
								\begin{pmatrix}
									| & | & | \\
									g^{(1)} &  g^{(i)} & g^{(I)} \\
									| & | & | \\
								\end{pmatrix}}_{N \times I}
								\color{white}{\Bigg)}
							\]</span>
							<span class="fragment fade-in-then-out" data-fragment-index="2">\[
								\underbrace{\begin{pmatrix}
								&   &   \\
								& \Sigma^{(ji)} &   \\
								&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
								\color{white}{\Bigg(}
								\underbrace{\begin{pmatrix}
								— & h^{(1)} & — \\
								— & h^{(j)} & — \\
								— & h^{(J)} & — \\
								\end{pmatrix}}_{J \times N} 
								\cdot{}
								\color{red}{\Bigg(}
								\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
								\color{white}{\Bigg)
								}
								\cdot{}
								\underbrace{
								\begin{pmatrix}
									| & | & | \\
									g^{(1)} &  g^{(i)} & g^{(I)} \\
									| & | & | \\
								\end{pmatrix}}_{N \times I}
								\color{red}{\Bigg)}
							\]</span>
							<span class="fragment fade-in" data-fragment-index="3">\[
								\underbrace{\begin{pmatrix}
								&   &   \\
								& \Sigma^{(ji)} &   \\
								&   &   \end{pmatrix}^{\color{white}{T}}}_{J \times I} = 
								\color{blue}{\Bigg(}
								\underbrace{\begin{pmatrix}
								— & h^{(1)} & — \\
								— & h^{(j)} & — \\
								— & h^{(J)} & — \\
								\end{pmatrix}}_{J \times N} 
								\cdot{}
								\color{red}{\Bigg(}
								\underbrace{\boldsymbol{A}^{\color{white}{*}}}_{N \times N}
								\color{blue}{\Bigg)
								}
								\cdot{}
								\underbrace{
								\begin{pmatrix}
									| & | & | \\
									g^{(1)} &  g^{(i)} & g^{(I)} \\
									| & | & | \\
								\end{pmatrix}}_{N \times I}
								\color{red}{\Bigg)}
							\]</span>
							<!-- <span style="background-color:#fff;" class="fragment fade-in" data-fragment-index="6">\[
								\underbrace{\begin{pmatrix}
								&   &   \\
								& \Sigma^{(ij)} &   \\
								&   &   \end{pmatrix}^{\color{black}{T}}}_{J \times I} = 
								\color{white}{\Bigg(}
								\underbrace{\begin{pmatrix}
								— & g^{(1)} & — \\
								— & g^{(j)} & — \\
								— & g^{(J)} & — \\
								\end{pmatrix}}_{J \times N}
								\cdot{}
								\color{white}{\Bigg(}
								\underbrace{\boldsymbol{A}^*}_{N \times N}
								\color{white}{\Bigg)}
								\cdot{}
								\underbrace{
								\begin{pmatrix}
									| & | & | \\
									h^{(1)} &  h^{(i)} & h^{(I)} \\
									| & | & | \\
								\end{pmatrix}}_{N \times I}
								\color{white}{\Bigg)}
							\]</span> -->
						</div>
						<h4>Where to bracket?</h4>
						<p style="text-align: center" class="fragment fade-in" data-fragment-index="4">
							different computational costs: $\color{red}{I\times N^2 + IJ\times N} \text{ or } \color{blue}{J\times N^2 + IJ\times N}$
						</p>
						<span class="fragment fade-in" data-fragment-index="5">
							<h4>Generalizations: </h4>
							<ul>
								<li>
									$\boldsymbol A$ is solution of a linear system / linear (partial?) differential equation?
									\[
										\boldsymbol A g^{(i)} = \{u \in V \, | \, a(u, v) + \langle g^{(i)}, v \rangle = 0 \quad \forall v \in V\}
									\]
								</li>
								<li>
									$\boldsymbol A$ is a (Frechet-) derivative (structure: chain rule, product rule, ...)?
									\[
										\boldsymbol A g^{(i)} = \frac{\partial f(y(x))}{\partial x} g^{(i)} = \frac{\partial f(y(x))}{\partial y}\frac{\partial y(x)}{\partial x}g^{(i)}
									\]
								</li>
							</ul>
						</span>
						<span class="fragment fade-in" data-fragment-index="6">
							<p><h4 style="display:inline;">Two equivalent formulations</h4> using the <em>adjoint</em> operator $\boldsymbol A ^*$</p>
								\[
								\underbrace{\Sigma}_{J \times I} = \underbrace{H}_{J \times \infty} \cdot{}  \underbrace{\boldsymbol A}_{\infty \times \infty} \cdot{} \underbrace{G}_{\infty \times I} \quad \Leftrightarrow \quad \underbrace{\Sigma^T}_{I \times J} = \underbrace{G^T}_{I \times \infty} \cdot{}  \underbrace{\boldsymbol A^*}_{\infty \times \infty} \cdot{} \underbrace{H^T}_{\infty \times J}
								\]
							
						</span>
					</section>
				</section>

				<!-- An abstract adjoint method -->
				<section style="text-align: left;">
					<section style="text-align: left";>
						<h3>An Abstract Adjoint Method in a Computational Context</h3>
						<p>
							Definition of the <b>Adjoint</b> ($H, G$ Hilbert spaces, $A: G \to H$ continuous, linear)
							\[
								\langle h, A(g) \rangle_H = \langle A^*(h), g \rangle_G \quad \forall h \in H \,,  g \in G
							\]
						</p>
						<div class="fragment fade-in">
							<ul>
								<li>given multiple $g^{(1)}, ... g^{(I)} \in G$ and $h^{(1)}, ... h^{(J)} \in H$ we want to compute</li>
							</ul>
							\[
							\Sigma^{(ji)} = \langle h^{(j)}, A(g^{(i)}) \rangle_H = \langle A^*(h^{(j)}), g^{(i)} \rangle_G \quad \forall i=1, ...I\, j=1, ...J
							\]
							<table style="text-align: left;">
								<tr style="border-style: hidden;">
									<td style="padding:10px; "</td>
										<h4>Non-adjoint Implementation</h4>
									</td>
									<td style="width:100px; "></td>
									<td style="padding:10px; ">
										<h4>Adjoint Implementation</h4>
									</td>
								</tr>
								<tr style="border-style: hidden;">
									<td style="vertical-align: top; padding:0px;">
										<div style="margin-top: -20px;">
											\begin{align*}
												&\text{foreach } \color{orange}{i = 1, ..., I} \text{ do} \\
												&\qquad \color{orange}{v^{(i)} \leftarrow A(g^{(i)})}\\
												&\qquad \text{foreach } \color{green}{j = 1, ..., J} \text{ do}\\
												&\qquad \qquad \Sigma^{(ji)} \leftarrow \langle \color{green}{h^{(j)}}, \color{orange}{v^{(i)}} \rangle_H\\
												&\qquad \text{end}\\
												&\text{end}
											\end{align*}
										</div>
									</td>
									<td style="width:50px; "></td>
									<td style="vertical-align: top; padding:0px; ">
										<div style="margin-top: -20px;">
											\begin{align*}
												&\text{foreach } \color{green}{j = 1, ..., J} \text{ do} \\
												&\qquad \color{green}{\lambda^{(j)} \leftarrow A^*(h^{(j)})}\\
												&\qquad \text{foreach } \color{orange}{i = 1, ..., I} \text{ do}\\
												&\qquad \qquad \Sigma^{(ji)} \leftarrow \langle \color{green}{\lambda^{(j)}}, \color{orange}{g^{(i)}} \rangle_G\\
												&\qquad \text{end}\\
												&\text{end}
											\end{align*}
										</div>
									</td>
								</tr>
								<tr style="border-style: hidden;">
									<td>
										<ul>
											<li>cost: $\color{orange}{I} \times \mathcal{C}(A) + \color{orange}{I}\color{green}{J} \times \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_H)$</li>
										</ul>
									</td>
									<td style="width:100px; "></td>
									<td>
										<ul>
											<li>cost: $\color{green}{J} \times \mathcal{C}(A^*) + \color{orange}{I}\color{green}{J} \times \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_G)$
											</li>
										</ul>
									</td>
								</tr>
							</table>
						</div>
						<div class="fragment fade-in" style="margin-top:30px;">
							<ul>
								<li>typically: $\mathcal{C}(A) \approx \mathcal{C}(A^*)$ and $\mathcal{C}(\langle \cdot{}, \cdot{} \rangle_G) \approx \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_H)$ and $\mathcal{C}(A) \gg \mathcal{C}(\langle \cdot{}, \cdot{} \rangle_G)$ </li>
								<ul>
									<li>$\color{green}{J} > \color{orange}{I}$: non-adjoint implementation</li>
									<li>$\color{orange}{I} > \color{green}{J}$: adjoint implementation</li>
								</ul>
							</ul>
						</div>
						<div class="fragment fade-in" style="background-color: lightgray; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 5px; font-size: 100%; margin:30px;"> 
							<em>"Instead of computing/approximating the "solution" $v^{(i)}= A(g^{(i)})$, we approximate the <b>Riesz representation</b> $\lambda^{(j)}$ of $\cdot \mapsto \langle h^{(j)}, A(\cdot{}) \rangle_H$"</em>
						</div>
						<!-- <div style="background-color: lightgray; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 5px; font-size: 80%;">
							Notation: $f_{(\cdot{})}: X \to Y$ non-linear, differentiable
							<ul>
								<li><em>tangent operator</em> $\dot{f}_x(\dot{x}) = \lim_{h \to 0} \frac{f_{x+h\dot{x}} - f_x}{h}$</li>
								<li><em>adjoint operator</em> $\bar{f}_x: \langle \bar{y}, \dot{f}_x(\dot{x}) \rangle_Y = \langle \bar{f}_x(\bar{y}), \dot{x}\rangle_X \, \forall \bar{y}, \dot{x}$</li>
								<li><em>tangent variables</em> $\dot{x}, \dot{y}$, where $\dot{y} = \dot{f}_x(\dot{x})$</li>
								<li><em>adjoint variables</em> $\bar{x}, \bar{y}$, where $\bar{x} = \bar{f}_x(\bar{y})$</li>
							</ul>
						</div> -->
					</section>

					<section style="text-align: left;">
						<h3>The adjoint of the solution of a linear weak form (linear pde)</h3>
						$A: G \to H$ is the solution operator of a linear weak form ($U, G, H$ Hilbert spaces: $U \subseteq G, H$)
						<p>
							<ul>
								<li>
									$\color{orange}{A(g) = (u \in U | a(u, v) + b(v) = 0\, \forall v \in U)}$ where $b(v) = \langle v, g\rangle_G$
								</li>
								<li>
									$\color{green}{A^*(h) =}$ <span class="fragment custom blur" data-fragment-index="2">$ \color{green}{(\lambda \in U | a(v, \lambda) + c(v) = 0 \, \forall v \in U)}$ where $c(\lambda) = \langle h, \lambda \rangle_H$</span>
								</li>
							</ul>
						</p>
						<p class="fragment fade-in" data-fragment-index="1">
							Derivation: we introduce $\lambda \in U$ (in continuous adjoint method (for $\partial_p$): "Lagrange-multiplier")
							\begin{align*}
							\langle h, A(g) \rangle_H = \langle h, u \rangle_H &= c(u) \\
							&=c(u) + \color{orange}{\underbrace{a(u, \lambda) + b(\lambda)}_{=0 \, \forall \lambda \in U}} \\
							&=\color{green}{\underbrace{c(u) + a(u, \lambda)}_{=0 \, \forall u \in U}} + b(\lambda) \\
							& \hphantom{= c(u) + a(u, \lambda)} = b(\lambda) = \langle \lambda, g \rangle_G = \langle A^*(h), g \rangle_G
							\end{align*}
						</p>
						<div style="margin-top: 30px;" class="fragment fade-in" data-fragment-index="3" >
							<ul><li>given multiple $g^{(i)} \in G$ ($b^{(i)} \in L(G, \mathbb{R})$) and $h^{(j)} \in H$ ($c^{(j)} \in L(H, \mathbb{R})$)</li></ul>
							<div style="display:flex; margin-top: 30px;">
								 <div style="flex: 1;">
									<h4>non-adjoint implementation</h4>
										\begin{align*}
											&\text{foreach } \color{orange}{i = 1, ... I} \\
											&\qquad \color{orange}{\text{solve } a(u^{(i)}, v) + b^{(i)}(v) = 0 \quad \forall v \in U}\\
											&\qquad \text{foreach } \color{green}{j = 1, ...J}\\
											&\qquad \qquad \Sigma^{(ij)} = \color{green}{c^{(j)}}(\color{orange}{u^{(i)}})\\
											&\qquad \text{end}\\
											&\text{end} 
										\end{align*}
								</div>
								<div style="flex: 1;">
									<h4>adjoint implementation</h4>
										\begin{align*}
											&\text{foreach } \color{green}{j = 1, ...J} \\
											&\qquad \color{green}{\text{solve } a(v, \lambda^{(j)}) + c^{(j)} = 0 \quad \forall v \in U}\\
											&\qquad \text{foreach } \color{orange}{i = 1, ...I}\\
											&\qquad \qquad \Sigma^{(ij)} = \color{orange}{b^{(i)}}(\color{green}{\lambda^{(j)}})\\
											&\qquad \text{end}\\
											&\text{end}
										\end{align*}
								</div>
							</div>
							<ul>
								<li>switching trial and test function is the continuous analog to solving the transposed system</li>
							</ul>
						</div>
					</section>

				<!-- Riesz Representation Theorem/Def Adjoint -->
				<section style="text-align: left;">
					<h3>Riesz Representation Theorem/Definition of the Adjoint</h3>

					<h4>Riesz Representation Theorem</h4>
					<p>
						Let $(H, \langle \cdot{}, \cdot{} \rangle_H)$ be a (real) Hilbert space with inner product $\langle \cdot{}, \cdot{} \rangle_H$. For every continuous linear functional $f_h \in H^*$ (the dual of $H$), there exists a unique vector $h \in H$, called the <em>Riesz Representation</em> of $f_h$, such that
						\[
							f_h(x) = \langle h, x \rangle_H \quad \forall x \in H.
						\]
					</p>

					<h4>Definition: Adjoint Operator</h4>
					<p>
						Let $(G, \langle \cdot{}, \cdot{} \rangle_G)$ be another (real) Hilbert space, $A:G\to H$ a continuous linear operator between $G$ and $H$ and $f_h \in  H^*$ a continuous linear functional.
						\[
							f_h(A(g)) = \langle h, A(g) \rangle_H \quad \forall g \in G
						\]
						But $f_h(A(g))$ is also a continuous linear functional $f_\lambda(g)$ in $G$ with a Riesz Representation $\lambda \in G$
						\[
							f_h(A(g)) = f_\lambda(g) = \langle \lambda, g \rangle_G := \langle A^*(h), g \rangle_G.
						\]
					</p>
				</section>
					<section style="text-align: left;">
						<h3>Examples of (real) adjoint operators</h3>
						<div>
							<h4 style="color:forestgreen;">Scalar Multiplication</h4>
							<ul>
								<li>$G, H = \mathbb{R}$</li>
								<li>$A: \mathbb{R} \to \mathbb{R}, g \mapsto Ag = a \cdot{} g,\quad  a \in \mathbb{R}$</li>
								<li>$A^*: \mathbb{R} \to \mathbb{R}, \mu \mapsto A^* \mu = a \cdot{} \mu$</li>
							</ul>
							<p>
								<em>Derivation:</em>
								\begin{equation*}
									\langle \mu, Ag \rangle_{\mathbb{R}} = \langle \mu, a \cdot{} g \rangle_{\mathbb{R}} = \langle a \cdot{} \mu, g \rangle_{\mathbb{R}} = \langle A^*\mu , g \rangle_{\mathbb{R}} \quad \forall g \in \mathbb{R}\, \forall \mu \in \mathbb{R}
								\end{equation*}
							</p>
						</div>
						<div class="fragment fade-in">
							<h4 style="color:forestgreen;">Matrix Multiplication</h4>
							<ul>
								<li>$G = \mathbb{R}^n, H = \mathbb{R}^m$</li>
								<li>$A: \mathbb{R}^n \to \mathbb{R}^m, g \mapsto Ag = M \cdot{} g, \quad M \in \mathbb{R}^{m \times n}$</li>
								<li>$A^*: \mathbb{R}^m \to \mathbb{R}^n, \mu \mapsto A^*\mu = M^T \cdot{} \mu$</li>
							</ul>
							<p>
								<em>Derivation:</em>
								\begin{equation*}
									\langle \mu, Ag \rangle_{\mathbb{R}^m} = \langle \mu, M \cdot{} g \rangle_{\mathbb{R}^m} = \langle M^T \cdot{} \mu, g \rangle_{\mathbb{R}^n} = \langle A^*\mu, g \rangle_{\mathbb{R}^n} \quad \forall g \in \mathbb{R}^n \, \forall \mu \in \mathbb{R}^m
								\end{equation*}
							</p>
						</div>
						<div class="fragment fade-in">
							<h4 style="color:forestgreen;">Integration</h4>
							<ul>
								<li>$G = L^2(\Omega)$ (with $\langle \cdot{}, \cdot{} \rangle_{L^2(\Omega)} = \int_{\Omega} \cdot{} \cdot{} d x$), $H = \mathbb{R}$</li>
								<li>$A: L^2(\Omega) \to \mathbb{R}, g(\cdot{}) \mapsto Ag = \int_{\Omega} g(x) d x$</li>
								<li>$A^*: \mathbb{R} \to L^2(\Omega), \mu \mapsto (A^* \mu)(\cdot{}) = (x \mapsto \mu \cdot{} 1_{\Omega}(x))$</li>
							</ul>
							<p>
								<em>Derivation:</em>
								\begin{equation*}
									\langle \mu, Ag \rangle_\mathbb{R} = \mu \cdot{} \int_\Omega g(x) dx = \int_\Omega \mu \cdot{} 1_\Omega(x) g(x) dx = \langle (A^*\mu)(\cdot{}), g(\cdot{}) \rangle_{L^2(\Omega)} \quad \forall g \in L^2(\Omega) \, \forall \mu \in \mathbb{R}
								\end{equation*}
							</p>
						</div>
					</section>
					<section style="text-align: left;">
						<h3>Examples of (real) adjoint operators</h3>
						<h4 style="color:forestgreen;">Linear Solve</h4>
						<ul>
							<li>$G, H = \mathbb{R}^n$ and $M \in \mathbb{R}^{n\times n}$ invertible</li>
							<li>$A: \mathbb{R}^n \to \mathbb{R}^n, g \mapsto (h \in \mathbb{R}^n | M \cdot{} h = g)$, (or $A = M^{-1}$)</li>
							<li>$A^*: \mathbb{R}^n \to \mathbb{R}^n, \mu \mapsto (\lambda \in \mathbb{R}^n | M^T \cdot{} \lambda = \mu)$, (or $A^* = M^{-T}$)</li>
						</ul>
						<p>
							<em>Derivation (intentionally complicated):</em> We reinterpret
							\begin{equation*}
								M \cdot{} h = g \quad \Leftrightarrow \quad \langle v,  M \cdot{} h \rangle = \langle v, g \rangle \quad \forall v \in G 
							\end{equation*}
							in particular (for a fixed but unspecified $\lambda \in G$)
							\begin{equation}
								0 = \langle \lambda, g \rangle - \langle \lambda, M \cdot{} h \rangle \quad \Leftrightarrow \quad 0 = \langle \lambda, g \rangle - \langle M^T \cdot{} \lambda, h \rangle
							\end{equation}
							\begin{align*}
								\langle \mu, Ag \rangle &= \langle \mu, h \rangle\\
								&= \langle \mu, h \rangle - \langle M^T \cdot{} \lambda, h \rangle + \langle \lambda, g \rangle \\
								& \quad \text{ with } \langle \mu, h \rangle - \langle M^T \cdot{} \lambda, h \rangle = 0 \quad \forall h \in H \\
								& \quad \text{ or } M^T \cdot{} \lambda = \mu \\
								&= \langle A^* \mu, g \rangle
							\end{align*}
							(alternatively)
							\begin{equation*}
								\langle \mu, Ag \rangle_{\mathbb{R}^n} = \langle \mu, M^{-1} g \rangle_{\mathbb{R}^n} = \langle M^{-T} \mu, g \rangle_{\mathbb{R}^n} = \langle A^*\mu, g \rangle_{\mathbb{R}^n}
							\end{equation*}
						</p>
					</section>

					<section style="text-align: left;">
						<h3>Examples of (real) adjoint operators</h3>
						<h4 style="color:forestgreen;">Weak form</h4>
						<ul>
							<li>$G, H$ (real Hilbert spaces)</li>
							<li>$A: G \to H, g \mapsto \{h \in H | a(h, v) + \langle g, v \rangle_G = 0 \quad \forall v \in G\}$, $\quad a(\cdot{}, \cdot{}) \text{ coercive bilinear form}$</li>
							<li>$A^*: H \to G, \mu \mapsto \{\lambda \in G | a(v, \lambda) + \langle \mu,v \rangle_H = 0 \quad \forall v \in H\}$</li>
						</ul>
						<p>
							<em>Derivation:</em>
							\begin{align*}
								\langle \mu, Ag \rangle_H = f_\mu(h) &= f_\mu(h) + \underbrace{a(h, \lambda) + f_g(\lambda)}_{=0\quad \forall \lambda \in G} \\
								&= \underbrace{f_\mu(h) + a(h, \lambda)}_{!= 0 \quad \forall h \in H} + f_g(\lambda) = f_g(\lambda) = \langle \lambda, g \rangle_G
							\end{align*}
						</p>
					</section>

					<section style="text-align: left;">
						<h3>Derivative Notation (from Algorithmic Differentiation)</h3>
						<ul>
							<li>given $f_{(\cdot{})}: X \to Y, x \mapsto f_x$ ($X, Y$ Hilbert) non-linear, Frechet-differentiable</li>
						</ul>
						<p>
							we define:
						</p>	
						<ul>
							<li>the (continuous, linear) <em>tangent operator</em> $\dot{f}_x \in L(X, Y)$ (directional derivative)
								\begin{align*}
									\dot{f}_x(\dot{x}) = \frac{\partial f_x}{\partial x}(\dot{x}) = \lim_{h \to 0} \frac{f_{x + h\dot{x}} - f_{x}}{h}
								\end{align*}
							</li>
							<li>the (continuous, linear) <em>adjoint operator</em> $\bar{f}_x \in L(Y, X)$
								\begin{align*}
									\langle \bar{y} , \dot{f}_x(\dot{x}) \rangle_Y = \langle \bar{f}_x (\bar{y}), \dot{x} \rangle_X \quad \forall \bar{y} \in Y, \dot{x} \in X
								\end{align*}
							</li>
						</ul>
						<p>
							Also we define ($y = f_x$):
							<ul>
								<li><em>tangent variables</em> $\dot{x} \in X, \dot{y} \in Y$: given $\dot{x} \in X$, $\dot{y} = \dot{f}_x(\dot{x})$</li>
								<li><em>adjoint variables</em> $\bar{x} \in X, \bar{y} \in Y$: given $\bar{y} \in Y$, $\bar{x} = \bar{f}_x(\bar{y})$</li>
							</ul>
						</p>
						Applying this to a composition $ f_x = \varphi^{(N)}_{v^{(N-1)}} \circ \, ...\, \circ\, \varphi^{(1)}_x$ where $\varphi^{(n)}$ are single assignments (chain rule!) is tangent/adjoint mode automatic differentiation. (neglecting all implementational details..)
					</section>

					<section style="text-align: left;">
						<h3>Automatic/Algorithmic Differentiation</h3>
						<h4>Example</h4>
						\begin{equation}
							y = f_x = g \circ h_x \quad v = h_x
						\end{equation}
						by the chain rule, the <em>tangent operator</em> $\dot{f}_x$ is
						\begin{equation}
							\dot{y} = \dot{f}_x(\dot{x}) = \dot{g}_v(\dot{h}_x(\dot{x}))
						\end{equation}
						for the <em>adjoint operator</em> $\bar{f}_x$ we use 
						\begin{align*}
							\langle \bar{y} , \dot{f}_x(\dot{x}) \rangle_Y = \langle \bar{y}, \dot{g}_v(\dot{h}_x(\dot{x})) \rangle_Y = \langle \bar{g}_v (\bar{y}), \dot{h}_x(\dot{x}) \rangle_V = \langle \bar{h}_x(\bar{g}_v(\bar{y})), \dot{x} \rangle_X \quad \forall \bar{y} \in Y, \dot{x} \in X
						\end{align*}
						and find
						\begin{equation}
							\bar{f}_x = \bar{h}_x(\bar{g}_v(\bar{y}))
						\end{equation}
						<div style="display:flex;">
							<div style="flex: 1;">
								<h4>Tangent mode</h4>
								\begin{align*}
								&\text{foreach } j = 1, ... J \\
								&\qquad \dot{x} \leftarrow e_j \\
								&\qquad \dot{y} \leftarrow \dot{g}_v(\dot{h}_x(\dot{x}))\\
								&\qquad \text{foreach } i = 1, ... I \\
								&\qquad \qquad (Df)^{(i, j)} = \langle e_j, \dot{y} \rangle\\
								&\qquad \text{end}\\
								&\text{end}
								\end{align*}
							</div>
							<div style="flex: 1;">
								<h4>Adjoint mode</h4>
								\begin{align*}
								&\text{foreach } i = 1, ... I \\
								&\qquad \bar{y} \leftarrow e_i \\
								&\qquad \bar{x} \leftarrow \bar{h}_x(\bar{g}_v(\bar{y}))\\
								&\qquad \text{foreach } j = 1, ... J \\
								&\qquad \qquad (Df)^{(i, j)} = \langle \bar{x}, e_j \rangle\\
								&\qquad \text{end}\\
								&\text{end}
								\end{align*}
							</div>
						</div>
						<ul>
							<li>AD is even more systematic..</li>
						</ul>
					</section>

					<section style="text-align: left;">
						<h3>Automatic Algorithmic Differentiation</h3>
						Every numerical program $f: \mathbb{R}^{n-1} \to \mathbb{R}^{m+1}$ can be decomposed into <em>single assignments</em> $(\varphi^{(1)}, ... \varphi^{(N)})$.
						\begin{align*}
							&(v^{(0)}, v^{(-1)}, ..., v^{(-n)})^T = x \\
							&v^{(n)} = \varphi^{(n)}_{(v^{(j)})_{j \prec n}} \quad \forall n = 1, ... N \\
							&y = (v^{(N)}, v^{(N-1)}, ..., v^{(N-m)})^T
						\end{align*}
						<p>
							For every single assignment $\varphi^{(n)}_{(v^{(j)})_{j \prec n}}$ we know
						</p>
						<ul>
							<li>
								<em>tangent operator </em> $\dot{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\dot{v}^{(j)})_{j \prec n})$ and
							</li>
							<li>
								<em>adjoint operator</em> $\bar{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\bar{v}^{(k)})_{k \succ n})$
							</li>
						</ul>

						<div style="display:flex;">
							<div style="flex: 1;">
								<h4>Tangent mode</h4>
								\begin{align*}
									&(v^{(0)}, v^{(-1)}, ...)^T = x \\
									&(\dot{v}^{(0)}, \dot{v}^{(-1)}, ...)^T = \dot{x} \\
									&\hspace{-30px}\begin{cases}
										v^{(n)} = \varphi^{(n)}_{(v^{(j)})_{j \prec n}} \\
										\dot{v}^{(n)} = \dot{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\dot{v})_{j \prec n})
									\end{cases} \small{\quad \forall n = 1, ... N}\\
									&y = (v^{(N)}, v^{(N-1)}, ...)^T\\
									&\dot{y} = (\dot{v}^{(N)}, \dot{v}^{(N-1)}, ...)^T
								\end{align*}
							</div>
							<div style="flex: 1;">
								<h4>Adjoint mode</h4>
								\begin{alignat*}{2}
									&(v^{(0)}, v^{(-1)}, ...)^T = x \\
									&v^{(n)} = \varphi^{(n)}_{(v^{(j)})_{j \prec n}} &&\small{\quad \forall n = 1, ..., N}\\
									&y = (v^{(N)}, v^{(N-1)}, ...)^T\\
									&(\bar{v}^{(N)}, \bar{v}^{(N-1)}, ...)^T = \bar{y} \\
									&\bar{v}^{(n)} = \bar{\varphi}^{(n)}_{(v^{(j)})_{j \prec n}}((\bar{v}^{(k)})_{k \succ n}) &&\small{\quad \forall n = N-m-1, ..., -n}\\
									&\bar{x} = (\bar{v}^{(0)}, \bar{v}^{(-1)}, ...)^T
								\end{alignat*}
							</div>	
						</div>
					</section>
				</section>

				<section style="text-align: left">
					<h3>Abstract Adjoint: Algorithmic Differentiation (AD)</h3>
					<ul>
						<li>
							consider $f: \mathcal{X} \to Y, x \mapsto f(x) =: y$ with open $\mathcal{X} \subseteq X $ 
						</li>
						<li>define the (Frechet-) derivative (or tangent operator) $\dot{f}_x: T_xX \to T_yY$ by
							\begin{equation}
							\lim_{||\dot{x}|| \to 0} \frac{||f(x + \dot{x}) - f(x) - \dot{f}_x(\dot{x})||}{||\dot{x}||} \quad \text{with } \dot{x} \in T_xX
							\end{equation}
						</li>
						<li>assuming that the tangent operator is continuous and $T_xX$, $T_yY$ are Hilbert spaces:
							\begin{equation}
							\langle \bar{y}, \dot{f}_x(\dot{x}) \rangle_{T_y Y} = \langle \bar{f}_x(\bar{y}), \dot{x} \rangle_{T_x X} \quad \forall \dot{x} \in T_xX, \bar{y} \in T_yY
							\end{equation}
							<div style="font-size:60%; width:100%; text-align: right; margin-top:-30px;">(notation adopted from AD)</div>
						</li>
						<li>Choose cartesian unit vectors for $\bar{y}$ and $\dot{x}$ (seeding), then (glossing over details): 
							<ul>
								<li>Non-Adjoint Implementation = tangent mode of AD</li>
								<li>Adjoint Implementation = adjoint mode of AD</li>
							</ul>
						</li>
					</ul>

					<h4>Special Case: Implicit Algorithmic Differentiation</h4>
					<ul>
						<li>
							consider $f: \mathcal{X} \to Y, x \mapsto (y \in Y : F(x, y) = 0)$
						</li>
						<li>$\dot{f}_x(\dot{x}) = (\dot{y} \in T_yY : \dot{v} = -\dot{F}^{(1)}_{x, y}(\dot{x}) \land  \dot{F}^{(2)}_{x, y}(\dot{y}) = \dot{v} )$
						</li>
						<li>$\bar{f}_x(\bar{y}) = (\bar{x} \in T_xX : \bar{F}^{(1)}_{x, y}(\bar{v}) = \bar{x} \land \bar{y} = -\bar{F}^{(2)}_{x, y}(\bar{v}) )$
						</li>
					</ul>
					
					<div class="r-stretch"></div>
					<ul style="font-size:60%; ">
						<li>Griewank, A., & Walther, A. (2008). <b>Evaluating Derivatives</b>. <a href="https://doi.org/10.1137/1.9780898717761">doi:10.1137/1.9780898717761</a></li>
						<li>Naumann, U. (2011). <b>The Art of Differentiating Computer Programs</b>. <a href="https://doi.org/10.1137/1.9781611972078">doi:10.1137/1.9781611972078</a></li>

						<li>Pearlmutter, B. A., & Siskind, J. M. (2008). <b>Reverse-mode AD in a functional framework</b>. <a href="https://doi.org/10.1145/1330017.1330018">doi:10.1145/1330017.1330018</a></li>
					</ul>
				</section>

				<section style="text-align: left">
					<h3>Abstract Adjoint: Dual/Adjoint Consistency</h3>
					A <b>discretization</b> is dual consistent, if:
					<div style="text-align: center;">
						<em>"The adjoint of the discretized problem is a consistent discretization of the adjoint of the continuous problem."</em>
					</div>
					<ul>
						<li>with $h \in H$, $g \in G$ vectors, $A: G \to H$ continuous operator, $\cdot{}_\Delta$ the "discretization"</li>
						<li>if the discretization if dual consistent, the following diagram commutes:</li>
					</ul>
					\begin{equation}
						\begin{matrix}
							\langle h, A(g) \rangle_H & \leftrightarrow & \langle A^*(h), g \rangle_G \\
							\downarrow_{\Delta} & & \downarrow_{\Delta}\\
							\langle h_\Delta, A_\Delta \cdot{} g_\Delta \rangle_{H_{\Delta}} & \leftrightarrow & \langle A^T_{\Delta}\cdot{} h_{\Delta}, g_{\Delta} \rangle_{G_{\Delta}}
						\end{matrix}
					\end{equation}
					<ul>
						<li>
							interpretability of the "adjoint solution" $A_\Delta^T \cdot{} h_\Delta$ as the solution of the continuous adjoint equation
						</li>
						<li>
							"superconvergence" of linear functionals $\langle \cdot{}, g_\Delta\rangle_{G_\Delta}$ (convergence order doubling)
						</li>
					</ul>

					<div style="background-color: lightgray; border-radius: 10px; border-color: gray; border-width: 2px; border-style: dashed; padding: 10px; font-size: 80%; margin:30px;">
						<h5>"optimize-then-discretize" vs. "discretize-then-optimize"</h5>
						<h5>"continuous adjoint method" vs. "discrete adjoint method"</h5>
						<ul>
							<li>related concepts from optimization/optimal control literature</li>
							<li>for a dual consistent discretization both approaches are identical</li>
						</ul>
					</div>
					<div class="r-stretch"></div>

					<ul style="font-size:60%">
						<li>Hicken, J. E., & Zingg, D. W. (2014). <b>Dual consistency and functional accuracy: a finite-difference perspective</b>. <a href="https://doi.org/10.1016/j.jcp.2013.08.014">doi:10.1016/j.jcp.2013.08.014</a> 
						</li>
						<li>Oliver, T. A., & Darmofal, D. L. (2009). <b>Analysis of Dual Consistency for Discontinuous Galerkin Discretizations of Source Terms</b>. <a href="https://doi.org/10.1137/080721467">doi:10.1137/080721467</a> 
						</li>
						<li>Hartmann, R. (2007). <b>Adjoint Consistency Analysis of Discontinuous Galerkin Discretizations</b>. <a href="https://doi.org/10.1137/060665117">doi:10.1137/060665117</a> 
						</li>
						<li>Giles, M. B., & Pierce, N. A. (2003). <b>Adjoint Error Correction for Integral Outputs</b>. <a href="https://doi.org/10.1007/978-3-662-05189-4_2">doi:10.1007/978-3-662-05189-4_2</a> 
						</li>
					</ul>
				</section>

				<section style="text-align: left">
					<h3>Applying Adjoints Twice</h3>
					<div style="display:flex;">
						<div style="flex: 4; font-size:80%">
							<ul>
								<li>parameters $p \in P$</li>
								<li>parameter-dependent bilinear form $a_p: U \times V (\times P) \to \mathbb{R}$ </li>
								<li>excitations $b^{(i)} : V \to \mathbb{R}$</li>
								<li>extractions $c^{(j)}: U \to \mathbb{R}$</li>
								<li>observables $\Sigma^{(ji)} \in \mathbb{R}$</li>
							</ul>
						</div>
						<div style="flex: 3; font-size:80%">
							<ul>
								<li>tangent of $a_p$ $\dot{a}_p: U \times V \times P \to \mathbb{R}$</li>
								<li>corres. adjoint $\bar{a}_p: U \times V \times \mathbb{R} \to P$</li>
								<li>tangent spaces = spaces (?)</li>
								<li>given tangent/adjoint vectors $\dot{p}$ and $\bar{\Sigma}^{(ji)}$ resp.</li>
							</ul>
						</div>
					</div>
					<table style="width: 100%; margin-top: 50px;">
						<tr style="border-style: hidden;">
							<td style="text-align: left">
								<h4>Model ($u^{(i)} \in U)$:</h4>
								\begin{align*}
									a_p(u^{(i)}, v) + b^{(i)}(v) &= 0 \quad \forall v \in V \\
									\Sigma^{(ji)} &= c^{(j)}(u^{(i)})
								\end{align*}
							</td>
							<td class="fragment fade-in" data-fragment-index="1">
								<h4>1st "adjoint method" ($\lambda^{(j)} \in V$):</h4>
									\begin{align*}
										a_p(v, \lambda^{(j)}) + c^{(j)}(v) &= 0 \quad \forall v \in U \\
										\Sigma^{(ji)} &= b^{(i)}(\lambda^{(j)}) \\
									\end{align*}
							</td>
						</tr>
						<tr style="border-style: hidden;">
							<td class="fragment fade-in" data-fragment-index="2">
								<h4>Tangent/sensitivity model ($\dot{\lambda}^{(j)} \in V)$):</h4>
								<div class="r-stack">
									<div class="fragment fade-in" data-fragment-index="2"> 
										\begin{align*}
											a_p(v, \dot{\lambda}^{(j)}) + \dot{a}_p(v, \lambda^{(j)}, \dot{p}) &= 0 \quad \forall v \in U\\
											\dot{\Sigma}^{(ji)} &= b^{(i)}(\dot{\lambda}^{(j)})
										\end{align*}
									</div>
								</div>
								
							</td>
							<td class="fragment fade-in" data-fragment-index="4">
								<h4>2nd "adjoint method" ($\bar{\lambda}^{(j)} \in U$):</h4>
								<div class="r-stack">
									<div class="fragment fade-out" data-fragment-index="5">
										\begin{align}
											a_p(\bar{\lambda}^{(j)}, v) + \bar{\Sigma}^{(ji)} b^{(i)}(v) &= 0 \quad \forall v \in V \\
											\bar{\Sigma}^{(ji)} \dot{\Sigma}^{(ji)} &= \dot{a}_p(\bar{\lambda}^{(j)}, \lambda^{(j)}, \dot{p})
										\end{align}
									</div>
									<div class="fragment fade-in" data-fragment-index="5"> 
										\begin{align}
											a_p(\bar{\lambda}^{(j)}, v) + \bar{\Sigma}^{(ji)} b^{(i)}(v) &= 0 \quad \forall v \in V \\
											\bar{\Sigma}^{(ji)} \dot{\Sigma}^{(ji)} &= \langle \bar{a}_p(\bar{\lambda}^{(j)}, \lambda^{(j)}, 1), \dot{p} \rangle
										\end{align}
									</div>
								</div>
							</td>
						</tr>
						<tr style="border-style: hidden;">
							<td colspan=2 style="text-align: center;">
								<div class="fragment fade-in" data-fragment-index="3" style="display: inline-block; text-align: left;">
									<h4>Algorithmic Differentiation ($\langle \bar{\Sigma}, \dot{\Sigma} \rangle = \langle \bar{p}, \dot{p} \rangle$):</h4>
									\begin{align}
										a_p(\bar{\Sigma}^{(ji)} v, \dot{\lambda}^{(j)}) + \dot{a}_p(\bar{\Sigma}^{(ji)} v, \lambda^{(j)}, \dot{p}) &= 0 \quad \forall v \in U \\
										\bar{\Sigma}^{(ji)} \dot{\Sigma}^{(ji)} &= \bar{\Sigma}^{(ji)} b^{(i)}(\dot{\lambda}^{(j)})
									\end{align}
								</div>
							</td>
						</tr>
					</table>
				</section>


				<section style="text-align: left;">
					<h3>Applying Adjoints Twice: Summary</h3>
					<table>
						<tr style="border-style: hidden;">
							<td style="text-align: center; ">
								<h4>non-adjoint forward + tangent derivative</h4>
							</td>
							<td style="text-align: center; ">
								<h4>adjoint forward + adjoint derivative</h4>
							</td>
						</tr>
						<tr>
							<td style="text-align: center; ">
								\begin{align*}
									a_p(u^{(i)}, v) + b^{(i)}(v) &= 0 \quad \forall v \in V \\
									\Sigma^{(ji)} &= c^ {(j)}(u^{(i)}) \\
									C &= g(\Sigma^{(\cdot{}\cdot{})}) \\
									a_p(\dot{u}^{(in)}, v) + \dot{a}_p(u^{(i)}, v, e^{(n)}) &= 0 \quad \forall v \in V\\
									\dot{\Sigma}^{(jin)} &= c^{(jin)}(\dot{u}^{(in)}) \\
									(\nabla_p C)^{(n)} &= \dot{g}_{\Sigma^{(\cdot{}\cdot{})}}(\dot{\Sigma}^{(\cdot{}\cdot{}n)})
								\end{align*}
							</td>
							<td style="text-align: center; ">
								\begin{align}
									a_p(v, \lambda^{(j)}) + c^{(j)}(v) &= 0 \quad \forall v \in U\\
									\Sigma^{(ji)} &= b^{(i)}(\lambda^{(j)}) \\
									C &= g(\Sigma^{(\cdot{}\cdot{})}) \\
									\bar{\Sigma}^{(\cdot{}\cdot{})} &= \bar{g}_{\Sigma^{(\cdot{}\cdot{})}}(\bar{C})\\
									a_p(\bar{\lambda}^{(j)}, v) + \bar{\Sigma}^{(ji)} b^{(i)}(v) &= 0 \quad \forall v \in V \\
									(\nabla_p C)^{(n)} &= \langle \bar{a}_p(\bar{\lambda}^{(j)}, \lambda^{(j)}, 1), e^{(n)}\rangle
								\end{align}
							</td>
						</tr>
					</table>
					<h4>Generalization:</h4>
					<ul style="width: 100%;">
						<li>parameter-dependent $b_p^{(i)}(v)$ and $c_p^{(j)}(u)$</li>
						\begin{equation}
						\bar{p} = \bar{a}_p(\bar{\lambda}^{(j)}, \lambda^{(j)}, 1) + \bar{c}_p^{(j)}(\lambda^{(j)}, 1) + \bar{b}^{(i)}_p(\lambda^{(j)}, \bar{\Sigma}^{(ji)})
						\end{equation}
					</ul>
					<h4 style="margin-top: 20px;">Assumptions:</h4>
					<ul>
						<li>$\mathcal{C}(\text{"solving" } a_p(\cdot{}, \cdot{})) \gg \mathcal{C}(\text{"integrating" } b^{(i)}, c^{(j)}, ...)$ </li>
						<li>$a_p(\cdot{}, \cdot{})$ cannot be solved directly (e.g. LU)</li>
						<li>$I > J$, $N > J$, $C \in \mathbb{R}$</li>
					</ul>
					<h4 style="margin-top: 20px;">Costs:</h4>
					<ul>
						<li>non-adjoint forward + tangent derivative: $(I + IN)\times \mathcal{C}(a) + ... \mathcal{C}(\langle\cdot\rangle)$</li>
						<li>adjoint forward + adjoint derivative: $2J \times \mathcal{C}(a) + ... \times \mathcal{C}(\langle\cdot\rangle)$</li>
					</ul>
				</section>

				<section style="text-align: left;">
					<section><h3>Example: An inverse problem based on the Poisson equation</h3>
						<table>
							<tr>
								<td style="text-align: center; vertical-align: top; padding: 0px;">
									<h4>forward (strong form)</h4>
									<div style="margin-top:-30px">
										\begin{align*}
											&\begin{cases}
												-\nabla \cdot{} m \nabla u^{(i)} = 0 \quad &\forall x \in \mathcal{R}\\
												u^{(i)} = g^{(i)} \quad &\forall x \in \partial\mathcal{R}
												\end{cases}\\
											&\Sigma^{(i, j)} = \int_{\mathcal{R}} h^{(j)} u^{(i)} dx\\
											&C = \frac{1}{2 IJ}\sum_{i, j=1}^{I,J} (\Sigma^{(i, j)} - \tilde{\Sigma}^{(i, j)})^2  
										\end{align*}
									</div>
								</td>
								<td style="text-align: center; vertical-align: top; padding: 0px;">
									<ul style="margin-left: 50px; ">
										<li>$m, h^{(j)} \in L^2(\mathcal{R})$, $g^{(i)} \in L^2(\partial \mathcal{R})$</li>
										<li>weak enforcement of boundary conditions $u, v \in H^1_h(\mathcal{R})$</li>
										<li>Aubin-Nitsche trick: stable bilinear form
											\[a(u^{(i)}, v) + b^{(i)}(v) = 0 \quad \forall v \in H^1_h(\mathcal{R})\]
										</li>
										<li>$I, N \gg J$ (many bc $g^{(i)}$, many parameters $p \in \mathbb{R}^N$, some extractions $h^{(j)}$)</li>
										<li>here: $I=800$, $N=757$, $J=7$</li>
									</ul>
								</td>
							</tr>
						</table>
						<table style="text-align: center;">
							<tr style="border-style: hidden;">
								<td style="text-align: center; padding: 0px; vertical-align:top;"><h4>forward solutions $u^{(i)}$</h4></td>
								<td style="width: 200px; "></td>
								<td style="text-align: center; padding: 0px; vertical-align:top;"><h4>measurements $\Sigma^{(i, j)}$</h4></td>
							</tr>
							<tr>
								<td style="text-align: center; padding: 0px; vertical-align:top;">  
									<video data-autoplay loop style="width: 800px; margin-top: 00px;">
										<source data-src="figures/poisson/forward_animated.webm" type="video/webm">
									</video>
								</td>
								<td></td>
								<td style="text-align: center; padding: 0px; vertical-align:top;">
									<img style="width: 700px; " src="figures/poisson/all_measurements_forward.svg">
								</td> 
							</tr> 

						</table>
					</section>

					<section>
						<h3>Example: Derivation</h3>
						<h4>weak forms</h4>
						\begin{align*}
							a_m(u, v) &= \int_{\mathcal{R}} m \nabla u \cdot{} \nabla v dx - \int_{\partial \mathcal{R}} m \nabla_n u v + u \nabla_n v \, d\Gamma + \int_{\partial \mathcal{R}} \alpha u v \, d \Gamma \\
							b^{(i)}(v) &= \int_{\partial \mathcal{R}} \nabla_n v  g^{(i)} \, d\Gamma - \int_{\partial \mathcal{R}} \alpha g^{(i)} v \, d\Gamma\\
							c^{(j)}(u) &= \int_{\mathcal{R}} h^{(j)} u \, dx
						\end{align*}
						<h4>adjoint forward</h4>
						\begin{align*}
							a_m(u, \lambda^{(j)}) + c^{(j)}(u) = 0 \quad \forall u\\
							\int_{\mathcal{R}} m \nabla u \cdot{} \nabla \lambda^{(j)} dx - \int_{\partial \mathcal{R}} m \nabla_n u \lambda^{(j)} + u \nabla_n \lambda^{(j)} \, d\Gamma + \int_{\partial \mathcal{R}} \alpha u \lambda^{(j)} \, d \Gamma + \int_{\mathcal{R}} \mu^{(j)} u \, dx = 0 \quad \forall u 
						\end{align*}
						<h4>adjoint forward (strong form)</h4>
						\begin{align*}
							\begin{cases}
								-\nabla m \cdot{} \nabla \lambda^{(j)} = -h^{(j)} \quad &\forall x \in \mathcal{R}\\
								\lambda^{(j)} = 0 \quad &\forall x \in \partial \mathcal{R}
							\end{cases}
						\end{align*}
					</section>
					<section>
						<h3>Example: Derivation</h3>
						<h4>adjoint derivative</h4>
						\begin{align*}
							&a_m(\bar{\lambda}^{(j)}, \dot{\lambda}) + \bar{\boldsymbol \Sigma}^{(j)T} \boldsymbol{b}(\dot{\lambda}) = 0 \quad \forall \dot{\lambda} \\
							&\int_{\mathcal{R}} m \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \dot{\lambda} dx - \int_{\partial \mathcal{R}} m \nabla_n \bar{\lambda}^{(j)} \dot{\lambda} + \bar{\lambda}^{(j)} \nabla_n \dot{\lambda} \, d\Gamma + \int_{\partial \mathcal{R}} \alpha \bar{\lambda}^{(j)} \dot{\lambda} \, d \Gamma \\
							&+ \int_{\partial \mathcal{R}} \nabla_n \dot{\lambda}  \sum_{i=1}^{I}(\bar{\Sigma}^{(i, j)} g^{(i)}) \, d\Gamma - \int_{\partial \mathcal{R}} \alpha \sum_{i=1}^{I} (\bar{\Sigma}^{(i, j)} g^{(i)}) \dot{\lambda} \, d\Gamma = 0 \quad \forall \dot{\lambda}
						\end{align*}
						<h4>adjoint derivative (strong form)</h4>
						\begin{align*}
							\begin{cases}
								- \nabla m \cdot{} \nabla \bar{\lambda}^{(j)} = 0 \quad &\forall x \in \mathcal{R} \\
								\bar{\lambda}^{(j)} = \sum_{i=1}^{I} \bar{\Sigma}^{(i, j)} g^{(i)} \quad &\forall x \in \partial \mathcal{R} 
							\end{cases}
						\end{align*}
						<h4>tangent model</h4>
						\begin{align*}
							\dot{C} = \sum_{j=1}^{J} \dot{a}_m(\bar{\lambda}^{(j)}, \lambda^{(j)}, \dot{m}) = \sum_{j=1}^{J} \int_{\mathcal{R}} \dot{m} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} \, dx - \int_{\partial \mathcal{R}} \dot{m} \nabla_n \bar{\lambda}^{(j)} \underbrace{\lambda^{(j)}}_{=0} \, d \Gamma
						\end{align*}
						<h4>adjoint model</h4>
						\begin{align*}
							\langle \bar{C}, \sum_{j=1}^{J} \sum_{j=1}^{J} \int_{\mathcal{R}} \dot{m} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} \, dx \rangle = \langle \bar{C} \sum_{j=1}^{J} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)}, \dot{m} \rangle_{L^2(\mathcal{R})} \\
							\bar{a}_m(\bar{\lambda}^{(:)}, \lambda^{(:)}, \bar{C}) = \sum_{j=1}^{J} \bar{C} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} 
						\end{align*}
					</section>
					<section>
						<h3>Example: Derivation</h3>
						<h4>adjoint parametrization</h4>
						\begin{align*}
							\langle \bar{m}, \dot{m} \rangle_{L^2(\mathcal{R})} = \langle \bar{m}, \dot{\gamma}_p(\dot{p}) \rangle_{L^2(\mathcal{R})} = \langle \int_{\mathcal{R}} \frac{\partial \gamma_p}{\partial p} \bar{m} \, dx, \dot{p} \rangle_{\mathbb{R}} 
						\end{align*}
					</section>
				</section>

				<section style="text-align: left;">
					<section style="text-align: left;">
						<h3>Example: An inverse problem based on the Poisson equation</h3>
						<table>
							<tr style="border-style: hidden;">
								<!-- <td style="text-align: center; "><h5>adjoint forward (strong form)</h5></td> -->
								<td style="text-align: center; vertical-align: top; ">
									<h4>adjoint forward (strong form)</h4>
									<div style="margin-top: -20px; font-size: 70%;">
										\begin{align*}
											&\begin{cases}
												-\nabla \cdot{} m \nabla \lambda^{(j)} = -h^{(j)} &\forall x \in \mathcal{R} \\
												\lambda^{(j)} = 0 \quad &\forall x \in \partial \mathcal{R}
											\end{cases}\\
											&\Sigma^{(i, j)} = \int_{\partial \mathcal{R}} \nabla_n \lambda^{(j)} g^{(i)} \, d x\\
											&C = \frac{1}{2 I J} \sum_{i, j}^{I, J} (\Sigma^{(i, j)} - \tilde{\Sigma}^{(i, j)})^2
										\end{align*}
									</div>
								</td>
								<td style="text-align: center; vertical-align: top; ">
									<h5>adjoint forward $\lambda^{(j)}$</h5>
									<video data-autoplay loop style="width: 600px; margin-top: 0px;">
										<source data-src="figures/poisson/forward_adjoint.webm" type="video/webm">
									</video>
								</td>
								<td>
									<ul>
										<li>rhs: extractions $h^{(j)}$</li>
										<li>numerical costs:
											<ul>
												<li style="background-color: lightgreen;">
													(adjoint) $\sim 52ms \approx 7 \times 7ms$
												</li>
												<li style="background-color: lightcoral;">
													(non-adjoint) $\sim 5.3s \approx 800 \times 7ms$
												</li>
											</ul>
										</li>
										<li>adjoint and non-adjoint measurements agree (numerical precision)</li>
									</ul>
								</td>
							</tr>
							<tr>
								<td style="text-align: center; vertical-align: top; ">
									<h4>adjoint derivative (strong form)</h4>
									<div style="margin-top: -20px; font-size: 70%;">
										\begin{align*}
											&\bar{\Sigma}^{(i, j)} = \frac{1}{IJ} (\Sigma^{(i, j)} - \tilde{\Sigma}^{(i, j)}) \bar{C}\\
											&\begin{cases}
												-\nabla \cdot{} m \nabla \bar{\lambda}^{(j)}  = 0 \quad  &\forall x \in \mathcal{R} \\
												\bar{\lambda}^{(j)} = \sum_{i=1}^{I} \bar{\Sigma}^{(i, j)} g^{(i)} \quad &\forall x \in \partial \mathcal{R}\\
											\end{cases} \\
											&\bar{m} = \bar{C} \sum_{j=1}^{J} \nabla \bar{\lambda}^{(j)} \cdot{} \nabla \lambda^{(j)} \\
										\end{align*}
									</div>
								</td>
								<td style="text-align: center; vertical-align: top; ">
									<h5>gradient $\bar{m}$</h5>
									<img style="width:800px; " src="figures/poisson/gradient.svg">
								</td> 
								<td style="text-align: center; vertical-align: top; width:500px;">
									<ul>
										<li>rhs: augmented excitations $\bar{\boldsymbol \Sigma}^{(j)T}{\boldsymbol g}$</li>
										<li>cost (adjoint):<br>
											<ul>
												<li  style="background-color: lightgreen;">
													(adjoint) $\sim 98 ms \approx 2 \times 7 \times 7ms$
												</li>
												<li  style="background-color: lightcoral;">
													(finite differences) $\sim 45 s \approx 757 \times 55ms$
												</li>
												<li  style="background-color: lightcoral;">
													(finite differences + non-adjoint)$\sim 1h\, 10min \approx 757 \times 800 \times 7ms$
												</li>
											</ul>
										</li>
										<li>gradient agrees with finite differences (approximately)
										</li>
									</ul>
								</td>
							</tr>
						</table>
						<!-- <div style="margin-top: 0px;">
							<table style="text-align: center; padding: 0px; margin-top:-30px;">
								<tr style="border-style: hidden;">
									<td style="text-align: center; padding: 0px"><h5>adjoint forward $\lambda^{(j)}$</h5></td>
									<td style="text-align: center; padding: 0px"><h5>adjoint derivative $\bar{\lambda}^{(j)}$</h5></td>
									<td style="text-align: center; padding: 0px">
										<div class="r-stack">
											<h5 class="fragment fade-out" data-fragment-index="2">gradient $\bar{m}$</h5>
											<h5 class="fragment fade-in" data-fragment-index="2">gradient $\bar{m}$ (finite differences)</h5>
										</div>
									</td>
								</tr>
								<tr style="border-style: hidden;">
									<td style="text-align: center; padding: 0px;">
										<img src="figures/poisson/forward_adjoint/7.svg" style="width: 600px;"/>
									</td>
									<td style="text-align: center; padding: 0px;">
										<img src="figures/poisson/derivative_adjoint/7.svg" style="width: 600px;"/>
									</td>
									<td style="text-align: center; padding: 0px;">
										<div class="r-stack">
											<div class="fragment fade-out" data-fragment-index="2" >
												<img src="figures/poisson/gradient.svg" style="width: 600px;"/>
											</div>
											<div class="fragment fade-in" data-fragment-index="2" >
												<img src="figures/poisson/gradient_fd.svg" style="width: 600px;"/>
											</div>
										</div>
									</td>
								</tr>
								<tr style="border-style: hidden;">
									<td style="text-align: center; padding: 0px;"></td>
									<td style="text-align: center; padding: 0px;"></td>
									<td style="text-align: center; padding: 0px;">
										<div class="r-stack" style="margin: -50px">
											<div class="fragment fade-out" data-fragment-index="2" style="background-color: green;">
												$\sim 92 ms \approx 2 \times 7 \times 7ms$
											</div>
											<div class="fragment fade-in-then-out" data-fragment-index="2" style="background-color: red;">
												$\sim 45 s \approx 757 \times 55ms$
											</div>
											<div class="fragment fade-in" data-fragment-index="3" style="background-color: red;">
												$\sim 1h\, 10min \approx 757 \times 800 \times 7ms$
											</div>
										</div>
									</td>
								</tr>
							</table>
						</div> -->
					</section>

					<section style="text-align: left;">
						<h3>Example: An inverse problem based on the Poisson equation</h3>
						\begin{align*}
							m = \gamma_p = x \mapsto (0.1, 0.9, 0.4)^T \cdot{} \mathcal{NN}^{2 \to 20 \text{(tanh)} \to 20\text{(tanh)} \to 3\text{(softmax)}}_p(x)
						\end{align*}
						<ul>
							<li>inverting on $m$ directly is ill-posed</li>
							<li>motivated from SciML: use a $\mathcal{NN}$ as the parametrization $\gamma_p$</li>
							<li>we use the parametrization $\gamma_p$ to "regularize" (here: phase classification)</li>
							<li>number of parameters: $544$</li>
							<li>coarser discretization for the inversion ($757$ material cells vs $2972$ for the true measurements)</li>
							<li>$1$% random noise added to the true measurements</li>
						</ul>
						<table>
							<tr style="border-style: hidden;">
								<td style="text-align: center; " width="620px">
									<h5>measurements $\Sigma^{(i, j)}$ and $\tilde{\Sigma}^{(i, j)}$</h5>
								</td>
								<td style="text-align: center; " width="580px">
									<h5>optimized material $m$</h5>
								</td>
								<td style="text-align: center; " width="600px">
									<div class="r-stack">
										<h5 class="fragment fade-out" data-fragment-index="2">objective $\text{MSE}(\Sigma^{(i, j)}, \tilde{\Sigma}^{(i,j)})$</h5>
										<h5 class="fragment fade-in" data-fragment-index="2">true material $\tilde{m}$</h5>
									</div>
								</td>
							</tr>
							<tr class="fragment highlight-red" data-fragment-index="1">
								<td style="text-align: center; vertical-align: top;">
									<video loop data-autoplay style="width:550px; margin-top: 20px;"> 
										<source data-src="figures/poisson/optimization_measurements.webm" type="video/webm">
									</video>
								</td>
								<td style="text-align: center; ">
									<div class="fragment highlight-red" data-fragment-index="1">
										<video loop data-autoplay style="width: 600px;">
											<source data-src="figures/poisson/optimization_material.webm" type="video/webm">
										</video>
									</div>
								</td>
								<td style="text-align: center; vertical-align: top;">
									<div class="r-stack">
										<video loop data-autoplay style="width:550px; margin-top: 50px; "> 
											<source data-src="figures/poisson/optimization_error.webm" type="video/webm">
										</video>
										<div class="fragment fade-in" data-fragment-index="2">
											<video loop style="width: 600px;">
												<source data-src="figures/poisson/true_material.webm" type="video/webm">
											</video>
										</div>
									</div>
									
								</td>
							</tr>
						</table>
						<ul>
							<li>convergence dependes in the inital guess (does not always converge)</li>
							<li>optimizer: BFGS</li>
							<li>taylor the parametrization $\gamma_p$ to a specific problem (layers, other geometry, phase values, e.g.)</li>
						</ul>
					</section>
				</section>

				

				<section>
					<section style="text-align: left;">
						<h3>Outlook: Material Reconstruction in EPMA</h3>
						<ul>
							<li>model: (stationary) radiative transfer/linear Boltzmann equation in continuous-slowing-down approximation (CSD)</li>
							<li>adjoint model: reversed in time and direction(or space) $a_m(\psi, \phi)$ vs. $a_m(\phi, \psi)$</li>
						</ul>
						<table>
							<tr style="border-style: hidden;">
								<td style="text-align: center; ">
									<h4>forward $\psi^{(i)}$</h4>
								</td>
								<td style="text-align: center; ">
									<h4>adjoint forward $\lambda^{(j)}$</h4>
								</td>
								<td style="text-align: center; ">
									<h4>gradient $\bar{\rho}_e$</h4>
								</td>
							</tr>
							<tr style="border-style: hidden;">
								<td style="text-align: center; vertical-align: top;">
									<video loop data-autoplay style="width: 600px; margin-top: 00px;">
										<source data-src="figures/adjoint_visualization/forward.webm" type="video/webm">
									</video>
								</td>
								<td style="text-align: center; vertical-align: top;">
									<video loop data-autoplay style="width: 600px; margin-top: 00px;">
										<source data-src="figures/adjoint_visualization/adjoint.webm" type="video/webm">
									</video>
								</td>
								<td style="text-align: center; vertical-align: top;">
									<img src="figures/adjoint_visualization/gradient.svg" style="width: 600px; margin-top: 0px;"> 
								</td>
							</tr>
						</table>
						<div style="display: flex;">
							<div style="font-size: 60%; flex:2;">
								<h4 style="margin-top: 150px; font-size: 150%;">References</h4>
								<ul>
									<li>J. Bünger: <em>Three-dimensional modelling of x-ray emission in electron probe microanalysis based on deterministic transport equations</em>. Phd thesis, RWTH Aachen University (2021)<br><a href="https://doi.org/10.18154/RWTH-2021-05180">doi:10.18154/RWTH-2021-05180</a></li>
									<li>T. Bui-Tanh: <em>Adjoint and Its roles in Sciences, Engineering, and Mathematics: A Tutorial</em>. arXiv (2023)<br><a href="https://doi.org/10.48550/arXiv.2306.09917">doi:10.48550/arXiv.2306.09917</a></li>
									<li>J.A. Halbleib and J.E. Morel: <em>Adjoint Monte Carlo Electron Transport in the Continuous-Slowing-Down-Approximation</em>. J. Comput. Phys. (1980)<br><a href="https://doi.org/10.1016/0021-9991(80)90106-0">doi:10.1016/0021-9991(80)90106-0</a></li>
									<li>U. Naumann: <em>The Art of Differentiating Computer Programs: An Introduction to Algorithmic Differentiation</em>. SIAM (2012)<br><a href="https://doi.org/10.1137/1.9781611972078">doi:10.1137/1.9781611972078</a></li>
									<li>R.E. Plessix: <em> A review of the adjoint-state method for computing the gradient of a functional with geophysical applications</em>. Geophys. J. Int. (2006)<br><a href="https://doi.org/10.1111/j.1365-246X.2006.02978.x">doi:10.1111/j.1365-246X.2006.02978.x</a></li>
								</ul>
							</div>
							<div style="flex:1 ; font-size: 60%"> 
								<h4 style="margin-top: 150px; ">Slides + Pluto.jl notebook (Poisson example)</h4>
								<a href="https://github.com/tam724/claus_applying_adjoints_twice_presentation">
									<img src="figures/tam724_github_qr.png">
									<br>
									github.com/tam724
								</a>
							</div>
						</div>
						<div style="position: absolute; top:700px; left: 500px;">
							<img src="figures/adjoint_visualization/measurements.svg" style="width:400px;">
						</div>
					</section>

					<section style="text-align: left;">
						<h3>Material Reconstruction in EPMA: Optimization Example</h3>
						<ul>
							<li>$\mathcal{NN}$ parametrization along a single dimension (but with an angle)</li>
						</ul>
						<img src="figures/jl_6dYjeVKPKV.gif"><br>
						<ul>
							<li>final measurements (not perfect..)</li>
						</ul><br>
						<img src="figures/final_measurements.svg">

					</section>

					<section style="text-align: left;">
						<h3>Material Reconstruction in EPMA</h3>
						<ul>
							<li>
								(stationary) radiative transfer/linear boltzmann equation in continuous-slowing-down approximation (CSD) ($x \in \mathcal{R} \subset \mathbb{R}^3$, $\epsilon \in \mathbb{R}^+$, $\Omega \in S^2$)
								\begin{align}
									-\partial_\epsilon (s(x, \epsilon) \psi^{(i)}(x, \epsilon, \Omega)) + \Omega \cdot{} \nabla_x \psi^{(i)}(x, \epsilon, \Omega) = \int_{S^2} k(x, \epsilon, \Omega \cdot{} \Omega') \psi^{(i)}(x, \epsilon, \Omega') \, d\Omega' - \tau(x, \epsilon) \psi^{(i)}(x, \epsilon, \Omega)
								\end{align}
							</li>
							<li>
								beam is modeled by boundary conditions (excitations)
								\begin{align}
									\psi^{(i)}(x, \epsilon, \Omega) = g^{(i)}(x, \epsilon, \Omega) \quad \forall x \in \partial \mathcal{R} | n \cdot{} \Omega < 0 
								\end{align}
							</li>
							<li>$i = 1, ..., I$: beam position, beam energy, beam direction</li>
							<li>
								additivity approximation (for $\sigma = s$, $k$ and $\tau$), $p \in \mathbb{R}^N$
								\begin{equation}
									\sigma(x, \cdot{}) = \sum_{e=1}^{n_e} \rho_{e}(x) \sigma_e(\cdot{})\quad \text{where} \quad (\rho_1, ... \rho_{n_e})^T = \gamma_p
								\end{equation}
							</li>
							<li>
								x-ray generation and detection (extraction)
								\begin{align}
									\Sigma^{(i, j)} = \int_{\mathcal{R}} \int_{0}^{\infty} \sigma^{ion}_j(\epsilon) \rho_j(x) e^{-\int_{d(x)} \sum_{e=1}^{n_e} \mu_e^{(j)} \rho_e(y) \,d y}\int_{S^2} \psi^{(i)}(x, \epsilon, \Omega) \,d \Omega \, d \epsilon \,d  x

								\end{align}
							</li>
							<li>$j = 1, ..., J$: number of measured x-ray lines/materials</li>
						</ul>
					</section>

					<section style="text-align: left;">
						<h3>Material Reconstruction in EPMA</h3>
						<ul>
							<li>compute measurements $\Sigma^{(i, j)}$ with the extraction as the source to the adjoint RT-CSD</li>
							<li>adjoint RT-CSD: reversed in energy and direction(or space) $a_m(\psi, \phi)$ vs. $a_m(\phi, \psi)$</li>
							<table>
								<tr style="border-style: hidden;">
									<td style="text-align: center; ">
										<h4>forward $\psi^{(i)}$</h4>
									</td>
									<td style="text-align: center; ">
										<h4>adjoint forward $\lambda^{(j)}$</h4>
									</td>
								</tr>
								<tr style="border-style: hidden;">
									<td style="text-align: center; ">
										<video data-autoplay style="width: 800px; margin-top: 40px;">
											<source data-src="figures/adjoint_visualization/forward.webm" type="video/webm">
										</video>
									</td>
									<td style="text-align: center; ">
										<video data-autoplay style="width: 800px; margin-top: 40px;">
											<source data-src="figures/adjoint_visualization/adjoint.webm" type="video/webm">
										</video>
									</td>
								</tr>
								<tr style="border-style: hidden;">
									<td style="text-align: center; ">
										cost: $I \times \mathcal{C}(a) \sim I \times 2\text{min}$
									</td>
									<td style="text-align: center; ">
										cost: $J \times \mathcal{C}(a) \sim J \times 2\text{min}$
									</td>
								</tr>
							</table>
						</ul>
						<table>
							<tr>
								<td style="width:800px; vertical-align: top;"> 
									<ul>
										<li>orange/green marker: measurements from forward</li>
										<li>green lines: measurements from adjoint forward</li>
									</ul>
								</td>
								<td>
									<img src="figures/adjoint_visualization/measurements.svg" style="width: 800px;">
								</td>
							</tr>
						</table>
					</section>

					<section style="text-align: left;">
						<h3>Material Reconstruction in EPMA</h3>
						<ul>
							<li>compute the gradient $\bar{\rho}_e$ using the "adjoint augmented" excitations as the source to the RT-CSD</li>
						</ul>
						<table>
							<tr style="border-style: hidden;">
								<td style="text-align: center; "><h4>adjoint derivative $\bar{\lambda}^{(j)}$</h4></td>
								<td style="text-align: center; "><h4>gradient $\bar{\rho}_e$</h4></td>
							</tr>
							<tr style="border-style: hidden;">
								<td style="text-align: center; ">
									<video data-autoplay style="width: 800px; margin-top: 40px;">
										<source data-src="figures/adjoint_visualization/adjoint_derivative_1.webm" type="video/webm">
									</video>
								</td>
								<td style="text-align: center; ">
									<img src="figures/adjoint_visualization/gradient.svg" style="width: 800px">
								</td>
							</tr>
						</table>
					</section>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/

			Reveal.configure({pdfSeparateFragments: true});
			Reveal.initialize({
				hash: true,
				controls: true, 
				controlsLayout: 'bottom-right',
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: 1920,
				height: 1440,

				// Factor of the display size that should remain empty around
				// the content
				margin: 0.1,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.01,
				maxScale: 2.0,
				center: false,

				transition: 'none',
				backgroundTransition: 'fade-in',

				slideNumber: slide => {
					// Ignore numbering of vertical slides
					return [ Reveal.getIndices(slide ).h+"/"+(Reveal.getHorizontalSlides().length-1)];},

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
			});
		</script>
	</body>
</html>
